\documentclass[11pt]{article}

\usepackage{fancybox}


\usepackage{color}
\usepackage{url}
\usepackage[margin=1in]{geometry}

\setlength\parindent{0pt}
\renewcommand{\textfraction}{0.0}
\renewcommand{\topfraction}{1.0}
%\renewcommand{\textfloatsep}{5mm}

\usepackage{comment}
% Definitions of handy macros can go here
\usepackage{amsmath,amssymb,amsthm,bm,mathtools}
\usepackage{multirow}
\usepackage{extarrows}
\usepackage{dsfont,multirow,hyperref,setspace,natbib,enumerate}
%\usepackage{dsfont,multirow,hyperref,setspace,enumerate}
\hypersetup{colorlinks,linkcolor={blue},citecolor={blue},urlcolor={red}} 
\usepackage{algpseudocode,algorithm}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}
\usepackage{dsfont}
\usepackage{colonequals}

\mathtoolsset{showonlyrefs=true}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\subG}{subG}


\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{assumption}{Assumption}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}


\setcounter{figure}{0}   
\setcounter{table}{0}  


\newcommand{\cmt}[1]{{\leavevmode\color{red}{#1}}}

\usepackage{dsfont}
\usepackage{multirow}

\DeclareMathOperator*{\minimize}{minimize}



\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}
\usepackage{xr}

\input macros.tex



\title{Solution to ``Chapter 2: Basic tail and concentration bounds"}

\date{\today}
\author{%
Jiaxin Hu
}



\begin{document}

% Makes the title and author information appear.

\maketitle

\section{Summary}
\begin{thm}[Markov's inequality]\label{thm:markov}
	Let $X \geq 0$ be a random variable  with a finite mean. We have
	\begin{equation}\label{eq:thmmarkov}
		\bbP(X\geq t) \leq \frac{\bbE[X]}{t},\quad \text{ for all } \ t > 0.
	\end{equation}  
\end{thm}

\begin{thm}[Chebyshev's inequality]\label{thm:cheb}
	Let $X \geq 0$ be a random variable  with a finite mean $\mu$ and a finite variance. We have
	\begin{equation}\label{eq:cheby}
		\bbP(|X - \mu |\geq t) \leq \frac{\var(X)}{t^2},\quad \text{ for all } \ t > 0.
	\end{equation}  
\end{thm}

\begin{thm}[Markov's inequality for polynomial moments]\label{thm:polymoment}
	Let $X$ be a random variable. Suppose that the order $k$ central moment of $X$ exists. Applying Markov's inequality to the random variable $|X - \mu|^k$ yields
	\begin{equation}\label{eq:poly}
		\bbP(|X - \mu |\geq t) \leq \frac{\bbE\left[|X-\mu|^k\right]}{t^k},\quad \text{ for all } \ t > 0.
	\end{equation}  
\end{thm}

\begin{thm}[Chernoff bound]\label{thm:chernoff}
 Let $X$ be a random variable. Suppose that the moment generating function of $X$, denoted $\varphi_X(\lambda)$, exists in the neighborhood of 0; i.e., $\varphi_X(\lambda) = \bbE[e^{\lambda X}] < +\infty,$ for all $\lambda \in (-b,b)$ with some $b>0$. Applying Markov's inequality to the random variable $Y = e^{\lambda(X- \mu)}$ yields
	\begin{equation}\label{eq:chernoff1}
		\bbP((X - \mu) \geq t) \leq \frac{\bbE\left[e^{\lambda (X-\mu)}\right]}{e^{\lambda t}}, \quad \text{ for all } \lambda\in (-b,b).
	\end{equation}
	Optimizing the choice of $\lambda$ for the tightest bound, we obtain the Chernoff bound
	\begin{equation}\label{eq:chernoff2}
		\bbP((X - \mu) \geq t) \leq \inf_{\lambda \in [0,b)} \frac{\bbE\left[e^{\lambda (X-\mu)}\right]}{e^{\lambda t}}.
	\end{equation}
\end{thm}


\begin{thm}[Hoeffding bound for bounded variable]\label{thm:boundvar}
Let $X$ be a random variable with $\mu = \bbE(X)$. Suppose that $X \in [a,b]$ almost surely, where $a\leq b\in\bbR$ are two constants. Then, we have
\begin{equation}
	\bbE[e^{\lambda X}] \leq e^{\frac{s(b-a)^2}{8}}, \quad \text{for all } \lambda \in \bbR.
\end{equation}
Consequently, the variable $X \sim \subG\left(\frac{(b-a)^2}{4}\right)$.
\end{thm}

\begin{proof}
	See Exercise~\ref{pro:boundvar}.
\end{proof}



\begin{thm}[Moment of sub-Gaussian variable]\label{thm:momsubg}
Let $X \sim \subG(\sigma^2)$. For all integer $k \geq 1$, we have
\begin{equation}\label{eq:thmmomsubg}
	\bbE[|X|^k] \leq  k 2^{k/2} \sigma^k \Gamma(\frac{k}{2} ),
\end{equation} 
where the Gamma function is defined as $\Gamma(x) = \int_{0}^{+\infty} t^{x -1} e^{-t} dt$. 
\end{thm}


\begin{thm}[One-sided Bernstein’s inequality]\label{thm:oneber}
 Let $X$ be a random variable. Suppose $X \leq b$ almost surely. We have 
\begin{equation}\label{eq:thmoneber1}
	\bbE\left[e^{\lambda (X-\bbE[X])}\right] \leq exp\left\{ \frac{\lambda^2 \bbE[X^2]/2}{1 - b\lambda /3}  \right\}, \quad \text{for all } \lambda \in [0, 3/b).
\end{equation}
Consequently, let $X_i$ be independent variables, and $X_i \leq b$ almost surely, for all $i \in [n]$. We have
\begin{equation}\label{eq:thmoneber2}
	\bbP\left[\sum_{i = 1}^n (X_i - \bbE[X_i] ) \geq n\delta \right] \leq exp\left\{ - \frac{n\delta^2}{\sum_{i = 1}^n \bbE[X_i^2] /n + b \delta /3} \right\},\quad \text{ for all } \delta \geq 0.
\end{equation}  
Particularly, let $X_i$ be independent nonnegative variables, for all $i \in [n]$. The equation~\eqref{eq:thmoneber2} becomes
\begin{equation}\label{eq:thmonber0}
	\bbP\left[\sum_{i = 1}^n (Y_i - \bbE[Y_i] ) \leq n\delta \right] \leq exp\left\{ - \frac{n\delta^2}{\sum_{i = 1}^n \bbE[Y_i^2] /n} \right\},\quad \text{ for all } \delta \geq 0.
\end{equation} 
\end{thm}

\begin{defn}[Bernstein's condition]
Let $X$ be a random variable with mean $\mu = \bbE[X]$ and variance $ \sigma^2 = \var(X)$. We say $X$ satisfies the Bernstein's condition with parameter $b$ if
\begin{equation}
	\left| \bbE [(X -\mu)^k ]\right| \leq \frac{1}{2} k! \sigma^2 b^{k-2}, \quad \text{for } k =3, 4,....
\end{equation} 
\end{defn}
Note that bounded random variables satisfy the Bernstein's condition.

\begin{thm}[Bernstein-type bound] For any variable $X$ satisfying the Bernstein's condition, we have
\begin{equation}
	\bbE\left[e^{\lambda (X-\mu)}\right] \leq \exp\left\{ \frac{\lambda^2 \sigma^2}{2(1 - b |\lambda|)} \right\},\quad \text{for all } |\lambda| \leq \frac{1}{b}, 
\end{equation}
and the concentration inequality
\begin{equation}
	\bbP\left[|X - \mu| \geq t\right] \leq 2 \exp \left\{ -\frac{t^2}{2(\sigma^2 + bt)} \right\}, \quad \text{for all }t \geq 0.
\end{equation}
	
\end{thm}

\section{Exercises}

\subsection{Exercise 2.1} (Tightness of inequalities.) The Markov's and Chebyshev's inequalities are not able to be improved in general. 
\begin{enumerate}
	\item[(a)] Provide a random variable $X \geq 0$ that attains the equality in Markov's inequality~\eqref{eq:thmmarkov}.  
	\item[(b)] Provide a random variable $Y$ that attains the equality in Chebyshev's inequality~\eqref{eq:cheby}.  
\end{enumerate}

\paragraph{Solution:} 
\begin{enumerate}

\item[(a)] For a given constant $t >0$, we define a variable $Y_{t} = X - t \mathds{1}{[X\geq t]}$, where $\mathds{1}$ is the indicator function. Note that $Y_{t}$ is a nonnegative variable. The Markov's inequality follows by taking the expectation to $Y_t$,
\begin{equation}
	\bbE[Y_t] = \bbE[X] - t \bbP[X \geq t] \geq 0.
\end{equation}
Therefore, Markov's inequality meets the equality if and only if the expectation $\bbE[Y_t] = 0$. Since $Y_t$ is nonnegative, we have $\bbP(Y_t = 0) = 1$. Note that $Y_t = 0$ if and only if $X = 0$ or $X =t$.

Hence, for the given constant $t >0$, the nonnegative variable $X$ with distribution $\bbP(X \in \{ 0,t\}) = 1$ attains the equality of Markov's inequality.

\item[(b)]  Chebyshev's inequality follows by applying Markov's inequality to the nonnegative random variable $Z = {(X - \bbE[X])^2}$. Simialrly as in part (a), given a constant $t > 0$, the variable $Z = {(X - \bbE[X])^2}$ with distribution $\bbP(Z \in \{ 0,t^2 \}) = 1$ attains the equality of the Markov's inequality for $Z$. Consequently, the variable $X$ attains the equality of the Chebyshev's inequality for $X$. By transformation, the distribution of $X$ satisfies the followings formula,
\begin{align}
\bbP(X = x) = 
	\begin{cases}
		\quad p & \text{if } x = c,\\
		\quad \frac{1-p}{2} & \text{if } x = c-t \text{ or } x = c+t, \\
		\quad 0 & \text{ otherwise },
	\end{cases}
	\end{align}
where $c \in \bbR$ is a constant and $p \in [0,1]$.

\iffalse
	\item[(a)] Recall the proof of Markov's inequality. For any $t >0$,
	\begin{equation}\label{eq:markov}
		\bbE[X] = \int_{0}^t x f_X(x) dx + \int_{t}^{+\infty} x f_X(x) dx \geq \int_{t}^{+\infty} x f_X(x)\geq t \int_{t}^{+\infty} f_X(x) = t \bbP( X \geq t).
	\end{equation}
	Below, given a constant $t > 0$, we construct a random variable that attains the equality in each line of equation~\eqref{eq:markov}.
	 
	 Consider a variable $X$ with distribution $P(X = t) = 1$. The variable $X$ satisfies 
	 \[ \int_{0}^t x f_X(x) dx = 0 \text{ and } \int_{t}^{+\infty} x f_X(x) dx =\int_{t}^{+\infty} t f_X(x) dx. \]
	 Therefore, for given $t$, the variable $X$ attains the equality of Markov's inequality.
	 
	 \item[(b)] Chebyshev's inequality follows by applying Markov's inequality to the non-negative random variable $Y = {(X - \bbE[X])^2}$. Let the distribution of $Y$ be $\bbP(Y = t^2) = 1$. Then the Markov's inequality for $Y$ and the Chebyshev's inequality for $X$ meet the equalities. Consider the variable $Z$ with distribution $\bbP(Z = c+t) = \bbP(Z = c-t)  = 1/2 $ for any $ c \in \bbR$. For $Z$, the transformation probability satisfies $\bbP((Z - \bbE[Z])^2= t^2) = 1$. Therefore, the variable $Z$ attains the equality of Chebyshev's inequality.
	 
\fi
	 
	 
\end{enumerate}

\begin{rmk}[Tightness of Markov's inequality]
	Only a few variables attain the equalities in Markov's and Chebyshev's inequalities. In research, we should pay attention to the concentration bounds tighter than Markov's inequality.
\end{rmk}


\subsection{Exercise 2.2}
\begin{lem}[Standard normal distribution]\label{lem:n01}
	Let $\phi(z)$ be the density function of a standard normal variable $Z \sim N(0,1)$. Then, 
	\begin{equation}\label{eq:e22a}
		\phi'(z) + z \phi(z) = 0,
	\end{equation}
	and \begin{equation}\label{eq:e22b}
		\phi(z)\left(\frac{1}{z} - \frac{1}{z^3} \right) \leq \bbP(Z \geq z) \leq \phi(z)\left(\frac{1}{z} - \frac{1}{z^3}+ \frac{3}{z^5} \right), \quad \text{for all }z>0.
	\end{equation}
\end{lem}
\begin{proof}First, we prove the equation~\eqref{eq:e22a}. 

The pdf of the standard normal distribution is
\[  \phi(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{z^2}{2}  \right).\]

The equation~\eqref{eq:e22a} follows by taking the derivative of $\phi(z)$. Specifically,
\[\quad \phi'(z) = -z \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{z^2}{2} \right) = -z \phi(z). \] 
	Next, we prove the equation~\eqref{eq:e22b}. 
	
	We write the upper tail probability of the standard normal variable as
	\begin{equation}\label{eq:e22pro1}
		\bbP(Z \geq z) = \int_{z}^{+\infty} \phi(t) dt = \int_{z}^{+\infty} - \frac{1}{t} \phi'(t) dt = \frac{1}{z} \phi(z) - \int_{z}^{+\infty} \frac{1}{t^2} \phi(t) dt,  
	\end{equation}
	where the second equality follows by the equation~\eqref{eq:e22a}. Applying the equation~\eqref{eq:e22a} to the last term in equation~\eqref{eq:e22pro1} yields
	\begin{equation}\label{eq:e22pro2}
	\int_{z}^{+\infty} \frac{1}{t^2} \phi(t) dt = \int_{z}^{+\infty} \frac{1}{t^3} \phi'(t) dt = - \frac{1}{z^3} \phi(z) + \int_{z}^{+\infty} \frac{3}{t^4} \phi(t) dt \geq  - \frac{1}{z^3} \phi(z)
	\end{equation}
	Plugging the equation~\eqref{eq:e22pro2} into the equation~\eqref{eq:e22pro1}, we obtain $\bbP(Z \geq z) \geq \phi(z)\left(\frac{1}{z}  - \frac{1}{z^3} \right)$. Applying the equation~\eqref{eq:e22a} again to the equation~\eqref{eq:e22pro2} yields
	\begin{equation}\label{eq:e22pro3}
		\int_{z}^{+\infty} \frac{3}{t^4} \phi(t) dt = \int_{z}^{+\infty} - \frac{3}{t^5} \phi'(t) dt =\frac{3}{ z^5} \phi(z) -  \int_{z}^{+\infty} \frac{15}{t^6} \phi(t) dt \leq \frac{3}{ z^5} \phi(z).
	\end{equation}
	
	Combing equations~\eqref{eq:e22pro1}, \eqref{eq:e22pro2} and  \eqref{eq:e22pro3}, we obtain $\bbP(Z \geq z) \leq \phi(z)\left(\frac{1}{z}  - \frac{1}{z^3} + \frac{3}{z^5} \right).$
\end{proof}

\begin{rmk}
	Direct calculation of tail probability for a univariate normal variable is hard. Equation~\eqref{eq:e22b} provides a numerical approximation to the tail probability. Particularly, the tail probability decays at the rate of $z^{-1}e^{-z^2/2}$ as $z \rightarrow +\infty$. The decay rate is faster than polynomial rate $\tO(z^{-\alpha})$, for any $\alpha \geq 1$.
\end{rmk}

\subsection{Exercise 2.3}

\begin{lem}[Polynomial bound and Chernoff bound]\label{lem:momcher}
	Let $X \geq 0$ be a nonnegative variable. Suppose that the moment generating function of $X$, denoted $\varphi_{X}(\lambda)$, exists in the neighborhood of $\lambda = 0$. Given some $\delta > 0$, we have
	\begin{equation}\label{eq:lempoly}
		\inf_{k \in \mathbb{Z_+} } \frac{\bbE[|X|^k]}{\delta^k} \leq \inf_{\lambda >0} \frac{\bbE[e^{\lambda X}]}{e^{\lambda \delta}}. 
	\end{equation}
	  
	Consequently, an optimized bound based on polynomial moments is always at least as good as the Chernoff upper bound.
\end{lem}

\begin{proof}
	By power series, we have
	\begin{equation}\label{eq:power}
		 e^{\lambda X} = \sum_{k = 0}^{+\infty } \frac{X^k \lambda^k }{k!}, \quad \text{ for all } \lambda \in \bbR.
	\end{equation}
	Since the moment generating function $\varphi_X(\lambda)$ exists in the neighborhood of $\lambda = 0$, there exists a constant $b > 0$ such that 
		\[   \bbE [e^{\lambda X} ]= \sum_{k = 0}^{+\infty } \frac{ \bbE[|X|^k]  \lambda^k }{k!} < +\infty,\quad \text{ for all } \lambda \in (0,b). \]
	 Hence, the moment  $\bbE[|X|^k]$ exists, for all $ k \in \mathbb{Z_+}$. Applying power series~\eqref{eq:power} to the right hand side of equation~\eqref{eq:lempoly} yields
	\begin{equation}\label{eq:powertopoly}
		\inf_{\lambda >0} \frac{\bbE[e^{\lambda X}]}{e^{\lambda \delta}} = \frac{ \sum_{k = 0}^{+\infty } \frac{ \bbE[|X|^k]  \lambda^k }{k!} }{ \sum_{k = 0}^{+\infty } \frac{\lambda^k \delta^k }{k!}  }.	\end{equation}
	By Cauchy's third inequality, we have
	\begin{equation}\label{eq:cauchy3}
		\frac{ \sum_{k = 0}^{+\infty } \frac{ \bbE[|X|^k]  \lambda^k }{k!} }{ \sum_{k = 0}^{+\infty } \frac{\lambda^k \delta^k }{k!}  } \geq \inf_{k \in \mathbb{Z}_{+} } \frac{\bbE[|X|^k]}{\delta^k}  
	\end{equation}
	Therefore, we obtain the equation~\eqref{eq:lempoly} by combining the equation~\eqref{eq:powertopoly} with equation~\eqref{eq:cauchy3}.	
\end{proof}

\begin{rmk}
	Applying different functions $g(X)$ to the Markov's inequality leads to different bounds for the tail probability of variable $X$. Equation~\eqref{eq:lempoly} implies that the optimized polynomial bound is at least as tight as the Chernoff bound, provided that the moment generating function of $X$ exsits in the neighborhood of 0.  
\end{rmk}

\subsection{Exercise 2.4}\label{pro:boundvar}
In Exercise~\ref{pro:boundvar}, we prove Theorem~\ref{thm:boundvar},  the Hoeffding bound for a bounded variable. 

\begin{proof}
	
Let $X$ be a bounded random variable, and $X \in [a,b]$ almost surely, where $a 
\leq b \in \bbR$ are two constants. Let $\mu = \bbE[X]$. Define the function 
\[ g(\lambda) = \log \bbE[e^{\lambda X }], \quad  \text{ for all } \lambda \in \bbR. \]
Applying Taylor Expansion to $g(\lambda)$ at 0, we have
\begin{equation}\label{eq:taylor}
	g(\lambda) =  g(0) + g'(0) \lambda  + \frac{g''(\lambda_0) }{2} \lambda^2, \quad \text{where } \lambda_0 = t \lambda, \text{ for some } t \in [0,1]. 
\end{equation}

In equation~\eqref{eq:taylor}, the term $g(0) =\log \bbE[e^{0}] = 0$. By power series \eqref{eq:power}, we obtain the first derivative $g'(\lambda)$ as follows,
\begin{align}
	g'(\lambda ) & = \left( \log \sum_{k = 0}^n \frac{\lambda^k }{k!} \bbE[X^k]  \right)' \\
	&= {\sum_{k = 0}^n \frac{ \lambda^k }{k!} \bbE[X^{(k+1)}] }\Bigg/{\sum_{k = 0}^n \frac{\lambda^k }{k!} \bbE[X^k]}  \label{eq:firstde} \\
	&= \frac{\bbE[X e^{\lambda X} ]}{\bbE[ e^{\lambda X} ]}.
\end{align}
Therefore, $g'(0) = \bbE[X] = \mu$. Taking the derivative to equation~\eqref{eq:firstde}, we obtain the second-order derivative $g''(\lambda)$ as follows,
\begin{align}
	g''(\lambda)
	&= {\sum_{k = 0}^n \frac{ \lambda^k }{k!} \bbE[X^{(k+2)}] }\Bigg/{\sum_{k = 0}^n \frac{\lambda^k }{k!} \bbE[X^k]} - \left( {\sum_{k = 0}^n \frac{ \lambda^k }{k!} \bbE[X^{(k+1)}] }\Bigg/{\sum_{k = 0}^n \frac{\lambda^k }{k!} \bbE[X^k]}  \right)^2\\
	&= \frac{\bbE[X^2 e^{\lambda X} ]}{\bbE[ e^{\lambda X} ]} - \left( \frac{\bbE[X e^{\lambda X} ]}{\bbE[ e^{\lambda X} ]} \right)^2.\\
	%& \overset{\Delta}{=} \bbE_{\lambda} [X^2] - (\bbE_{\lambda} [X])^2
\end{align}
We interpret the second-order derivative $g''(\lambda)$ as the variance of $X$ with the re-weighted distribution $d P' = {e^{\lambda X}}/{\bbE[ e^{\lambda X} ]} d P_X $, where $P_X$ is the distribution of $X$. Taking the integral of 1 with respect to $dP'$, we have
\begin{equation}
	\int_{-\infty}^{+\infty} d P' = \int_{-\infty}^{+\infty}  \frac{e^{\lambda X}}{\bbE[ e^{\lambda X} ]} d P_X = 1,
\end{equation} 
which implies that the function $P'$ is a valid probability distribution. Under all possible re-weighted distributions, the variance of $X$ is upper bounded as follows,
\[ \var(X) = \var(X - \frac{a+b}{2}) \leq \bbE[(X - \frac{a+b}{2} )^2] \leq \frac{(b-a)^2}{4}, \]
where the term $\frac{(b-a)^2}{4}$ follows by letting $X$ supported on the boundaries $a$ and $b$ only. Hence, the second-order derivative $g''(\lambda) \leq \frac{(b-a)^2}{4}$. We plug the results of $g'$ and $g''$ into the equation~\eqref{eq:taylor}. Then,
\begin{equation}\label{eq:e24}
	g(\lambda) =  g(0) + g'(0) \lambda  + \frac{g''(\lambda_0) }{2} \lambda^2 \leq 0 + \lambda \mu+ \frac{(b-a)^2}{8} \lambda^2. 
\end{equation}
Taking the exponentiation on both sides of the inequality~\eqref{eq:e24}, we have
\begin{equation}\label{eq:boundvar}
	\bbE[e^{\lambda X}] = \exp(g( \lambda)) \leq e^{\mu \lambda + \frac{(b-a)^2}{8} \lambda^2}.
\end{equation}
The equation~\eqref{eq:boundvar} implies that $X$ is a sub-Gaussian variable with at most $\sigma = \frac{(b-a)}{2}$.
\end{proof}

\begin{rmk}
	For any bounded random variable $X$ supported on $[a,b]$, $X$ is a sub-gaussian variable with parameter at most $\sigma^2 = (b-a)^2/4$. All the properties for sub-Gaussian variables apply to the bounded variables. 
\end{rmk}


\subsection{Exercise 2.5}
\begin{lem}[Sub-Gaussian bounds and means/variance] Let $X$ be a random variable  such that
\begin{equation}\label{eq:subg}
	 \bbE[e^{\lambda X}] \leq e^{\frac{\lambda^2 \sigma^2}{2} + \mu \lambda}, \quad \text{ for all } \lambda \in \bbR. 
\end{equation}
	Then, $\bbE[X] = \mu$ and $\var(X) \leq \sigma^2$.
\end{lem}

\begin{proof} By equation~\eqref{eq:subg}, the moment generating function of $X$, denoted $\varphi_X(\lambda)$, exists in the neighborhood of $\lambda = 0$.  Hence, the mean and variance of $X$ exist. For all $\lambda$ in the neighborhood of $\lambda = 0$, applying power series on both sides of equation~\eqref{eq:subg} yields 
\begin{equation}\label{eq:taylor2}
	 \lambda \bbE[X] + \frac{\lambda^2}{2} \bbE[X^2] + o(\lambda^2) \leq  \mu \lambda + \frac{\lambda^2 \sigma^2 + \lambda^2 \mu^2}{2} + o(\lambda^2). 
\end{equation}
Dividing by $\lambda >0$ on both sides of equation~\eqref{eq:taylor2} and letting $\lambda \rightarrow 0^{+}$, we have $\bbE(X) \leq \mu$. Dividing by $\lambda <0$ on both sides of equation~\eqref{eq:taylor2} and letting $\lambda \rightarrow 0^{-}$, we have $\bbE(X) \geq \mu$. Therefore, we obtain the mean $\bbE[X] = \mu$. Then, we divide ${2}/{\lambda^2}$ on both sides of equation~\eqref{eq:taylor2}, for $\lambda \neq 0$. The term $\bbE[X]\lambda$ and  $\mu \lambda$ are cancelled. We have $ \bbE[X^2] \leq \sigma^2 + \mu^2$, and thus the $\var(X) \leq \bbE[X^2]  - (\bbE[X])^2 = \sigma^2$.
\end{proof}

\paragraph{Question:} Let $\sigma^2_{min}$ denote the smallest possible $\sigma$ satisfying the inequality~\eqref{eq:subg}. Is it true that $\var(X) = \sigma_{min}^2$?

\paragraph{Solution:} The statement that $\var(X) = \sigma_{min}^2$ is not necessarily true. Recall the function $g(\lambda)$ in Exercise~\ref{pro:boundvar}. By the results in Exercise~\ref{pro:boundvar},  the equation~\eqref{eq:subg} is equal to 
\begin{equation}\label{eq:question}
	g''(\lambda) \leq  \sigma^2, \quad \text{ for all } \lambda \in \bbR,
\end{equation}
where $g'' (\lambda)$ is the variance of $X$ with the re-weighted distribution defined in Exercise~\ref{pro:boundvar}. Therefore, we have $\max_{\lambda}g''(\lambda) = \sigma_{min}^2$. Note that $g''(0) = \var(X)$. To let the equality $\var(X) = \sigma_{min}^2$ hold, we need to show that $\max_{\lambda}g''(\lambda) = g''(0)$ holds for $X$. 

\vspace{0.2cm}
However, the statement $\max_{\lambda}g''(\lambda) = g''(0)$ is not necessarily true.  A counter example is below. 
Consider a random variable $Y \sim Ber(1/3)$. The variance of $Y$ is $\var(Y) = 2/9$. Let $\lambda = 1$. The re-weighted distribution $dP'$ is
\[ P'(Y = 0) = \frac{2}{3\bbE[ e^{Y} ]}\quad  \text{and}\quad  P'(Y = 1) = \frac{e}{3 \bbE[ e^{ Y } ]}, \quad \text{where } \bbE[ e^{Y} ] = \frac{2}{3} + \frac{e}{3}.  \]
The variance of $Y$ with $dP'$ is ${2}/{3\bbE[ e^{Y} ]} \times  {e}/{3\bbE[ e^{Y} ]} = 0.2442 > 2/9$. Therefore, we have $\var(Y) <  g''(1) \leq \max_{\lambda}g''(\lambda) = \sigma^2_{min}$. The statement $\max_{\lambda}g''(\lambda) = g''(0)$ is not true for this variable $Y$.
\begin{rmk}
	Parameters of a sub-Gaussian distribution provide the exact value of the mean and an upper bound of the variance; i.e., $ \bbE[X] = \mu$ and $\var(X) \leq \sigma^2$. Suppose the moment generating function of variable  $X$ exists over the entire real interval. Then, the tail distribution of $X$ is bounded by a sub-Gaussian distribution with a proper choice of $\sigma^2$.
\end{rmk}



\subsection{Exercise 2.6}


\begin{lem}[Lower bounds on squared sub-Gaussians]\label{lem:lowersquare} Let $\{ X_i \}_{i=1}^n$ be an i.i.d.\ sequence of zero-mean sub-Gaussian variables with parameter $\sigma$. The normalized sum $Z_n = \frac{1}{n} \sum_{i=1}^n X_i^2$ satisfies
\begin{equation}\label{eq:lowersquare}
	\bbP[Z_n - \bbE[Z_n] \leq \sigma^2 \delta ] \leq e^{-n\delta^2 /16}, \quad \text{ for all } \delta \geq 0.
\end{equation}
	The equation~\eqref{eq:lowersquare} implies that the lower tail of the sum of squared sub-Gaussian variables behaves in a sub-Gaussian way.
\end{lem}

\begin{proof}
	Since $X_i^2$ are i.i.d.\ nonnegative variables, we apply the equation~\eqref{eq:thmonber0} to the variables $\{X_i^2\}_{i=1}^n$. Then, we have
	\begin{equation}\label{eq:e26a}
		\bbP\left[\sum_{i=1}^n (X_i^2 -\bbE[X_i^2]) \leq n \sigma^2 \delta\right] \leq \exp\left\{ - \frac{n\delta^2 \sigma^4}{ \bbE[X_1^4]} \right\},  \quad \text{for all }\delta \geq 0.
	\end{equation}
	By equation~\eqref{eq:thmmomsubg}, we have
	\begin{equation}\label{eq:e26x4}
		\bbE[X_1^4] \leq 16 \sigma^4. 
	\end{equation}
	Combing equations~\eqref{eq:e26a}, \eqref{eq:e26x4} and the definition of $Z_n$, we obtain 
	\begin{equation}
		\bbP[Z_n - \bbE[Z_n] \leq \sigma^2 \delta ]  \leq \exp\left\{ - \frac{n\delta^2}{16} \right\},  \quad \text{for all }\delta \geq 0.
	\end{equation}
\end{proof}

\begin{rmk}
	Equation~\eqref{eq:lowersquare} implies that the lower tail of the sum of squared sub-Gaussian variables behaves in a sub-Gaussian way. In following sections, we will show that the variable $Z_n - \bbE[Z_n]$ in Lemma~\ref{lem:lowersquare} is a sub-exponential variable.
\end{rmk}

\subsection{Exercise 2.7}
\begin{lem}[Bennett’s inequality]\label{lem:bennett}
	Let $X_1,...,X_n$ be a sequence of independent zero-mean random variables with $|X_i| \leq b$ and $\var(X_i) = \sigma_i^2$, for all $i \in [n]$. Then, we have the Bennett's inequality
	\begin{equation}\label{eq:bennett}
		\mathbb{P}\left[\sum_{i=1}^{n} X_{i} \geq n \delta\right] \leq \exp \left\{-\frac{n \sigma^{2}}{b^{2}} h\left(\frac{b \delta}{\sigma^{2}}\right)\right\}, \quad \text{for all }\delta \geq 0,
	\end{equation} 
	where $\sigma^2 = \frac{1}{n} \sum_{i=1}^n \sigma_i^2$ and $h(t) \colonequals (1+t)\log (1+t) - t$ for $t \geq 0$.
\end{lem}

\begin{proof}
	First, we consider the moment generating function of $X_i$, for all $i \in [n]$.
	
	\vspace{0.2cm}
	By power series, for all $i \in [n]$, we have
	\begin{align}\label{eq:e27power}
		\bbE\left[e^{\lambda X_i}\right] = \sum_{k = 0}^{+\infty} \frac{\lambda^k \bbE[X_i^k]}{k!} 
		= 1 + 0 + \sum_{k = 2}^{+\infty} \frac{\lambda^k \bbE[X_i^k]}{k!} \leq \exp\left\{ \sum_{k = 2}^{+\infty} \frac{\lambda^k \bbE[X_i^k]}{k!} \right\},
	\end{align}
	where the 0 comes from the fact that $\bbE[X_i] = 0$, and the last inequality follows from $1 + x \leq e^x$. By $|X_i|<b$, we bound the last term in equation~\eqref{eq:e27power} as follows
	\begin{equation}\label{eq:e27upper}
		\sum_{k = 2}^{+\infty} \frac{\lambda^k \bbE[X_i^k]}{k!}  \leq \sum_{k = 2}^{+\infty} \frac{\lambda^k \bbE[X_i^2 |X_i|^{k-2}] }{k!} \leq  \sum_{k = 2}^{+\infty} \frac{\lambda^k \sigma^2_i b^{k-2}}{k!} = \sigma_i^2  \left(  \frac{e^{\lambda b} - 1 -\lambda b}{b^2}   \right).
	\end{equation}
	Combing the equation~\eqref{eq:e27power} with equation~\eqref{eq:e27upper}, we obtain the following upper bound of the moment generating function of $\sum_{i=1}^n X_i$.
	\begin{equation}\label{eq:e27uppert}
		\bbE\left[e^{\lambda \sum_{i = 1}^n X_i }\right] = \prod_{i = 1}^n \bbE\left[e^{\lambda X_i}\right] \leq \exp\left\{ n \sigma^2 \left( \frac{e^{\lambda b} - 1 -\lambda b}{b^2}   \right)  \right\},
	\end{equation}
	where $\sigma^2 = \frac{1}{n} \sum_{i=1}^n \sigma_i^2$. Combing the Chernoff bound with equation~\eqref{eq:e27uppert}, the upper tail of $\sum_{i=1}^n X_i$ follows 
	\begin{align}
		\mathbb{P}\left[\sum_{i=1}^{n} X_{i} \geq n \delta\right] &\leq  \exp\left\{  n \sigma^2 \left(  \frac{e^{\lambda b} - 1 -\lambda b}{b^2}   \right)  - 
		\lambda n \delta \right\} \\
		&= \exp \left\{ \frac{n\sigma^2}{b^2} \left( e^{\lambda b} - \lambda b- \lambda \frac{\delta b^2}{\sigma^2} - 1 \right)  \right\}, \quad \text{for all } \delta \geq 0. \label{eq:e27exp}
	\end{align} 
	The upper bound \eqref{eq:e27exp} achieves the minimum when $\lambda = b^{-1}\log\left(1 + \frac{\delta b}{\sigma^2} \right)$ by the first-order condition of minimization. Plugging $\lambda =b^{-1} \log\left(1 + \frac{\delta b}{\sigma^2} \right)$ into the equation~\eqref{eq:e27exp}, we obtain the Bennett's inequality 
	\begin{equation}\label{eq:e27bennett}
		\mathbb{P}\left[\sum_{i=1}^{n} X_{i} \geq n \delta\right] \leq  \exp \left\{-\frac{n \sigma^{2}}{b^{2}} h\left(\frac{b \delta}{\sigma^{2}}\right)\right\}, \quad \text{for all }\delta \geq 0,
	\end{equation}
	where $h(t) \colonequals (1+t)\log (1+t) - t$ for $t \geq 0$.
	
	\vspace{0.2cm}
	Further, we show that the Bennett's inequality is at least as good as the Bernstein’s inequality.
	
	\vspace{0.2cm}
	The Bernstein's inequality for $\sum_{i=1}^n X_i$ is
	\begin{align}\label{eq:e27bern}
			\mathbb{P}\left[\sum_{i=1}^{n} X_{i} \geq n \delta \right] \leq \exp \left\{ \frac{-3n\delta^2}{(2 b \delta + 6 \sigma^2)} \right\}  %=\exp \left\{ -\frac{n \sigma^{2}}{ b^{2}} \frac{3b^2\delta^2}{2\sigma^2(b\delta + 6 \sigma^2)} \right\} 
			= \exp \left\{ -\frac{n \sigma^{2}}{ b^{2}} g\left(\frac{b\delta}{\sigma^2}\right) \right\}, \quad \text{for all } \delta \geq 0,
	\end{align}
	where $g(t) \colonequals  \frac{3t^2}{2t + 6} $ for $ t \geq 0$. Since $g(t)\leq h(t)$ holds for all $t \geq 0$, we conclude that the Bennett's inequality~\eqref{eq:e27bennett} is at least as good as Bernstein's inequality~\eqref{eq:e27bern}. 
\end{proof}

\begin{rmk} 
	So far, we have three inequalities controlling the tail of bounded variables: Hoeffding's inequality, Bernstein's inequality, and Bennett's inequality. Particularly, Hoeffding's inequality implies the sub-Gaussianity of bounded variables. As the proof for Lemma~\ref{lem:bennett} shows, Bennett's inequality is at least as good as the Bernstein's inequality, for bounded random variables.
\end{rmk}



\end{document}