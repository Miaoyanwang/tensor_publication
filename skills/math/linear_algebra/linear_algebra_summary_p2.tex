\documentclass[11pt]{article}

\usepackage{fancybox}


\usepackage{color}
\usepackage{url}
\usepackage[margin=1in]{geometry}

\setlength\parindent{0pt}
\renewcommand{\textfraction}{0.0}
\renewcommand{\topfraction}{1.0}
%\renewcommand{\textfloatsep}{5mm}

\usepackage{comment}
% Definitions of handy macros can go here
\usepackage{amsmath,amssymb,amsthm,bm,mathtools}
\usepackage{multirow}
\usepackage{dsfont,multirow,hyperref,setspace,natbib,enumerate}
%\usepackage{dsfont,multirow,hyperref,setspace,enumerate}
\hypersetup{colorlinks,linkcolor={blue},citecolor={blue},urlcolor={red}} 
\usepackage{algpseudocode,algorithm}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}



\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{assumption}{Assumption}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}


\setcounter{figure}{0}   
\setcounter{table}{0}  


\newcommand{\cmt}[1]{{\leavevmode\color{red}{#1}}}

\usepackage{dsfont}
\usepackage{multirow}

\DeclareMathOperator*{\minimize}{minimize}


\mathtoolsset{showonlyrefs}
\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}
\usepackage{xr}

\input macros.tex



\title{\textbf{Linear Algebra -- Part II}\\A summary for MIT 18.06SC}

\date{\today}
\author{%
Jiaxin Hu
}



\begin{document}

% Makes the title and author information appear.

\maketitle


% Abstracts are required.
\section{Orthogonality}

\subsection{Orthogonal vectors and subspaces}
\begin{defn}[\textit{Orthogonal vectors}]\label{def:vortho}
	Consider two vectors $x,y \in \bbR^{n}$. The vectors $x$ and $y$ are orthogonal,  denoted $x \perp y$, if and only if $x^T y = y^T x = 0$.
\end{defn}

\begin{defn}[\textit{Orthogonal subspaces}]\label{def:sortho}
	Consider two subspaces $S,T$. The subspaces $S$ and $T$ are orthogonal,  denoted $S \perp T$, if and only if for any $s \in S$ and $ t \in T$, $s^T t = t^T s = 0$.
\end{defn}

Given a matrix $\mA \in \bbR^{m \times n}$, there are four subspaces related to $\mA$\colon column space $C(\mA)$, row space $C(\mA^T)$, nullspace $N(\mA)$, and left nullspace $N(\mA^T)$. Suppose the rank of $\mA$ is $rank(\mA) = r$, the dimensions of these subspaces are: 
\[ dim(C(\mA)) = dim(C(\mA^T)) = r,\quad  dim(N(A)) = n-r,\quad dim(N(\mA^T)) = m-r. \] 

\begin{thm}[Orthogonality of matrix subspaces]\label{thm:ortho}
	Consider a matrix $\mA \in \bbR^{m \times n}$. The row space $C(\mA^T)$ and the nullspace $N(\mA)$ are orthogonal; the column space $C(\mA)$ and the left nullspace $N(\mA^T)$ are orthogonal, i.e.\
	\[ C(\mA^T) \perp N(\mA); \quad C(\mA) \perp N(\mA^T).  \]
\end{thm}

\begin{proof}
	Consider the matrix $\mA \in \bbR^{m \times n}$. For any vector $x \in N(\mA)$, we have $\mA x = 0$. 
	\begin{align}
		\mA x  = \begin{bmatrix}
			a_1^T x \\ \vdots \\ a_m^T x 
		\end{bmatrix} = \begin{bmatrix}
			0\\ \vdots \\0
		\end{bmatrix},
	\end{align}
	where $a_i \in \bbR^n, i \in [m]$ is the $i$-th  row of $\mA$. By definition~\ref{def:vortho}, $x$ is orthogonal to the rows in matrix $\mA$. For any vector $v \in C(\mA^T)$, $v$ is a linear combination of the rows $v = c_1 a_1 + ...+c_m a_m$, where $c_i, i\in [m]$ are constants. Multiplying vectors $v$ and $x$, we have
	\[  v^T x =  c_1 a_1^T x + \cdots + c_m a_m^T x = 0.  \]
	Therefore, $v \perp x$, and $N(\mA) \perp C(\mA^T)$.
	
	Similarly, for any $x \in N(\mA^T)$, we have $\mA^T x = 0$, which implies $N(\mA^T) \perp C(\mA)$. 
\end{proof}

\begin{thm}[Relationship between $\mA^T \mA$ and $\mA$]\label{thm:aa}
	Consider a matrix $\mA \in \bbR^{m \times n}$. We have
	\begin{equation}
		N(\mA^T \mA) = N(\mA) ; \quad rank(\mA^T \mA) = rank(\mA).
	\end{equation}
\end{thm}
\begin{proof}
	Consider a matrix $\mA \in \bbR^{m \times n}$. First, we prove that $N(\mA^T \mA) = N(\mA)$.
	
	On one hand, if $x \in N(\mA)$, then $\mA x = 0 \Rightarrow \mA^T  \mA x = \mA^T 0 = 0$. Therefore, for any $x \in N(\mA)$, the vector $x \in N(\mA^T \mA)$. 
	
	
	On the other hand, we  prove that for any $x \in N(\mA^T \mA)$ the vector $x \in N(\mA)$ by contradiction.
	
	Suppose there is a vector $x \in N(\mA^T \mA^T)$ but $x \notin N(\mA)$. We have
	\begin{equation}\label{eq:aaa}
		\mA x = b \neq 0, \quad \mA^T \mA x = 0 \quad \Rightarrow \quad \mA^T b = 0.
	\end{equation} 
	By the first equality in \eqref{eq:aaa}, $b \in C(\mA)$, and by the third equation in \eqref{eq:aaa}, $ b \in N(\mA^T)$. This contradicts the Theorem~\ref{thm:ortho}, $C(\mA) \perp N(\mA^T)$. Therefore, for any $x \in N(\mA^T \mA)$, the vector $x \in N(\mA)$.
	
	Next, given $N(\mA^T \mA) = N(\mA)$, we have $rank(\mA^T \mA) = n - dim(N(\mA^T \mA)) = n - dim(N(\mA)) = rank(\mA)$.
\end{proof}

\begin{cor}[Invertibility of $\mA^T \mA$]\label{cor:invert}
	If $\mA$ has independent columns, then $\mA^T \mA$ is invertible.
\end{cor}
\begin{proof}
	If $\mA \in \bbR^{m \times n}$ has independent columns, then $rank(\mA) = n$. By Theorem~\ref{thm:aa}, $rank(\mA^T \mA) = rank(\mA) = n$. Since $\mA^T \mA \in \bbR^{n \times n}$ is a square matrix, $\mA^T \mA$ is invertible.
\end{proof}


\subsection{Projections onto subspaces}
\begin{defn}[\textit{Projection and projection matrix}]\label{def:project}
	Given a vector $x \in \bbR^{n}$ and a matrix $\mA^{m \times n}$ with independent columns, the vector $p \in C(\mA)$, such that
	\begin{equation}\label{eq:project}
		(x-p) \perp C(\mA),
	\end{equation} 
	
	is the projection of vector $x$ onto the space $C(\mA)$. Since $p \in C(\mA)$, there exists a vector $\hat x$ such that $p = \mA \hat x$. By equation~\eqref{eq:project}, we have
	\[ \mA^T (x - p) = \mA^T(x - \mA \hat x) = 0  \quad  \Rightarrow\quad  \hat x = (\mA^T \mA)^{-1} \mA^T x,\quad p = \mA(\mA^T \mA)^{-1} \mA^T x.  \] 
	The matrix $\mP \overset{\Delta}{=}   \mA(\mA^T \mA)^{-1} \mA^T$ is called a projection matrix.
\end{defn}

\begin{thm}[Properties of projection matrix]\label{thm:projectm}
	Consider a projection matrix $\mP$ to the column space $C(\mA)$, where $\mA \in \bbR^{m \times n}$ is a matrix. Then,
	\begin{equation}\label{eq:projectm}
		\mP^T = \mP;\quad  \mP^2 = \mP.
	\end{equation}
\end{thm}
\begin{proof}
	Since the projection matrix $\mP =   \mA(\mA^T \mA)^{-1} \mA^T$ and $\mA^T \mA$ is symmetric, then
	\[  \mP^T = (\mA(\mA^T \mA)^{-1} \mA^T )^T = \mA (\mA^T \mA)^{-1} \mA^T = \mP. \]
	\[  \mP^2 = \mP^T \mP =  \mA (\mA^T \mA)^{-1} \mA^T  \mA (\mA^T \mA)^{-1} \mA^T =  \mA (\mA^T \mA)^{-1} \mA^T = \mP. \]
\end{proof}

\begin{cor}[Projection onto $N(\mA^T)$]\label{cor:iprojectm}
	Suppose $\mP$ is a projection matrix in Theorem~\ref{thm:projectm}, then $\mI-\mP$ is also a projection matrix to the left nullspace $N(\mA^T)$. 
\end{cor}

\begin{proof}
	For any $x \in \bbR^n$, we have
	\[ x - \mP x \perp C(\mA) \quad \Rightarrow \quad (I-\mP)x \perp C(\mA) \quad \Rightarrow \quad  (\mI-\mP)x \in N(\mA^T) \text{ and } (x - (\mI-\mP)x) \perp N(\mA^T). \]
	Therefore, $\mI-\mP$ is a projection matrix to the left nullspace $N(\mA^T)$.
\end{proof}

\subsection{Projection matrices and least squares}
Given an observation vector $y \in \bbR^n$ and a design matrix $\mX \in \bbR^{n \times (k+1)}$, we propose the linear regression model
\[ y = \mX \beta +\epsilon,\]
where $\beta = (\beta_0,\beta_1,...,\beta_k)$ are regression coefficients and $\epsilon$ is the noise. The least square estimate of $\beta$ is the minimizer of the loss, i.e.\
\[  \hat \beta_{LS} = \argmin_{\beta \in \bbR^{k+1}} \onorm{y - \mX \beta}^2, \]
where $\onorm{\cdot}$ is the euclidean norm. The vector $\mX 
\hat \beta_{LS}$ can be considered as a the projection of $y$ onto the column space of $\mX$, which minimizes the distance from $y$ to the column space $C(\mX)$. Therefore, we may use projection tools to solve the least square estimate. 

The projection $\mX \hat \beta_{LS}$ satisfies
\[ \mX^T( y - \mX \hat \beta_{LS} ) = 0 \quad \Rightarrow \quad \hat \beta_{LS} = (\mX^T \mX)^{-1} \mX^T y. \]

The estimate $\hat \beta_{LS}$ consists with the estimates solved by using the derivative. 

\subsection{Orthogonal matrices and Gram-Schimidt}
\begin{defn}[\textit{Orthonormal vectors}]\label{def:othronov}
	The vectors $q_1,...,q_n$ are orthonormal if
	\begin{align}
		q_i^T q_j = \begin{cases}
			0& \text{if } i\neq j\\
			1 & \text{if } i =  j
		\end{cases}.
	\end{align}
\end{defn}

Orthonormal vectors are always independent.

\begin{defn}[\textit{Orthonormal matrix and orthogonal matrix}]\label{def:orthonom}
	Consider a matrix $\mQ \in \bbR^{m \times n}$. If the columns of $\mQ$ are orthonormal, the matrix $\mQ$ is an orthonormal matrix. If $ m= n$, the square matrix $\mQ$ is an orthogonal matrix.
\end{defn}

If $\mQ \in \bbR^{m \times n}$ is an orthonormal matrix, $\mQ^T \mQ = \mI_n$.  If $\mQ$ is an orthogonal matrix, $\mQ^T = \mQ^{-1}$.
For orthonormal matrix $\mQ$, the projection matrix to $C(\mP)$  becomes $\mP = \mI_m$.

\begin{defn}[\textit{Gram-Schimidt Process and QR decomposition}]\label{gram-schimidt}
Consider a matrix $\mA \in \bbR^{m \times n}$ with $rank(\mA) = n$. Gram-Schimidt process finds the orthonormal basis for $C(\mA)$. Let $a_i \in \bbR^{m}, i \in [m]$ be the columns of matrix $\mA$. 
\begin{align}
	&u_1 = a_1, &&\quad e_1 = \frac{u_1}{\onorm{u_1}}\\
	&u_2 = a_2 - \frac{u_1^T a_2}{u_1^T u_1} u_1 ,&&\quad  e_2 = \frac{u_2}{\onorm{u_2}}\\
	&u_3 = a_3 - \frac{u_1^T a_3}{u_1^T u_1} u_1 - \frac{u_2^T a_3}{u_2^T u_2} u_2,&&\quad  e_2 = \frac{u_3}{\onorm{u_3}}\\
	\vdots
\end{align}
 The vectors $e_1,...,e_n$ are orthonormal basis of the $C(\mA)$. By matrix operations, we obtain a decomposition of matrix $\mA$
 \begin{equation}\label{eq:qr}
 	\mA = [a_1,...,a_n] = [e_1,....,e_2] \begin{bmatrix}
 		e_1^T a_1 & e_1^T a_2 & \cdots & e_1^T a_n\\
 		0&e_2^T a_2 & \cdots & e_2^T a_n\\
 		\vdots &\vdots & & \vdots \\
 		0 &0&\cdots & e_n^T a_n
 	\end{bmatrix} = \mQ \mR
 \end{equation}
	where $\mQ \in \bbR^{m \times n}$ is an orthonormal matrix and $\mR \in \bbR^{n \times n}$ is an upper triangular matrix. We call the matrix decomposition in equation~\eqref{eq:qr} QR decomposition.
\end{defn}

\section{Determinants}
The \textit{determinant} is a number associated with any square matrix. For a square matrix $\mA$, let $\det(\mA)$ or $|\mA|$ denote the determinant of matrix $\mA$.

\subsection{Properties of determinants}
We give ten properties of determinants. The last seven properties are deduced by first three basic properties.
\begin{enumerate}
	\item\label{prop1} The determinant of identity matrix is equal to 1, i.e.\ $\det(\mI) = 1$.
	\item\label{prop2} Exchanging two rows of a matrix reverses the sign of the matrix's determinant. Hence, an odd number of row exchanges reverse the sign of the determinant while an even number of row exchanges maintain the sign of the determinant.
	\item\label{prop3} \begin{enumerate} 
	\item[(a)] If one row of the matrix is multiplied by a constant $t$, the determinant of the matrix is multiplied by $t$.
	\begin{align}
		\begin{vmatrix}
			ta&tb\\c &d 
		\end{vmatrix} = t \begin{vmatrix}
			a&b\\c &d
			\end{vmatrix}
	\end{align} 
	\item[(b)] The determinant is linearly additive on the rows of the matrix.
	\begin{align}
		\begin{vmatrix}
			a+a'&b+b'\\c &d 
		\end{vmatrix} =  \begin{vmatrix}
			a&b\\c &d
			\end{vmatrix} +  \begin{vmatrix}
			a'&b'\\c &d
			\end{vmatrix} 
	\end{align}
	\end{enumerate}

	\item \label{propidenticalrow} If two rows of a matrix are equal, the determinant of the matrix is 0.  
	\begin{proof}
		Suppose $\mA$ has two identical rows $a_i, a_j$. The matrix after exchanging $a_i, a_j$, $\mA'$, is the same as the original matrix $\mA$. By property~\ref{prop2}, $\det(\mA) = -\det(\mA') = -\det(\mA)$. Therefore, $\det(\mA) = 0$.
	\end{proof}

	\item \label{propelim} For a square matrix $\mA \in \bbR^{n \times n}$, subtracting $t$ times of the $i$-th row from the $j$-th row does not change the determinant of $\mA$, where $i \neq j$.
	\begin{proof}
		By property~\ref{prop3} and property~\ref{propidenticalrow}, 
	\begin{align}
		 \begin{vmatrix}
			a & b \\ c - ta&d -tb
		\end{vmatrix} = \begin{vmatrix}
			a & b \\ c &d
		\end{vmatrix} -t \begin{vmatrix}
			a & b \\ a& b
		\end{vmatrix}  = \begin{vmatrix}
			a & b \\ c &d.
		\end{vmatrix}
	\end{align}
	The proof for higher-dimension matrix is similar.
	\end{proof}
	
	\item\label{prop0row} If the matrix $\mA$ has a row that is all zeros, the determinant of $\mA$ is 0.
	\begin{proof}
		By property~\ref{prop3}, letting $t = 0$ leads to property~\ref{prop0row}.
	\end{proof}

	
	\item\label{propdiag} Suppose $\mA \in \bbR^{n \times n}$ is a triangular matrix with diagonal elements $d_1,...,d_n$. Then, the determinant $\det(\mA) = \prod_{i = 1}^n d_i$. 
	\begin{proof}
		By property~\ref{propelim}, eliminating the triangular matrix $\mA$ to a diagonal matrix does not change the determinant of $\mA$. By property~\ref{prop3} and \ref{prop1}, $\det(\mA) = d_1 ...d_n \det(\mI_n) = d_1 ...d_n $. 
	\end{proof}

	
	\item\label{propsingular} If the square matrix $\mA$ is singular, the determinant of $\mA$ is 0.
	
	\begin{proof}
		If $\mA$ is a singular matrix, the reduced row echelon form of $\mA$, RREF($\mA$), has a row with all 0 entries. By property~\ref{propelim} and \ref{prop0row}, the determinant of $\mA$ is 0.
	\end{proof}

	
	\item\label{propproduct} For square matrices $\mA, \mB \in \bbR^{n \times n}$, the determinant of the product satisfies $\det(\mA \mB) = \det(\mA) \det(\mB).$
	
	\begin{proof}
		If at least one of $\mA, \mB$ is singular, the product $\mA \mB$ is also singular. Then, $\det(\mA \mB) = \det(\mA) \det(\mB) = 0$. If both $\mA, \mB$ are invertible, $\mA, \mB$ can be eliminated to diagonal matrices $\mD_{A}, \mD_{B}$ by elimination. Suppose $a_1,...,a_n$ and $b_1,...,b_n$ are diagonal elements for $\mD_A$ and $\mD_B$ respectively. By property~\ref{propdiag}, $\det(AB) = \prod_{i=1}^n a_i b_i = \det(\mA) \det(\mB)$. 
	\end{proof}
	
	\item\label{proptrans} For a square matrix $\mA$, the determinant $\det(\mA^T) = \det(\mA)$. 
	
	\begin{proof}
		By LU decomposition, the matrix $\mA = \mL \mU$, where $\mL$ is a lower-triangular matrix and $\mU$ is an upper-triangular matrix. The transport $\mL^T ,\mU^T$ have the same diagonal elements with $\mL, \mU$ respectively. By property~\ref{propdiag}, $\det(\mL^T)= \det(\mL),  \det(\mU^T) = \det(\mU)$. Therefore, by property~\ref{propproduct}, $\det(\mA) = \det(\mL) \det(\mU) = \det(\mU^T) \det(\mL^T) = \det(\mU^T \mL^T) = \det(\mA^T)$. 
			\end{proof}
\end{enumerate}

\subsection{Determinant computation}
\begin{prop}[Big formula for computing determinant]
	Consider a square matrix $\mA = \entry{a_{ij}} \in \bbR^{n \times n}$. The big formula for computing the determinant of $\mA$ is
	\[ \det(\mA) =  \sum_{(\alpha_1,...,\alpha_n) \in \tP} (-1)^{N(\alpha_1,...,\alpha_n)} a_{1,\alpha_1} ... a_{n \alpha_n},  \]
	where $(\alpha_1, \alpha_2, ...,\alpha_n)$ is the permutation of $(1,2,...,n)$, $\tP$ is the collection of all possible  $(\alpha_1,...,\alpha_n)$, and $N(\alpha_1,...,\alpha_n)$ is the number of necessary exchanges from $(\alpha_1,\alpha_2,...,\alpha_n)$  to $(1,2,...,n)$. 
\end{prop}

\begin{proof}
	For a square matrix $\mA = \entry{a_ij} \in \bbR^{n \times n}$, we decompose the  matrix $\mA$ by the summation of $n^n$ matrices, in which there is only one entry comes from $\mA$ in each row and other entries are 0. By property~\ref{prop3}, the determinant of $\mA$ is equal to the summation of the determinants of these $n^n$ matrices. Take a 3-by-3 square matrix as an example.
	\begin{align}
		\begin{vmatrix}
			a_{11} & a_{12} & a_{13}\\
			a_{21} & a_{22} & a_{23}\\
			a_{31} & a_{32} & a_{33}
		\end{vmatrix} = \begin{vmatrix}
			a_{11} & 0& 0\\
			a_{21} & 0 & 0\\
			a_{31} &0 & 0
		\end{vmatrix} + \begin{vmatrix}
			a_{11} & 0& 0\\
			0 & a_{22} & 0\\
			a_{31} &0 & 0
		\end{vmatrix} + \cdots +\begin{vmatrix}
			0 & 0& a_{13}\\
			0& 0 & a_{23}\\
			 0&0 & a_{33}
		\end{vmatrix} \overset{\Delta}{=} \mA_1 + \mA_2 +\cdots + \mA_{27}.
	\end{align}
	For the square matrix $\mA$,  there are $n!$ nonsingular matrices among the $n^n$ matrices. The number of nonsingular matrices coincides with the size of $\tP$. Intuitively, there are $n$ ways to choose an element from the first row, after which there are only $n-1$ ways to choose an element from the second row to avoid the zero determinant. Therefore, we have $n \times (n-1) \times (n-2) \times \cdots \times 2 = n!$ nonsingular matrices.
	
	By property~\ref{propdiag} and \ref{prop2}, the determinant for the nonsingular matrix follows the formula \[(-1)^{N(\alpha_1,...,\alpha_n)}  a_{1,\alpha_1} ... a_{n \alpha_n},\]
	where $(\alpha_1, \alpha_2, ...,\alpha_n)$ is the permutation of $(1,2,...,n)$, $\tP$ is the collection of all possible  $(\alpha_1,...,\alpha_n)$, and $N(\alpha_1,...,\alpha_n)$ is the number of necessary exchanges from $(\alpha_1,\alpha_2,...,\alpha_n)$  to $(1,2,...,n)$. 
\end{proof}


\begin{defn}[\textit{Cofactors, cofactor matrix, and cofactor formula}]
	Consider a square matrix $\mA=  \entry{a_{ij}} \in \bbR^{n \times n}$. Let $\mA_{-i,-j}$ be the submatrix of $\mA$ after removing the $i$-th row and $j$-th column. The cofactor associated with $a_{ij}$ is \[C_{ij} = (-1)^{i+j}\det(\mA_{-i,-j}).\]
	The matrix $\mC = \entry{C_{ij}} \in \bbR^{n \times n}$ is called cofactor matrix.  The cofactor formula of $\det(\mA)$ is 
	\[ \det(\mA) = \sum_{j}^n a_{ij} C_{ij} = \sum_{j}^n a_{ji} C_{ji} ,\quad \forall i \in [n].  \]
\end{defn}

\begin{example}[Tridiagonal matrix]
	One example of cofactor formula is computing the determinant of \textit{tridiagonal matrix}. A tridiagonal matrix is a matrix in which only nonzero elements lie on or adjacent to the diagonal. Let $\mT_n \in \bbR^{n \times n}$ denote the tridiagonal matrix of 1's, i.e.\
\begin{align}
	\mT_n = \begin{bmatrix}
		1 &1&0 &\cdots & 0 &0\\
		1&1&1&\cdots &0&0\\
		0&1&1&\cdots &0&0\\
		\vdots &\vdots &\vdots &&\vdots &\vdots \\
		0&0&0&\cdots&1&1
	\end{bmatrix}_{n \times n}
\end{align} 

Let $\mT_{n, -i,-j}$ be the submatrix of  $\mT_n$ after removing the $i$-th row and the $j$-th column.   By cofactor formula, 
\[  \det(\mT_n) = 1 \times \det(\mT_{n, -1,-1}) - 1 \times \det(\mT_{n,-1,-2}). \]
By the definition of tridiagonal matrix, $\mT_{n,-1,-1} = \mT_{n-1}$ and $\det( \mA_{n,-1,-2}) = 1 \times \det(\mT_{n-2}) $. Therefore, 
\[ \det(\mT_n) = \det(\mT_{n-1}) - \det(\mT_{n-2}).  \] 
\end{example}

\subsection{Inverse matrices}
Previously, we use Gauss-Jordan elimination to obtain the inverse matrix of an invertible matrix. Here, we apply cofactor formula to compute the inverse matrix.

\begin{thm}[Inverse matrix by cofactors]\label{thm:inver}
Let $\mA \in \bbR^{n \times n}$ be an invertible matrix and $\mC$ be the cofactor matrix of $\mA$. The inverse matrix of $\mA$ satisfies
\[  \mA^{-1} = \frac{1}{\det(\mA)} \mC^T. \]	
\end{thm}

\begin{proof}
	It is equivalent to  show that $\mA \mC^T = \det(\mA) \mI_n$. By the definition of cofactor matrix,
	\begin{align}
		\mA  \mC^T = \begin{bmatrix}
			a_{11} & a_{12} &\cdots &a_{1n}\\
			a_{21} & a_{22} &\cdots &a_{2n}\\
			\vdots & \vdots & & \vdots\\
			a_{n1} & a_{n2} &\cdots &a_{nn}\\
		\end{bmatrix} \begin{bmatrix}
			C_{11} & C_{21} &\cdots &C_{n1}\\
			C_{12} & C_{22} &\cdots &C_{n2}\\
			\vdots & \vdots & & \vdots\\
			C_{1n} & C_{2n} &\cdots &C_{nn}.\\
		\end{bmatrix}
	\end{align}
	
	Therefore, the diagonal elements of $\mA \mC^T$ are $(\mA \mC^T)_{ii} = \sum_k^n a_{ik} C_{ik} = \det(\mA), \forall i \in [n]$. Consider the off-diagonal entries $(\mA \mC^T)_{ij} = \sum_{k}^n a_{ik} C_{jk}$, where $i \neq j$. By cofactor formula, the summation   $ \sum_{k}^n a_{ik} C_{jk}$ is equal to the determinant of a matrix whose $i$-th row and $j$-th row are identical. By property~\ref{prop0row}, the off-diagonals of $\mA \mC^T$ are 0. Then, we have $\mA \mC^T = \det(\mA) \mI_n$.
\end{proof}

\begin{defn}[\textit{Cramer's rule}]
Consider the linear system $\mA x=b$, where $\mA\in\bbR^{n \times n}$ is an invertible matrix, and $ x, b \in \bbR^n$ are vectors. Applying Theorem~\ref{thm:inver}, we obtain the Cramer's rule of $x = \mA^{-1} b$.
\[  x = \frac{1}{\det(\mA)} \mC^T b; \quad x_j = \frac{\det(\mB_j)}{\mA_j}, \] 
	where $B_j$ the matrix that replaces the $j$-th column of $\mA$ by $b$.
\end{defn}

\begin{defn}[\textit{Parallelepiped}]
	The parallelepiped determined by $n$ vectors $v_1,...,v_n \in \bbR^n$ is the subset 
	\[ P = \{ a_1 v_1 + \cdots + a_n v_n \colon 0 \leq a_1,...,a_n \leq 1 \}. \]
	 We use $vol(P)$ to denote the volume of the parallelepiped  $P$. 
\end{defn}

\begin{prop}[Determinants and volumes]
The absolute determinant of a square matrix $\mA$ is the volume of parallelepiped determined by the rows of $\mA$, i.e.\
	\[ |\det(\mA)| = vol(P(\mA)), \]
	where $P(\mA)$ is the parallelepiped determined by the rows of $\mA$.
\end{prop}

\begin{example}
	The area of a triangle with vertices at $(x_1,y_1), (x_2,y_2), (x_3,y_2)$ is
	\begin{align}
		\frac{1}{2} \begin{vmatrix}
			x_1 & y_1 & 1\\
			x_2 & y_2 & 1\\
			x_3 & y_3 & 1
		\end{vmatrix}.
	\end{align}
	
	\begin{proof}
		The two edges of the triangle are $v_1 = (x_2 - x_1, y_2 -y_1)$ and $v_2 = (x_3 - x_1, y_3 -y_1)$. The area of the triangle is a half of the area of the parallelepiped determined by $v_1, v_2$. Therefore, the area of the triangle is 
		\begin{align}
			\frac{1}{2} \begin{vmatrix}
				x_2 - x_1 &  y_2 - y_1\\
				x_3 - x_1& y_3 -y_1
			\end{vmatrix} =  \frac{1}{2} \left(  \begin{vmatrix}
				x_2  &  y_2 \\
				x_3 & y_3
			\end{vmatrix} + \begin{vmatrix}
				x_1 &  y_1\\
				x_2& y_2
			\end{vmatrix}  - \begin{vmatrix}
				x_1 &  y_1\\
				x_3 & y_3 
			\end{vmatrix}    \right) = \frac{1}{2} \begin{vmatrix}
			x_1 & y_1 & 1\\
			x_2 & y_2 & 1\\
			x_3 & y_3 & 1
		\end{vmatrix}.
		\end{align}
	\end{proof}
\end{example}

\section{Eigenvalues and eigenvectors}
\begin{defn}[\textit{Eigenvalues and eigenvectors}]\label{def:eigen}
	Consider a square matrix $\mA$, an eigenvector of $\mA$ is a nonzero vector such that:
	\[ \mA x = \lambda x, \quad \text{for some } \lambda \in \mathbb{C}. \]
	The value $\lambda$ is called the eigenvalue of $\mA$, and $x$ is the eigenvector associated with eigenvalue $\lambda$. Usually, eigenvectors are normalized, i.e.\ $\onorm{x} = 1$, where $\onorm{\cdot}$ is the euclidean distance. For simplicity, all the eigenvectors mentioned below are normalized. Besides, the eigenvectors associated with eigenvalue 0 span the nullspace of $\mA$.
\end{defn}

\begin{defn}[\textit{Trace of square matrix}]\label{def:trace}
Consider a square matrix $\mA = \entry{a_{ij}} \in \bbR^{n \times n}$. The trace of $\mA$ is defined as
\[ tr(\mA) = \sum_{i}^n a_{ii}.\] 
\end{defn}

\begin{defn}[\textit{Characteristic polynomial}]\label{def:charac}
	Let $\lambda$ denote an eigenvalue of the matrix $\mA\in \bbR^{n \times n}$. Then the matrix $\mA - \lambda \mI$ is a singular matrix. Therefore,
	\[ \det(\mA - \lambda I_n) = 0 \quad  \Leftrightarrow \quad (-1)^n \lambda^n + tr(\mA)\lambda^{n-1} + \cdots + (-1)^n \det(\mA) \overset{\Delta}{=} P(\lambda) = 0.   \]
	 The polynomial $P(\lambda)$ is called the characteristic equation of $\mA$. 
\end{defn}

Since the characteristic polynomial $P(\lambda)$ is of degree $n$, there are $n$ solutions to the equation $P(\lambda) = 0$. The solutions $\lambda_1,...,\lambda_n$ are $n$ eigenvalues of $\mA$, which may be complex and may not be distinct with each other. Note that the complex eigenvalues come in conjugate pairs, because $\overline{ P(\lambda)} = P(\overline{ \lambda})$. 

\subsection{Properties for eigenvectors and eigenvalues}

\begin{thm}[Summation and production of eigenvalues]\label{thm:speigen}
	For a square matrix $\mA \in \bbR^{n \times n}$, let $\lambda_1,...\lambda_n$ denote the $n$ eigenvalues of $\mA$. Then 
	\begin{equation}
		\sum_{i=1}^n \lambda_i = tr(\mA); \quad \prod_{i=1}^n \lambda_i = \det(\mA).
	\end{equation} 
\end{thm}

\begin{proof}
	First, we re-write the characteristic polynomial $P(\lambda)$ of $\mA$ as
	\begin{equation}\label{eq:poly}
		P(\lambda) = (-1)^n (\lambda - \lambda_1)\cdots (\lambda - \lambda_n), 
	\end{equation}   
	where $\lambda_i, i \in [n]$ are $n$ eigenvalues of $\mA$. By equation~\eqref{eq:poly}, the coefficient for $\lambda^{n-1}$ is equal to $(-1)^n\sum_{i=1}^n \lambda_i$. Compared with definition~\ref{def:charac}, we have $tr(\mA) = \sum_{i=1}^n \lambda_i$. Similarly, the constant term in equation~\eqref{eq:poly} is $(-1)^n \prod_{i=1}^n \lambda_1 \cdots \lambda_n $. Therefore, $\prod_{i=1}^n \lambda_i = \det(\mA)$.
\end{proof}

\begin{defn}[\textit{Similar matrices}]\label{def:similar}
	Consider a square matrix $\mA \in \bbR^{n \times n}$. For any invertible matrix $\mB \in \bbR^{n \times n}$, the matrix product $\mB \mA \mB^{-1}$ is called the similar matrix of $\mA$.
\end{defn}

\begin{thm}[Eigenvalues for similar matrices]\label{thm:similar}
	Consider a sqaure matrix $\mA$. The similar matrices of $\mA$ share the same eigenvalues of $\mA$.
\end{thm}
\begin{proof}
	Let $\mB \mA \mB^{-1}$ be the similar matrix of $\mA$, and $\lambda_1,...,\lambda_n$ be the eigenvalues of $\mA$. Then, for any $\lambda_i$, there is an eigenvector, such that, 
	\[ \lambda x = \mA x \quad \Leftrightarrow \quad \lambda \mB x = \mB \mA \mB^{-1} \mB x.\]
	Since $x$ is a nonzero vector, and $\mB$ is invertible, the vector $\mB x$ is also a nonzero vector. Therefore, $\lambda$ is an eigenvalue of $\mB \mA \mB^{-1}$ with associated eigenvector $\mB x$.
\end{proof}

\begin{thm}[Eigenvalues for powers of the matrix]
	Consider a matrix $\mA$ with a eigenvalue $\lambda$ and the associated eigenvector $x$. For any polynomial $P$, 
	\[ P(\mA) x = P(\lambda) x. \]
\end{thm}
\begin{proof}
	For any integer $k \geq 0$ and constant $c \in \bbR$,
	\[ c\mA^k x = c\mA^{k-1} \mA x= c\lambda \mA^{k-1} x = \cdots = c\lambda^k x.\]
	Therefore, for any polynomial $P$, $P(\mA)x = P(\lambda)x$.
\end{proof}

\begin{thm}[Eigenvalues for the inverse]
Suppose $\mA \in \bbR^{n \times n}$ is an invertible matrix. Let $\lambda_1,...,\lambda_n$ denote the eigenvalue of $\mA$, and $x_1,...x_n$ denote the associated eigenvectors. Then,  $\lambda_1^{-1},...,\lambda_n^{-1}$ and $x_1,...x_n$ are the eigenvalues and associated eigenvectors of $\mA^{-1}$, respectively.
\end{thm}

\begin{proof}
	Since the nullspace of $\mA$ contains only a zero vector, the eigenvaluest $\lambda_i >0,  \forall i \in [n]$. For any $\lambda_i$ and $x_i$, multiplying $x_i$ on both sides of the equation $\mA^{-1} \mA = \mI_n$, we have 
	\[ \mA^{-1} \mA x_i = x_i \quad \Rightarrow\quad  \mA^{-1} x_i = {\lambda_i}^{-1} x_i.  \]
	Therefore, $\mA^{-1}$ share the same eigenvectors set with $\mA$, and the reciprocal of the eigenvalues of $\mA$ are the eigenvalues of $\mA^{-1}$.
\end{proof}

\begin{thm}[Independence of eigenvectors]\label{thm:indepeigen} 
Suppose the square matrix $\mA$ has at least two different eigenvalues. The eigenvectors associated with different eigenvalues are independent.
\end{thm}

\begin{proof}
	Suppose the square matrix $\mA \in \bbR^{n \times n}$ has $m$ different eigenvalues $\lambda_1,...,\lambda_m$ with associated eigenvectors  $v_1,...,v_m$. Let the linear combination $c_1 v_1 + ... + c_m v_m = 0$.	
	Then we have
	\begin{equation}\label{eq:indep1}
		\mA(c_1 v_1 + ... + c_m v_m ) = c_1 \lambda_1 v_1 + ... + c_m \lambda_m v_m = 0;  
		\end{equation}
		\begin{equation}\label{eq:indep2}
		\lambda_1 (c_1 v_1 + ... + c_m v_m ) = c_1 \lambda_1 v_1 + ... + c_m \lambda_1 v_m = 0. 
	\end{equation}
	Subtracting the equations~\eqref{eq:indep1} from \eqref{eq:indep2}, 
	\begin{equation}\label{eq:indep3}
		 c_2 (\lambda_1 - \lambda_2) v_2 + \cdots + c_m (\lambda_1 - \lambda_m) v_m  = 0
	\end{equation}
	Repeating the steps from equation~\eqref{eq:indep1} to equation~\eqref{eq:indep3}, finally we have 
	\[ c_m(\lambda_1 - \lambda_m)(\lambda_2 - \lambda_m)\cdots (\lambda_{m-1} - \lambda_m) v_m = 0.    \]
	 Since $\lambda_1 ,..., \lambda_m$ are distinct, and $v_m$ is a nonzero vector, we have $c_m = 0$.  Plugging $c_m = 0$ back to previous steps leads to that $c_1 = \cdots = c_m = 0$. Therefore, $v_1,...,v_m$ are independent.
\end{proof}

\begin{thm}[Eigenvalues for triangular matrices]\label{thm:trianeigen}
	For triangular matrices, the eigenvalues are the entries on the diagonal.
\end{thm}

\begin{proof}
Suppose $\mA = \entry{a_{ij}} \in \bbR^{n \times n}$ is a triangular matrix. By the property of determinant~\ref{propdiag}, 
\[ \det(\mA) = \prod_{i=1}^n a_{ii}  ; \quad \det(\mA - \lambda \mI_n) = \prod_{i=1}^n (a_{ii}-\lambda). \]
To let $\det(\mA - \lambda \mI_n) = 0$, $\lambda = a_{ii}, \forall i\in[n]$. Therefore, the entries on the diagonal are the eigenvalues of a triangular matrix.
\end{proof}


\subsection{Eigenvalues and eigenvectors for symmetric matrices}

\begin{thm}[Eigenvalues for symmetric and antisymmetric matrices]\label{thm:symmeign}
	Symmetric square matrices  with all real entries have real eigenvalues. For antisymmetric matrices which satisfy $\mA^T = - \mA$, all eigenvalues are imaginary, i.e.\ $\lambda = b i$, where $b \in \bbR, i = \sqrt{-1}$.
\end{thm}
\begin{proof}
	First, we prove that symmetric square matrices with all real entries have real eigenvalues by contradiction. 
	
	Suppose a symmetric real matrix $\mA$ has a complex eigenvalue $\lambda$ with eigenvector $x$. Since $\mA$ is real, then the eigenvector $x$ is also complex. Let $\overline \lambda$ and $\overline x$ denote the conjugate eigenvalue and eigenvector, respectively. Then we have
	\begin{equation}\label{eq:symm}
		\mA x = \lambda x;\quad \mA \overline x = \overline \lambda \overline x.
	\end{equation} 
	Premultiply $\overline x^T$ on both sides of the first equation in \eqref{eq:symm} and premultiply $x^T$ on both sides of the second equation in \eqref{eq:symm}. By the symmetry of $\mA$,
	\[ \overline x^T \mA x  = x^T \mA^T \overline x = x^T \mA \overline x .\]
	Subtracting the two equations in \eqref{eq:symm}, we have
	\[  0 = ( \overline x^T \mA x -  x^T \mA \overline x ) = (\lambda - \overline \lambda) x^T \overline x.  \]
	By the fact that $x^T \overline x$ is a real number, the imaginary part of $\lambda$ is equal to 0, which contradicts that $\lambda$ is complex.
	
	Next, we prove that the eigenvalues for antisymmetric matrices are imaginary. 
	Suppose $\mA$ is an antisymmetric matrix with a complex eigenvalue $\lambda$ and complex eigenvector $x$.  By similar procedures for symmetric matrices, we have
	\[ \overline x^T \mA x  = x^T \mA^T \overline x  =  -x^T \mA \overline x .  \] 
	Then,
	\[  0 = ( \overline x^T \mA x +  x^T \mA \overline x ) = (\lambda + \overline \lambda) x^T \overline x.   \]
	Since $x^T \overline x$ is a real number, the real part of $\lambda$ is equal to 0. Therefore, eigenvalues for antisymmetric matrices are imaginary.
\end{proof}

\begin{thm}[Orthogonality of eigenvectors for symmetric matrices]\label{thm:symmortho}
The eigenvectors of a symmetric matrix $\mA \in \bbR^{n \times n}$ associated with different eigenvalues are orthogonal.
\end{thm}


\begin{proof}
	Suppose $\lambda_1$ and $\lambda_2$ are two different eigenvalues of $\mA$ with associated eigenvector $x_1, x_2$, respectively. By the symmetry of $\mA$, we have
	\[ x_2^T \mA x_1 = x_1^T \mA^T x_2  = x_1^T \mA x_2,\]
	then 
	\[  0 = ( x_2^T \mA x_1 -  x_1^T \mA x_2 ) = (\lambda_1 - \lambda_2) x_2^T x_1. \]
	Since $\lambda_1, \lambda_2$ are different, we have $\lambda_1 - \lambda_2 \neq 0$ and $x_2^T x_1 = 0$. Therefore, $x_1$ and $x_2$ are orthogonal. 
\end{proof}

\begin{thm}[Eigenvectors for repeated eigenvalues of symmetric matrices]\label{thm:symmrepeat} Consider a symmetric matrix $\mA \in \bbR^{n \times n}$. If $\lambda_0$ is a repeated eigenvalue of $\mA$ with multiplicity $2 \leq m \leq n$, then there exist $m$ orthonormal eigenvectors corresponding to $\lambda_0$.
\end{thm}

\begin{proof}
	First, there is at least one nonzero eigenvector $x_0$ for $\lambda_0$. For any $x_0 \in \bbR^{n}$, there are $n-1$ additional orthogonal vectors $y_1,...,y_{n-1}$, such that $y_j \perp x_0, \forall j \in [(n-1)]$ and $(x_0, y_1,...,y_{n-1})$ forms the basis of $\bbR^n$. Let $\mY = [y_1,...y_{n-1}]$ and $\mX = [x_0,Y]$. Since $\mA$ is a symmetric matrix, $x_0^T \mA = \lambda_0 x_0^T $. We have
	\begin{align}\label{eq:symmrepeat}
		\mX^T \mA \mX =  \begin{bmatrix}
			x_0^T \mA x_0 & x_0^T \mA \mY\\
			\mY^T \mA x_0 & \mY^T \mA \mY 
		\end{bmatrix} =  \begin{bmatrix}
			\lambda_0 & 0\\
			0 & \mY^T \mA \mY 
		\end{bmatrix} .
	\end{align}
	Note that $\mX$ is an orthogonal matrix which satisfies $\mX^T = \mX^{-1}$. By Theorem~\ref{thm:similar}, $\mX^T \mA \mX$ is a similar matrix of $\mA$, and $\mX^T \mA \mX$ share the same eigenvalues of $\mA$. Then, $\lambda_0$ is also a repeated eigenvalue of $\mX^T \mA \mX$. The determinant of $\mX^T \mA \mX - \lambda \mI_n$ is 
	\[  \det(\mX^T \mA \mX - \lambda \mI_n) = (\lambda_0 - \lambda) \det(\mY^T \mA \mY - \lambda \mI_{n-1}). \]
	Since $\lambda_0$ has multiplicity $m \geq 2$, then $\det(\mY^T \mA \mY - \lambda_0 \mI_{n-1}) = 0$. Therefore, the dimension of the nullspace of $\mX^T \mA \mX - \lambda_0 \mI_n$ is larger than 2. Let $v_1,v_2$ be the two orthogonal vectors in the nullspace of $\mX^T \mA \mX - \lambda_0 \mI_n$. Then $v_1, v_2$ are two eigenvectors of $\mX^T \mA \mX$, and $\mX v_1, \mX v_2$ are two orthogonal eigenvectors of $\mA$ associated with $\lambda_0$. 
	
	Replace $x_0$ by two orthogonal vectors $x_1 = \mX v_1, x_2 = \mX v_2$ and repeat the steps above. Finally, we will have $m$ orthogonal eigenvectors of $\mA$ associated with $\lambda_0$.
\end{proof}

\subsection{Diagonalization}

\begin{defn}[Diagonalization]
	Suppose the square matrix $\mA \in \bbR^{n \times n}$ has $n$ independent eigenvectors. Then the matrix $\mA$ can be diagonalized as
	\[ \mA = \mS^{-1} \Lambda \mS, \]
	where $\mS \in \bbR{n \times n}$ is a matrix whose columns are $n$ independent eigenvectors of $\mA$ and $\Lambda \in \bbR^{n \times n}$ is a diagonal matrix whose entries are $n$ eigenvalues of $\mA$.
\end{defn}

\begin{thm}[Diagonalization of symmetric matrices]
Suppose $\mA$ is a symmetric matrix, then $\mA$ can be diagonalized.
\end{thm}

\begin{proof}
	By Theorems~\ref{thm:symmortho} and Theorem~\ref{thm:symmrepeat}, for any symmetric matrix $\mA \in \bbR^{n \times n}$, $\mA$ have $n$ orthogonal eigenvectors, even though there may have repeated eigenvalues.
\end{proof}

\begin{thm}[Descent of matrix powers]
	If a square matrix $\mA \in \bbR^{n \times n}$ has $n$ independent eigenvectors with eigenvalues $\lambda_1,...,\lambda_n$, then $\mA^k \rightarrow 0$ as $k \rightarrow +\infty$ if and only if all $|\lambda_i| < 1$.
\end{thm}

\begin{proof}
	We diagonalize the matrix $\mA$ as $\mA =\mS^{-1} \Lambda \mS$. For any positive integer $k$, $\mA^k = \mS^{-1} \Lambda^{k} \mS$. Then 
	\[ \lim_{k \rightarrow +\infty} \mA^k = \lim_{k \rightarrow +\infty} \mS^{-1} \Lambda^{k} \mS  =  \lim_{k \rightarrow +\infty} \mS^{-1} \begin{bmatrix}
		|\lambda_i|^k & \cdots &0\\
		\vdots & &\vdots\\
		0 & \cdots & |\lambda_n|^k
	\end{bmatrix} \mS  = 0 \quad \Leftrightarrow \quad  |\lambda_i| < 1, \forall i \in [n]. \]
\end{proof}



\end{document}