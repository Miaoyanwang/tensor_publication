\documentclass[11pt]{article}

\usepackage{fancybox}


\usepackage{color}
\usepackage{url}
\usepackage[margin=1in]{geometry}

\setlength\parindent{0pt}
\renewcommand{\textfraction}{0.0}
\renewcommand{\topfraction}{1.0}
%\renewcommand{\textfloatsep}{5mm}

\usepackage{comment}
% Definitions of handy macros can go here
\usepackage{amsmath,amssymb,amsthm,bm,mathtools}
\usepackage{multirow}
\usepackage{extarrows}
\usepackage{dsfont,multirow,hyperref,setspace,natbib,enumerate}
%\usepackage{dsfont,multirow,hyperref,setspace,enumerate}
\hypersetup{colorlinks,linkcolor={blue},citecolor={blue},urlcolor={red}} 
\usepackage{algpseudocode,algorithm}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}

\mathtoolsset{showonlyrefs=true}



\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{assumption}{Assumption}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}


\setcounter{figure}{0}   
\setcounter{table}{0}  


\newcommand{\cmt}[1]{{\leavevmode\color{red}{#1}}}

\usepackage{dsfont}
\usepackage{multirow}

\DeclareMathOperator*{\minimize}{minimize}



\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}
\usepackage{xr}

\input macros.tex



\title{Summary for Probability Theory}

\date{\today}
\author{%
Jiaxin Hu
}



\begin{document}

% Makes the title and author information appear.

\maketitle


% Abstracts are required.
\section{Preliminary}
\begin{itemize}
	\item \textit{DeMorgan's Laws:} Let $\{A_i \}_{i=1}^{\infty}$ be a collection of set. Then, $(\bigcup_{i = 1}^{\infty} A_i)^c = \bigcap_{i=1}^{\infty} A_i^c$ and   $(\bigcap_{i = 1}^{\infty} A_i)^c = \bigcup_{i=1}^{\infty} A_i^c$.
	\item \textit{Some set operations:} Suppose $A,B$ are two sets. (1) $A-B = A \cap B^c$; (2) $\bigcup_{i= 1}^{\infty} A_i = \{x: x\in A_i \text{ for some }i\}$,  $\bigcap_{i= 1}^{\infty} A_i = \{x: x\in A_i \text{ for all }i\}$.
\end{itemize}

\section{Single variable}
\subsection{Probability and conditional probability}
\begin{defn}[\textit{Sample space}]\label{def:samplespace}
     The set $S$ containing all possible outcomes is called the sample space.
\end{defn}

\begin{defn}[$\sigma$-\textit{field}]\label{def:sfield} 
	A collection $\tF$ of subsets of a sample space $S$ is called a $\sigma$-field (or $\sigma$-algebra) if and only if (\textbf{iff}) it has the following properties: \\(1) The empty set $\varnothing \in \tF$; \\(2) If $A \in \tF$, then the complement $A^c \in \tF$;\\ (3) If $A_i \in \tF, i = 1,2,...$, then their union $\cup_i A_i \in \tF$.\\
	If $A \in \tF$, then $A$ is called an \textit{event}.
\end{defn}

\begin{defn}[\textit{Measure and probability}]\label{def:measure}
	A set function $v$ defined on a $\sigma$-field $\tF$ is called a measure \textbf{iff} it has the following properties:\\
	(1) $0 \leq v(A) \leq \infty$ for any $A \in \tF$;
	\\ (2) $v(\varnothing) = 0$;
	\\(3) If $A_i \in \tF,i = 1,2,...$ and $A_i$'s  are disjoint, i.e.\ $A_i \cap A_j = \varnothing, \forall i\neq j$, then 
	\[ v(\bigcup_{i=1}^{\infty} A_i ) = \sum_{i=1}^{\infty} v(A_i). \]
	If $v(\tF) = 1$, then $v$ is a probability defined on $\tF$ and we use notation $P$ instead of $v$. 
\end{defn}

\begin{thm}[Probability]\label{thm:probability}
	Let the sample space $S = \{s_1, s_2,...\}$ and $\tF$ be all subsets of $S$. Let $p_1, p_2,...$ be non-negative numbers that $\sum_i p_i = 1 $. The following defines a probability on $\tF$
	\[ P(A) = \sum_{i: s_i \in A} p_i, \quad A \in \tF. \]
\end{thm}

\begin{thm}[Properties of probability]
Let $P$ be a probability, $A,B$ be events and $\{A_i\}_{i=1}^{\infty}$ be a collection of event. Let $\{C_i\}_{i=1}^{\infty}$ be a partition of sample space $S$, i.e.\ $C_i \cap C_j, \forall i\neq j$ and $\bigcup_{i=1}^{\infty} C_i = S$.   Then, 
\begin{align}
&1. P(A) \leq 1; P(A^c) = 1 - P(A); P(A) = P(A \cap B) + P(A \cap B^c);\\
&2. \text{If } A \subset B\text{, then }P(A) \leq P(B); \\
&3. \text{(General addition formula)}\\
&P(\bigcup_{i = 1}^n A_i) = \sum_{i = 1}^n P(A_i) - \sum_{i < j} P(A_i \cap A_j) + \sum_{i<j<k} P(A_i \cap A_j \cap A_k) + \cdots +(-1)^{n-1} P(A_1 \cap \cdots \cap A_n);\\	
&4. P(A) = \sum_{i=1}^{\infty} P(A \cap C_i); \\
&5. \text{(Boole's inequality) } P(\bigcup_{i=1}^{\infty} A_i) \leq \sum_{i=1}^{\infty} P(A_i);\\
&6. \text{(Bonferroni's inequality) } P(\bigcap_{i=1}^n A_i) \geq \sum_{i=1}^n P(A_i) - (n-1).
\end{align}
\end{thm}

\begin{defn}[\textit{Conditional Probability}]\label{def:condprobability}
If $A$ and $B$ are events with $P(B) > 0$, then the conditional probability of $A$ given $B$ is \[ P(A | B) = \frac{P(A\cap B)}{P(B)}. \]
For convenience, we define $P(A|B) = 0$ when $P(B) = 0$.
\end{defn}

\begin{thm}[Useful formulas for conditional probability]\label{thm:condprobability}
Let $A,\{A_i\}_{i=1}^{\infty}, B, \{B_i\}_{i=1}^{\infty},C$ be events. Then we have:
\begin{align}
	&1.  P(A|B) = \frac{P(A)P(B|A)}{P(B)}; \\
	&2. P(A^c|B) = 1- P(A|B); P(A \cup C|B) = P(A|B) + P(C|B) - P(A \cap C|B);\\
	&3. P(\bigcap_{i=1}^{n} A_i) = P(A_1) P(A_2|A_1)\cdots P(A_n|\bigcap_{i=1}^{n-1} A_i);\\
	&4. \text{If } \{B_i\}_{i=1}^{\infty} \text{is a partition of }S, P(A) = \sum_{i=1}^{\infty} P(B_i) P(A|B_i).
\end{align}
\end{thm}

\begin{thm}[Bayes formula]\label{thm:bayesformula}
Let $A$ be an event and $\{ B_i\}_{i=1}^{\infty}$ be a partition of $S$. Then,
\[  P(B_i|A) = \frac{P(A|B_i) P(A)}{\sum_{j=1}^{\infty} P(A|B_i) P(B_i) }.  \]
\end{thm}

\begin{defn}[\textit{Independence}]\label{def:indep}
Two events $A,B$ are independent \textbf{iff} 
\[ P(A\cap B) = P(A) P(B) \quad \text{or} \quad P(A|B) = P(A) \quad \text{or} \quad  P(B|A) = P(B).\]
\end{defn}
If $A,B$ are independent, then the following pairs are also independent: $A$ and $B^c$,  $A^c$ and $B$, $A^c$ and $B^c$.

\begin{defn}[\textit{Mutual  and pairwise independence}]\label{def:mindep} A collection of events $A_1,...,A_n$ are mutually independent \textbf{iff} for any sub-collection $A_{i_1},...,A_{i_k},$
\[ P(A_{i_1} \cap \cdots \cap A_{i_k}) = P(A_{i_1})\cdots P(A_{i_K}). \]
The events $A_1,...,A_n$ are pairwise independent \textbf{iff} $A_i$ and $A_j$ are independent for all $i\neq j$.

Mutual independence is stronger than pairwise independence.
\end{defn}

\begin{defn}[\textit{Conditional independence}]\label{def:cindep}
Events $A$ and $B$ are conditionally independent given event $C$ \textbf{iff} 
\[ P(A\cap B|C) = P(A|C)P(B|C).\]
\end{defn}

Let $A,B,C$ are events. Independence does not imply conditional independence:
\[ P(A\cap B) = P(A) P(B) \nRightarrow P(A\cap B|C) = P(A|C) P(B|C). \]
Conditional independence does not imply independence:
\[ P(A\cap B|C) = P(A|C) P(B|C) \nRightarrow P(A\cap B) = P(A) P(B).  \]
Mutual independence implies conditional independence:
\[ A,B,C\text{ mutually independent} \Rightarrow P(A\cap B|C) = P(A|C) P(B|C). \]

\subsection{Random variable and distribution}
\begin{defn}[\textit{(Random variable and distribution)}]\label{def:rv}
 A random variable  $X$ is a function from $S$ to $\bbR$ such that, for any Borel set $\tB \subset \bbR$,    \[ \{X \in \tB\} = \{\omega \in S: X(\omega) \in \tB \}. \]
The induced probability of $X$ is \[ P_X(\tB) = P(X \in \tB) = P(\omega \in \{\omega \in S: X(\omega) \in \tB \}).\]
The probability $P_X$ is called the distribution of $X$.
\end{defn}

\begin{defn}[\textit{Cumulative distribution function(cdf)}]\label{def:cdf}
 The cdf of a random variable $X$, denoted by $F_X(x)$, is defined as
\[ F_X(x) = P(X \leq x), \quad x \in \bbR.\]	
\end{defn}

\begin{thm}[Cdf]\label{thm:cdf} The function $F(x)$ is a cdf \textbf{iff}
\begin{align}
	&1. \lim_{x \rightarrow -\infty} F(x) = 0 \text{ and } \lim_{x \rightarrow \infty} F(x) = 1;\\
	&2. F(x) \text{ is non-decreasing in }x;\\
	&3. F(x) \text{ is right-continuous}: \lim_{\epsilon>0, \epsilon \rightarrow 0} F(x + \epsilon) = F(x), \quad \forall x \in \bbR.
\end{align}
	
\end{thm}

\begin{defn}[\textit{Continuity of random variable}]\label{def:contrv}
A random variable\ $X$ is continuous if $F_X(x)$ is continuous in $x$. A random variable\ $x$ is discrete if $F_X(x)$ is a step function of $x$.
\end{defn}

Note that the continuity of a random variable depends on the cdf rather than pdf or pmf. There are random variable's that are mixtures of these two types. 

\begin{defn}[\textit{Probability mass function(pmf)}]\label{def:pmf} The pmf of a discrete random variable\ $X$ is \[ f_X(x) = P(X = x), \quad x \in \bbR.\]
The cdf of $X$, $F_X(x) = P(X \leq x) = \sum_{k \leq x} f_X(k)$.
\end{defn}

\begin{defn}[\textit{Probability density function(pdf)}] The pdf of a continuous random variable\ $X$ is the function $f_X(x)$ such that
\[ F_X(x) = \int_{-\infty}^x f_X(t) dt, \quad x \in \bbR, \]
if $f_X(x)$ exists.  The continuous random variable $X$ has a pdf \textbf{iff} $F_X$ is absolutely continuous. If $f$ is a pdf, the set $\{x: f(x) >0\}$ is called its support.
\end{defn}

If $F_X$ is differentiable, then $f_X(x) = \frac{d}{dx} F_X(x)$. 

\begin{thm}[Pdf]\label{thm:pdf} A function $f(x)$ is a pdf \textbf{iff:}
\begin{align}
	&1. f(x) \geq 0, \quad \forall x \in \bbR;\\
	&2. \int_{-\infty}^{\infty} f(x) dx = 1.
\end{align}
\end{thm}

How to find pdf given cdf? (1) $f_X(x) = F'_X(x)$ for $x$ at which $F_X$ is differentiable; (2) $f_X(x)$ can be any $c \geq 0$ for $x$ at which $F_X$ is not differentiable.

\subsection{Transformation}
Let $X$ be a random variable and $Y= g(X)$, where $g$ is function $\bbR \mapsto \tY$ and $\tY$ is the domain of $Y$. For any $A \in \tY$,
\[  P(Y \in A) = P(g(X) \in A) = P(X \in g^{-1}(A)), \quad \text{where } g^{-1}(A) = \{ x: g(x) \in A \}. \]

Given $F_X$ or $f_X$, we want to obtain $f_Y(y)$. 
If $X$ is discrete, then
\[ f_Y(y) = \sum_{x\in g^{-1}(\{y\}) } P(X =x)  \]

If $f_X$ is continuous and $g$ is a continuously differentiable monotone function, then
\begin{align}
	f_Y(y) = \begin{cases}
		f_X(g^{-1}(y)) \vert \frac{d}{dy} g^{-1}(y)\vert & y \in \tY\\
		0 & \text{otherwise}
	\end{cases}
\end{align}

\begin{thm}[Transformation for continuous random variable]\label{thm:transf}
	Let $X$ be a continuous random variable\ with pdf $f_X$. Suppose there are disjoint $\{A_i\}_{i=1}^{k}$ such that $P(X \in \bigcup_{i = 1}^k A_i) = 1$, and $f_X$ is continuous on each $A_i, i \in [k]$. There are functions $g_1(x),...,g_k(x)$ defined on $A_i, i \in [k]$  respectively, satisfying
	
	1. $g(x) = g_i(x), \forall x \in A_i$;\\
	2. $g_i(x)$ is strictly monotone on $A_i$;\\
	3. The set $\tY = \{ y: y = g_i(x)  \text{ for some } x \in A_i\}$ is the same for each $i$;\\
	4. $g^{-t}_i (y)$ has a continuous derivative on $\tY$ for each $i$.\\
	Then 
	\begin{align}
		f_Y(y) = \begin{cases} \sum_{i = 1}^k 
		f_X(g_i^{-1}(y)) \vert \frac{d}{dy} g_i^{-1}(y)\vert & y \in \tY\\
		0 & \text{otherwise}
	\end{cases}
	\end{align}
\end{thm}

\begin{example} Suppose a random variable\ $X \sim N(0,1)$. Obtain the distribution of $Y = |X|$.
\end{example}

\begin{proof}
	Let $A_1 = (-\infty, 0)$, $A_2 =(0, +\infty )$ and $\tY = (0,+\infty)$. On $A_1$, $g_1(x) = x$ and $g_1^{-1}(x) = x$. On $A_2$, $g_2(x) = -x$ and $g_2^{-1}(x) = -x$. 
	
	By theorem~\ref{thm:transf}, 
	\[  f_Y(y) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2} + \frac{1}{\sqrt{2\pi}} e^{-x^2/2} =  \sqrt{ \frac{2}{\pi}} e^{-x^2/2}.  \]
\end{proof}

\subsection{Expectation}
\begin{defn}[\textit{Expectation}]\label{def:expectation}
The expected value or mean of a random variable\ $g(x)$ is 
\begin{align}
	\mathbb{E}[g(x)] = \begin{cases} 
 	\int_{-\infty}^{+\infty} g(x) f_X(x) dx \xlongequal{Y = g(x)} \int_{-\infty}^{+\infty} y f_Y(y) dx  & \text{if } X \text{has pdf }f_X\\
 	\sum_x g(x) f_X(x)\xlongequal{Y = g(x)} \sum_x y f_Y(y)  & \text{if } X \text{has pmf }f_X
 \end{cases}
\end{align} 
	provided that $\mathbb{E}[|g(x)|] < \infty$. Otherwise, the expected value of $g(X)$ does not exist.
\end{defn}

\begin{thm}[Properties of expectation]\label{thm:propexpect}
Let $X,Y$ be random variables whose expectations exist. Let $a,b,c$ be constants.\\
1. $\mathbb{E}(aX + bY + c) = a \mathbb{E}(X) + b \mathbb{E}(Y) + c$;\\
2. If $X \geq Y$, then $\mathbb{E} (X) \geq \mathbb{E} (Y) $.
\end{thm}

\begin{thm}[Relationship between expectation and cdf]\label{thm:expectcdf}
	Let $F$ be the cdf of a random variable $X$. If $X$ has a pdf or pmf, then,
	\[ \mathbb{E}[|X|] = \int_{0}^{+\infty} [1 - F(x)] dx + \int_{-\infty}^0 F(x) dx,  \]
	and $\mathbb{E}[|X|] < +\infty $ \textbf{iff} both integrals are finite. In case where $\mathbb{E}[|X|] < +\infty $ , 
	\[   \mathbb{E}[X] = \int_{0}^{+\infty} [1 - F(x)] dx - \int_{-\infty}^0 F(x) dx.  \]
\end{thm}
\begin{proof}
	Without the loss of generality, suppose random variable $X$ has a pdf. Let $f$ be the pdf of $X$. We have
	\begin{align}
		\mathbb{E}[|X|] &= \int_{0}^{+\infty} x f(x) dx - \int_{-\infty}^0 x f(x) dx\\
		&= \int_{0}^{+\infty} \int_{0}^{x}  f(x) dt dx + \int_{-\infty}^0 \int_{x}^0 f(x)dt dx \\
		&=  \int_{0}^{+\infty} \int_{t}^{+\infty}  f(x) dx dt + \int_{-\infty}^0 \int_{-\infty}^{t} f(x) dx dt\\
		&= \int_{0}^{+\infty} [1 - F(t)] dt + \int_{-\infty}^0 F(t) dt.
	\end{align}
		Therefore, $\mathbb{E}[|X|]< +\infty$ \textbf{iff} the two integrals are finite. Simiarly,
		\begin{align}
			\mathbb{E}[X] &= \int_{0}^{+\infty} x f(x) dx + \int_{-\infty}^0 x f(x) dx\\
		&= \int_{0}^{+\infty} [1 - F(t)] dt - \int_{-\infty}^0 F(t) dt.
		\end{align}
\end{proof}

\begin{cor}[Relationship between expectation and cdf]\label{cor:expectcdf}
Let $F$ be the cdf of a random variable $X$. If $X$ has a pdf or pmf, then,
\[  \mathbb{E}[|X|] =  \int_{0}^{+\infty} P(X > x) + P(-X \leq x) dx, \] 
	and \[  \sum_{n=1}^{\infty} P(|X| \geq n) \leq \mathbb{E}[|X|] \leq 1 + \sum_{n=1}^{\infty} P(|X| \geq n). \]
\end{cor}
\begin{proof}
	By the proof of theorem~\ref{thm:expectcdf}, 
	\begin{align}
		\mathbb{E}[|X|] &= \int_{0}^{+\infty} [1 - F(t)] dt + \int_{-\infty}^0 F(t) dt\\
		&= \int_{0}^{+\infty} [1 - F(t)] dt + \int_{0}^{+\infty} F(-t) dt\\
		&= \int_{0}^{+\infty}  P( X > t ) + P( -X \leq t )  dt
	\end{align}
	To show $ \sum_{n=1}^{\infty} P(|X| \geq n) \leq \mathbb{E}[|X|]$, we have
	\begin{align}
		\mathbb{E}[|X|] &= \int_{0}^{+\infty}  P( X > t ) + P( -X \leq t )  dt\\
		&\geq \int_{0}^{+\infty}  P(| X |> t )  dt\\
		&= \sum_{n = 0}^{+\infty} \int_{n}^{(n+1)}  P(| X |> t )  dt\\
		&\geq \sum_{n = 0}^{+\infty} \int_{n}^{(n+1)}  P(| X |\geq (n+1) )  dt\\
		&= \sum_{n=1}^{+\infty}  P(| X |\geq n ) 
	\end{align} 
	To show $\mathbb{E}[|X|] \leq 1 + \sum_{n=1}^{\infty} P(|X| \geq n)$, we have
	\begin{align}
		\mathbb{E}[|X|] &= \int_{0}^{+\infty}  P( X > t ) + P( -X \leq t )  dt\\
		&\leq \int_{0}^{+\infty}  P(| X |\geq  t )  dt\\
		&= \sum_{n = 0}^{+\infty} \int_{n}^{(n+1)}  P(| X |\geq t )  dt\\
		&\leq \sum_{n = 0}^{+\infty} \int_{n}^{(n+1)}  P(| X |\geq n )  dt\\
		&\leq 1+ \sum_{n = 1}^{+\infty}  P(| X |\geq n ) .
	\end{align}
\end{proof}


\end{document}