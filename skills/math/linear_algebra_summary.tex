\documentclass[11pt]{article}

\usepackage{fancybox}


\usepackage{color}
\usepackage{url}
\usepackage[margin=1in]{geometry}

\setlength\parindent{0pt}
\renewcommand{\textfraction}{0.0}
\renewcommand{\topfraction}{1.0}
%\renewcommand{\textfloatsep}{5mm}

\usepackage{comment}
% Definitions of handy macros can go here
\usepackage{amsmath,amssymb,amsthm,bm,mathtools}
\usepackage{multirow}
\usepackage{dsfont,multirow,hyperref,setspace,natbib,enumerate}
%\usepackage{dsfont,multirow,hyperref,setspace,enumerate}
\hypersetup{colorlinks,linkcolor={blue},citecolor={blue},urlcolor={red}} 
\usepackage{algpseudocode,algorithm}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}



\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{assumption}{Assumption}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}


\setcounter{figure}{0}   
\setcounter{table}{0}  


\newcommand{\cmt}[1]{{\leavevmode\color{red}{#1}}}

\usepackage{dsfont}
\usepackage{multirow}

\DeclareMathOperator*{\minimize}{minimize}


\mathtoolsset{showonlyrefs}
\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}
\usepackage{xr}

\input macros.tex



\title{\textbf{Linear Algebra}\\A summary for MIT 18.06SC}

\date{\today}
\author{%
Jiaxin Hu
}



\begin{document}

% Makes the title and author information appear.

\maketitle


% Abstracts are required.
\section{Matrices \& Spaces}
\subsection{Basic concepts}
\begin{itemize}
    \item Given vectors $v_1 ,...,v_n$ and scalars $c_1, ..., c_n$, the sum $c_1 v_1 + \dots + c_n v_n$ is called a \textit{linear combination} of $v_1,...,v_n$.
    \item The vectors  $v_1 ,...,v_n$  are \textit{linearly independent} (or just \textit{indepedent}) if $c_1 v_1 + \dots + c_n v_n = 0$ holds only when all  $c_1,...,c_n = 0$. If the vectors  $v_1 ,...,v_n$  are \textit{dependent}, there exist scalars $c_1, ..., c_n$ which are not all equal to 0 such that $c_1 v_1 + \dots + c_n v_n = 0$.
	\item Given a matrix $\mA \in \bbR^{m \times n}$ and a vector $x \in \bbR^{n}$  the multiplication $\mA x$ is a linear combination of the columns of $\mA$, and $x^T \mA$ is a linear combination of the rows of $\mA$.
	\item Matrix multiplication is typically not communicative, i.e. $\mA \mB \neq \mB \mA$. Lemma~\ref{lem:matrixcomm} describes a special case where matrix multiplication is communicative.
	\item Suppose $\mA$ is a square matrix. The matrix $\mA$ is \textit{invertible} or \textit{non-singular} if there exists a $\mA^{-1}$ such that $\mA^{-1} \mA = \mA \mA^{-1} = I$. Otherwise, the matrix $\mA$ is \textit{singular}, and the determinant of $\mA$ is 0. 
	\item The inverse of a matrix product $\mA\mB$ is $(\mA \mB)^{-1} = \mB^{-1} \mA^{-1}$. The product of invertible matrices is still invertible.
	\item The transpose of a matrix product  $\mA\mB$ is $(\mA \mB)^{T} = \mB^{T} \mA^{T}$. For any invertible matrix $\mA$, $(\mA^T)^{-1} = (\mA^{-1})^T$.
	\item A matrix $\mQ$ is orthogonal if $\mQ^T = \mQ^{-1}$. A matrix $\mQ$ is unitary if $\mQ^* = \mQ^{-1}$, where $\mQ^*$ is the conjugate transpose of $\mQ$.
\end{itemize}

\begin{lem}[Communicative matrix multiplication]\label{lem:matrixcomm} For matrices $\mA \in \bbR^{n \times n}$ and $\mB \in \bbR^{n \times n}$, the matrix multiplication of $\mA$ and $\mB$ is communicative, i.e. $\mA\mB  = \mB \mA$, if all the normalized eigenvectors of $\mA$ and $\mB$ are identical.	
\end{lem}

\begin{proof}
	If all the normalized eigenvectors of $\mA$ and $\mB$ are identical, there exists a matrix $\mQ \in \bbR^{n \times n}$ such that
	\[ \mA = \mQ \mD_A \mQ^{-1}, \quad \mB = \mQ \mD_B \mQ^{-1},\]
	where columns of $\mQ$ are normalized eigenvectors of $\mA$ and $\mB$, and $\mD_A\in\bbR^{n \times n},  \mD_B\in\bbR^{n \times n} $ are diagonal matrices whose diagonal elements are corresponding eigenvalues of $\mA$ and $\mB$. Because matrix multiplication is communicative for two diagonal matrices with same dimensions, we have
	\[  \mA \mB = \mQ \mD_A \mQ^{-1} \mQ \mD_B \mQ^{-1} = \mQ \mD_A \mD_B \mQ^{-1}  =\mQ \mD_B \mD_A \mQ^{-1} = \mQ \mD_B \mQ^{-1} \mQ \mD_A \mQ^{-1}  = \mB \mA .\]
	Therefore, the matrix multiplication of $\mA$ and $\mB$ is communicative.
\end{proof}

\subsection{Permutation of matrices}
For any matrix $\mA$, we swap its rows by multiplying a \textit{permutation matrix } $\mP$ on the left of $\mA$. For example,
\begin{align}
	\mP \mA = \begin{bmatrix}
		0 &0&1\\1 & 0 & 0\\ 0 & 1 & 0 
	\end{bmatrix}  \begin{bmatrix}
		a_1\\a_2\\ a_3
	\end{bmatrix}  = \begin{bmatrix}
		a_3\\a_1\\ a_2
	\end{bmatrix}
\end{align}
where $a_k$ refers to the $k$-th row of $\mA$. The inverse of permutation matrix $\mP$ is $\mP^{-1} = \mP^T$, which implies the orthogonality of permutation matrix. For an $n \times m$ matrix, there are $n!$ different row permutation matrices, which form a \textit{multiplicative group}.

\vspace{.2cm}
Similarly, we also swap the columns of the matrix $\mA$ by multiplying a permutation matrix on the right of $\mA$.

\subsection{Elimination of matrices}
Elimination is an important technique in linear algebra. We eliminate the matrix by multiplications and subtractions. Take a 3-by-3 matrix $\mA$ as an example.
\begin{align}
	\mA = \begin{bmatrix}
		1 &2&1\\3 & 8 & 1\\ 0 & 4 & 1 
	\end{bmatrix}
	\xrightarrow{\text{step 1}} \begin{bmatrix}
		1 &2&1\\0 & 2 & -2\\ 0 & 4 & 1 
	\end{bmatrix}
	\xrightarrow{\text{step 2}} \mU =  \begin{bmatrix}
		1 &2&1\\0 & 2 & -2\\ 0 & 0 & 5
	\end{bmatrix}
\end{align}
In step 1, we choose the number 1 in row 1 column 1 as a \textit{pivot}, then we recopy the first row, multiply an appropriate number (in this case, 3), and subtract those values from the numbers in the second row. We have thus eliminated 3 in row 2 column 1. Similarly, in step 2, we choose 2 in row 2 column 2 as a pivot, and eliminate the number 4 in row 3 column 2.  The number 5 in row 3 column 3 is also a pivot. The matrix $\mU$ is an upper traingular matrix.

\vspace{.2cm}
The \textit{elimination matrix} used to eliminate the entry in row $m$ column $n$ is denoted $\mE_{mn}$. In the previous example, 
\begin{align}
\mE_{21} \mA = 
\begin{bmatrix}
		1 &0&0\\-3 & 1 & 0\\ 0 & 0 & 1 
	\end{bmatrix}
	\begin{bmatrix}
		1 &2&1\\3 & 8 & 1\\ 0 & 4 & 1 
	\end{bmatrix} = \begin{bmatrix}
		1 &2&1\\0 & 2 & -2\\ 0 & 4 & 1 
	\end{bmatrix};\quad \mE_{32} (\mE_{21} \mA) = \mU.
\end{align}

Pivots are non-zero. If there is a 0 in the pivot position, we must exchange the row with one below to get a non-zero value in pivot position. If there is not non-zero value below the 0 pivot, we skip this column, and find a pivot in next column. 

\vspace{.2cm}
We write $\mE_{32} (\mE_{21} \mA) = (\mE_{32} \mE_{21} ) \mA = \mU$ because matrix multiplication is associative. Let $\mE$ denote the product of all elimination matrices. If we need to permute the rows during the process, we multiply a permutation matrix on the left of $\mA$. Therefore, the elimination process of $\mA$ is
\begin{equation}\label{eq:factor}
	\mE \mP \mA = \mU,
\end{equation}
 where  $\mU$ is an upper triangular matrix.


\vspace{.2cm}
Next, we prove the invertibility of the elimination matrix.
\begin{lem}[Invertiblity of elimination matrix]\label{lem:elim}
 Suppose there is an elimination matrix $\mE_{ij} \in \bbR^{n \times n}$ that multiplies a scalar $-c$ to the  $j$-th row, and subtracts the row from $i$-th row, where $i \neq j$. The matrix $\mE_{ij}$ is invertible.
\end{lem}
\begin{proof}
	We write the elimination matrix as
	\[ \mE_{ij} = \mI_n + c e_i e_{j}^T, \]
	where $e_i \in \bbR^n$ denotes the vector with value 1 in the $i$-th entry and value 0 elsewhere. Note that $e^T_i e_{j} = 0$ because $i \neq j$. We have 
	\[ (\mI_n + c e_i e_{j}^T)(\mI_n - c e_i e_{j}^T)  = \mI_n - c^2 e_i e_{j}^T e_i e_{j}^T = \mI_n; \quad (\mI_n - c e_i e_{j}^T)(\mI_n + c e_i e_{j}^T)= \mI_n. \]
	Therefore, $\mI_n - c e_i e_{j}^T$ is the inverse of $\mE_{ij}$. The elimination matrix $\mE_{ij}$  is invertible.
\end{proof}

\begin{cor}[Inverse of elimination matrix]\label{cor:invelim} 
Suppose the elimination matrix $\mE_{ij}$ in lemma~\ref{lem:elim} is a lower/upper-triangular matrix. The inverse $\mE_{ij}^{-1}$ is also a lower/upper-triangular matrix.
\end{cor}
\begin{proof}
	By the proof of lemma~\ref{lem:elim}, the matrix $\mE_{ij}$ and its inverse are written as
	\[ \mE_{ij} = \mI_n + c e_i e_{j}^T, \quad \mE^{-1}_{ij} = \mI_n - c e_i e_{j}^T.\]
	WLOG, we assume $\mE_{ij}$ is a lower-triangular matrix. Then $c e_i e_{j}^T$ and  $-c e_i e_{j}^T$ are also lower-triangular matrices. Therefore, $\mE_{ij}^{-1}$ is a lower-triangular matrix.
\end{proof}



\subsection{Gauss-Jordan Elimination}
We also use elimination to find the inverse of any invertible matrix.

\vspace{0.2cm}
 Suppose $\mA \in \bbR^{n \times n}$ is an invertible matrix. The inverse of $\mA$, $\mA^{-1}$ , satisfies 
 \begin{equation}\label{eq:inv}
 	\mA \mA^{-1} = \mI_n.
 \end{equation}
 Suppose there is an elimination $\mE$ such that $\mE \mA = \mI_n$. Multiplying $\mE$ on the both side of the equation~\eqref{eq:inv}, we have $\mE  \mA \mA^{-1}= \mA^{-1} = \mE$. To obtain an such $\mE$, we eliminate the \textit{augmented matrix} $[\mA|\mI_n]$ until $\mA$ becomes $\mI_n$. Then, the augmented matrix becomes $\mE[\mA|\mI_n] = [\mI_n|\mE]$, where $\mE$ is the inverse of $\mA$.
 
 \vspace{0.2cm}
We call this elimination process of finding $\mE$ as \textit{Gauss-Jordan Elimination}.



\subsection{Factorization of matrices}
By elimination, for any square matrix $\mA$, we have equation~\eqref{eq:factor}.
 By lemma~\ref{lem:elim}, $\mE$ is invertible. We multiply $\mE^{-1}$ on both sides of equation~\eqref{eq:factor}. We have, 
\[ \mP \mA = \mE^{-1} \mU. \]
 Note that $\mE$ is a lower-triangular matrix. By corollary~\ref{cor:invelim}, $\mE^{-1}$ is also a lower-triangular matrix. Let $\mL$ denote $\mE^{-1}$, where the letter $L$ refers to ``lower triangular". Therefore, any square matrix $\mA$ has a factorization:
\begin{equation}\label{eq:LUfact}
	\mP \mA = \mL \mU,
\end{equation}
where $\mU$ is an upper triangular matrix with pivots on the diagonal, $\mL$ is lower triangular matrix with ones on the diagonal, and $\mP$ is a permutation matrix. However, the equation~\eqref{eq:LUfact} is not the unique factorization of $\mA$. For example, $c\mL$ and $c^{-1}\mU$ also factorize $\mA$, where $c$ is a non-zero scalar.

\subsection{Time complexity of elimination}
For an $n$-by-$n$ matrix, a single elimination multiplies a selected row and subtracts the selected row from another row. A single elimination requires $\tO(n)$ operations. To eliminate the elements below the first diagonal element, we need repeat single eliminations $(n-1)$ times,  and thus require $\tO(n^2)$ operations. Similarly, we require $\tO((n-1)^2)$  to eliminate the elements below the second diagonal element. Repeat the elimination until we meet the $n$-th diagonal element. Therefore, we require $\tO(n^3)$ operations to obtain an upper-triangular matrix by elimination:
 \[ 1^2 + 2^2 + \cdots +(n)^2 = \sum_{i}^{n} i^2 \approx \int_{0}^{n} x^2 dx = \frac{1}{3} n^3 = \tO(n^3).\]


\subsection{Reduced row echelon form of matrices}
In previous sections, we convert any matrix $\mA$ to an upper triangular matrix $\mU$. Next, we convert $\mU$ into \textit{reduced row echelon form} (RREF), which is a simpler form than upper triangle. We use $\mR = RREF(\mA)$ to denote the reduced row echelon form of $\mA$. In $\mR$, the pivots are equal to 1, and the elements above and below the pivots are eliminated to 0. In the previous example,
\begin{align}
	\mU = \begin{bmatrix}
		1 &2&1\\0 & 2 & -2\\ 0 & 0 & 5
	\end{bmatrix} \xrightarrow{\text{make pivots = 1}} \begin{bmatrix}
		1 &2&1\\0 & 1 & -1\\ 0 & 0 & 1
	\end{bmatrix} 
	\xrightarrow{\text{0 above and below pivots}} \mR =  \begin{bmatrix}
		1 &0&0\\0 & 1 & 0\\ 0 & 0 & 1
	\end{bmatrix} .
\end{align}

There is another example,
\begin{align}
	\mU = \begin{bmatrix}
		1 &2&2&2\\0 &0 & 2 & 4\\ 0 & 0 & 0&0
	\end{bmatrix} \xrightarrow{\text{make pivots = 1}} \begin{bmatrix}
		1 &2&2&2\\0 &0 & 1 & 2\\ 0 & 0 & 0&0
	\end{bmatrix} 
	\xrightarrow{\text{0 above and below pivots}} \mR =  \begin{bmatrix}
		1 &2&0&-2\\0 &0 & 1 & 2\\ 0 & 0 & 0&0
	\end{bmatrix} .
\end{align}

Assume there are $r$ pivots in $\mA \in \bbR^{m \times n}$. With proper permutation, the matrix $\mR$ is in form $\begin{bmatrix} \mI_r&\mF\\0 &0	\end{bmatrix}$, where $\mF \in \bbR^{ r \times (n-r)}$ is an arbitrary matrix. The columns in $\mA$ which correspond to the identity matrix $\mI_r$ are called \textit{pivot columns}. The other columns are \textit{free columns}.

\subsection{Vector space, Subspace and Column space}
\begin{itemize}
	\item \textit{Vector space} is a collection of vectors that is closed under linear combination (addition and multiplication by any real number); i.e. for any vectors in the collection, all the combinations of these vectors are still in the collection.
	\item \textit{Subspaces of the vector space} is a vector space that is contained inside of another vector space.
\end{itemize}

Note that any vector space or subspace must include an origin. For a vector space $\tA$, the subspace of $\tA$ can be $\tA$ itself or a set that contains only a zero vector. 

\begin{itemize}
	\item Vectors $v_1 ,...,v_n$ \textit{span} a space that consists all the combination of those vectors. 
	\item \textit{Column space} of matrix $\mA$ is the space spanned by the columns of $\mA$. Let $C(\mA)$ denote the column space of $\mA$.
\end{itemize}

If  $v_1 ,...,v_n$ span a space $\tS$, then $\tS$ is the smallest space that contain those vectors.

\begin{itemize}
	\item \textit{Basis} of a vector space is a sequence of vectors $v_1 ,...,v_n$ that satisfy: (1) $v_1 ,...,v_n$ are independent; (2) $v_1 ,...,v_n$ span the space.
	\item \textit{Dimension} of the space is the number of vectors in a basis of the space. Let $dim(\tA)$ denote the dimension of space $\tA$.
\end{itemize}

\subsection{Matrix rank}
The \textit{rank} of a matrix $\mA$ is defined as the dimension of the columns space of $\mA$. Rank of matrix $\mA$ is also equal to the number of pivot columns of $\mA$. Let $rank(\mA)$ denote the rank of matrix $\mA$. We have
\begin{equation}\label{eq:rank}
	rank(\mA) = \# \text{ of pivot columns of }\mA = dim(C(\mA)).
\end{equation}
If $\mA\in \bbR^{m \times n}$ and $rank(\mA) = r$ , we have $r \leq \min\{m,n\} $. We say the matrix is \textit{full rank} if $r = \min\{m,n\}$.

\vspace{.2cm}
The rank of a square matrix is closely related to its invertibility. 
\begin{lem}[Full rankness and invertibility]\label{lem:fullrank} A square matrix $\mA \in \bbR^{n \times n}$ is full rank, if and only if $\mA$ is an invertible matrix.
\end{lem}

\begin{proof}
	First, we assume $\mA$ is full rank and prove the invertibility of $\mA$.
	
	Consider the RREF form of $\mA$, $\mR = RREF(\mA)$. There are elimination matrix $\mE$ and permutation matrix $\mP$ such that
	\[ \mE \mP \mA =  \mR. \]
	By the full rankness of $\mA$,  $\mA$ has $n$ pivot columns, and thus $\mR = \mI_n$. By lemma \ref{lem:elim}, $\mE$ is invertible. The permutation matrix $\mP$ is also invertible. Then, the matrix product $\mE \mP$ is invertible and $\mA$ is the inverse of $\mE \mP$. Therefore, $\mA$ is invertible.
	
	Second, we assume $\mA$ is invertible and prove the full rankness of $\mA$ by contradiction. 
	
  Assume $\mA$ has an inverse $\mA^{-1}$ and $rank(\mA) < n$. By equation~\eqref{eq:rank}, $dim(C(\mA)) = rank(\mA)<n$, which implies that the columns of $\mA$ are linearly dependent. Then, there exists a non-zero vector $v$ such that 
  \begin{equation}\label{eq:profull}
  	\mA v = 0.
  \end{equation}
Multiplying $\mA^{-1}$ on both sides of equation~\eqref{eq:profull}, we have \[ v = \mA^{-1} 0 = 0.\] However, it contradicts the fact that $v$ is non-zero. Therefore, $\mA$ is full rank.
\end{proof}

The rank of $\mA$ also effects the number of solutions to the system $\mA x = b$. We will discuss it in next section.

\section{Solving $Ax = b$}

Here we discuss the solutions of the linear system $Ax = b$, where $\mA \in \bbR^{m \times n}$ is a matrix, and $x \in \bbR^n$, $b \in \bbR^m$ are vectors.

\subsection{Solving $Ax = 0$: Nullspace}
The \textit{nullspace} of matrix $\mA$ is the collection of all solutions $x$ to the system $\mA x = 0$. Let $N(\mA)$ denote the nullspace of $\mA$.

\vspace{.2cm}

\begin{lem}[Nullspace] 
The \textit{nullspace} of matrix $\mA$ is a vector space. 
\end{lem}

\begin{proof}
We need to show $N(\mA)$ is closed to linear combination to prove that $N(\mA)$ is a vector space. For $\forall v_1, v_2 \in N(\mA)$, we have,
\begin{equation}\label{eq:pronull}
	\mA(c_1 v_1 + c_2 v_2) = c_1 \mA v_1 +  c_2 \mA v_2 = 0, \quad \forall  c_1, c_2 \in \bbR.
\end{equation}
The equation~\eqref{eq:pronull} implies that any linear combination of vectors in $N(\mA)$ is also a vector in $N(\mA)$. Therefore, $N(\mA)$ is closed to linear combination.
\end{proof}

\begin{lem}[The rank of nullspace]\label{lem:ranknull}
If  $rank(\mA) = r$, the rank of nullspace $rank(N(\mA)) = n-r$ .
\end{lem}

\begin{proof} Let $\mR$ denote the RREF($\mA$).  We write $\mR$  in form $\mR = \begin{bmatrix} \mI_r & \mF \\ 0 & 0	\end{bmatrix}$, where $\mF \in \bbR^{ r \times (n-r)}$ is arbitrary matrix.
Let $\mX = \begin{bmatrix} -\mF \\ \mI_{n-r}	\end{bmatrix}$.  We have 
\begin{align}
	\mR \mX = \begin{bmatrix} \mI_r & \mF \\ 0 & 0	\end{bmatrix} \begin{bmatrix} -\mF \\ \mI_{n-r}	\end{bmatrix} = 0.
\end{align}
Therefore, each column of $\mX$ is a special solution to the system $\mA x = 0$. Next, we show other solutions to $\mA x = 0$ are linear combinations of those special solutions.

Suppose there is a solution $x = (x_1, x_2) \in N(\mA)$, where $x_1 \in \bbR^{r}$ and $x_2 \in \bbR^{n-r}$. We have 
\begin{align}
	\mR x = \begin{bmatrix} \mI_r & \mF \\ 0 & 0	\end{bmatrix} \begin{bmatrix} x_1 \\ x_2	\end{bmatrix} = \begin{bmatrix} x_1 + \mF x_2 \\  0 &\end{bmatrix} = 0.
\end{align}
That implies $x_1  = -\mF x_2$ and $x = \begin{bmatrix} x_1 \\ x_2	\end{bmatrix} = \begin{bmatrix} -\mF \\ \mI_{n-r}	\end{bmatrix} x_2 = \mX x_2$. Any vector  in $ N(\mA)$ is a linear combination of special solutions, i.e. $C(\mX) = N(\mA)$. Therefore, the rank of nullspace is $rank(N(\mA)) = dim(C(\mX)) = n-r$.
\end{proof}

Recall the definitions of pivot column and free column. In $\mA x = b$, the variables in $x$ that correspond to pivot columns are called \textit{pivot variables} and others are \textit{free variables}. If $rank(\mA) = r$, there are $n-r$ free variables, which coincides with $rank(N(\mA))$. 

In the proof of lemma~\ref{lem:ranknull}, the columns of $\mX = \begin{bmatrix} -\mF \\ \mI_{n-r}	\end{bmatrix}$ are special solutions that compose the basis of $N(\mA)$. Practically, we find those special solutions by assigning 1 to a free variable and 0 to other free variables, and then solving the system $\mA x = 0$. 



\subsection{Solving $Ax = b$: complete solutions}

\begin{lem}[Solvability of $\mA x = b$]\label{lem:solve} The system $\mA x = b$ is solvable only when $b \in C(\mA)$.	
\end{lem}

\begin{proof}
	 If $\mA x = b$ is solvable, there exists a $x$ such that $\mA x = b$. For any $x$, $\mA x \in C(\mA)$. Therefore, $b \in C(\mA)$.
\end{proof}

\begin{lem}[Complete solution]\label{lem:compsolu} The complete solution of $\mA x = b$ is given by $x_{comp} = x_p + x_n$, where $x_p$ is a particular solution such that $\mA x_p = b$, and $x_n \in N(\mA)$. 
\end{lem}

\begin{proof}
	Suppose $x = x_p + x_0$ is an arbitrary solution to $\mA x= b$. We have 
	\begin{align}
		\mA x - \mA x_p = \mA (x- x_p) = \mA x_0 = 0.
	\end{align}
	Therefore, $x_0 \in N(\mA)$.
\end{proof}

Usually, we find a particular solution by assigning 0 to free variables, and solving the system $\mA x = b$. 

\vspace{.2cm}
The following table discusses the rank of $\mA$, the form of $\mR$, dimension of nullspace $N(\mA)$, and solutions of $\mA x = b$.

\begin{center}
\begin{tabular}{ c|c |c| c |c } 
 \hline
  & $r = m=n$ & $r = n < m$ & $r = m < n$ & $r<m,r<n$ \\ \hline 
  $\mR$& $\mI$ & $\begin{bmatrix} \mI \\ 0	\end{bmatrix}$ & $\begin{bmatrix} \mI & \mF \end{bmatrix}$ & $\begin{bmatrix} \mI & \mF \\ 0 &0	\end{bmatrix}$\\
   $dim(N(\mA))$& 0 &0& $n -r$ & $n-r$ \\
  \# solutions to $\mA X = b$ & 1 & 0 or 1 & infinitely many & 0 or infinitely many\\
 \hline
\end{tabular}
\end{center}



\vspace{.2cm}








\end{document}