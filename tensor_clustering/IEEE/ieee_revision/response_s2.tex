\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{caption}

\usepackage{xr-hyper}
% this is for overleaf cross reference
\makeatletter
\newcommand*{\addFileDependency}[1]{% argument=file name and extension
  \typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother

\newcommand*{\myexternaldocument}[1]{%
    \externaldocument{#1}%
    \addFileDependency{#1.tex}%
    \addFileDependency{#1.aux}%
}


\usepackage[hidelinks]{hyperref}
\usepackage[margin=1in]{geometry}
% \geometry{
%  left=30mm,
%  right=30mm
%  }

\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage[authoryear]{natbib}
\usepackage{url}
\usepackage{enumitem}



\usepackage{amsmath,amssymb,amsthm,bm,mathtools}
\usepackage{algorithm}
\usepackage{dsfont,multirow,hyperref,setspace,enumerate}
\hypersetup{colorlinks,linkcolor=blue,citecolor=blue,urlcolor={black}}
\usepackage{lscape}
\usepackage{afterpage}
\usepackage{hyperref}

\usepackage{microtype}
\usepackage{wrapfig}
\allowdisplaybreaks
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{verbatim}
\usepackage{array}

\usepackage{tabularx,booktabs}
\newcolumntype{Y}{>{\centering\arraybackslash}X}


\theoremstyle{definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}
\newtheorem{condition}{Condition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{property}{Property}
\newtheorem{hypothesis}{Hypothesis}

\newtheorem{innercustomgeneric}{\customgenericname}
\providecommand{\customgenericname}{}
\newcommand{\newcustomtheorem}[2]{%
  \newenvironment{#1}[1]
  {%
   \renewcommand\customgenericname{#2}%
   \renewcommand\theinnercustomgeneric{##1}%
   \innercustomgeneric
  }
  {\endinnercustomgeneric}
}


\newcustomtheorem{customexample}{Example}

\usepackage{appendix}
\usepackage{fullpage} 
\usepackage{microtype}
\usepackage{wrapfig}
\mathtoolsset{showonlyrefs}

\newcommand{\of}[1]{\left(#1\right)}
\newcommand{\off}[1]{\left[#1\right]}
\newcommand{\offf}[1]{\left\{#1\right\}}
\newcommand{\aabs}[1]{\left|#1\right|}
\newcommand{\ang}[1]{\left\langle#1\right\rangle}


\newcommand{\distgap}{D_{\ell_2}}
\newcommand{\Mat}{\text{Mat}}


\def\ci{\perp\!\!\!\perp}


\newtheorem{innercustomgeneric}{\customgenericname}
\providecommand{\customgenericname}{}
\newcommand{\newcustomtheorem}[2]{%
  \newenvironment{#1}[1]
  {%
   \renewcommand\customgenericname{#2}%
   \renewcommand\theinnercustomgeneric{##1}%
   \innercustomgeneric
  }
  {\endinnercustomgeneric}
}

\newcustomtheorem{customthm}{Theorem}
\newcustomtheorem{customlemma}{Lemma}
\newcustomtheorem{customrmk}{Remark}
\newcustomtheorem{customprop}{Proposition}

\usepackage{booktabs}
\newcommand\doubleRule{\toprule\toprule}
\allowdisplaybreaks

\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}



\input macros.tex
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%
\setlength\parindent{0pt}
\usepackage[parfill]{parskip}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}


\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}

\newcommand\Algphase[1]{%
\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\textwidth}{0.4pt}%
\Statex\hspace*{-\algorithmicindent}\textbf{#1}%
\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\textwidth}{0.4pt}%
}
\usepackage{caption}
\def\fixme#1#2{\textbf{\color{red}[FIXME (#1): #2]}}


\myexternaldocument{ieee_revise_second}

\title{\textbf{Response Letter}}
\date{}


\begin{document}

\maketitle
%\doublespacing

\begin{center}
    \textbf{Response to Major Comments from Editor}
\end{center}

\textit{Reviewer 1 has identiﬁed a number of technical concerns in the proofs of some results. Please make sure that these are fully addressed in your revised manuscript.}

\textbf{Response:} We thank editor and reviewers for the comments and suggestions. Reviewer 1 has mainly commented on the Bernoulli initialization in Section~\ref{subsec:ber} and the lower bound statement in Section~\ref{sec:limits}. In comment \# 1, Reviewer 1 has identified the technical issue in the proof of Proposition~\ref{prop:ber}.  In comment \# 3, Reviewer 1 has concerned the proving hardness of our impossibility statements compared with tensor block model (TBM) and pointed out the technical issues in the proof of Theorem~\ref{thm:stats}.

 For comment \# 1, we have developed a new proof of Proposition~\ref{prop:ber} with a new lemma applicable for binary low-rank matrix estimation. The conclusion in Proposition~\ref{prop:ber} still holds. We have put the new proof in Appendix~\ref{sec:initial_prove} with a more rigorous presentation.
 
 For comment \# 3, we have proved a new stronger statistical impossibility result in Theorem~\ref{thm:stats}, inspired by the reviewer. With the new statement, we have added proof sketches in Section~\ref{sec:mainproof} to highlight the usage of dTBM-specific techniques in our proofs and revised the discussion in Section~\ref{sec:tbm} to emphasize the comparison with TBM. We have provided the proof of the new Theorem~\ref{thm:stats} in Appendix~\ref{sec:statprove1}. We have addressed all reviewer's technical concerns in the new proof. 
 
 In addition to the aforementioned main points, we have addressed all comments from Reviewers 1 and 2. See our response in \textbf{Point-by-point response to Reviewer 1} and \textbf{Point-by-point response to Reviewer 2} for details.

\newpage
\begin{center}
    \textbf{Point-by-point response to Reviewer 1}
\end{center}

\textit{The manuscript has been revised substantially and I appreciate the authors’ efforts on providing additional theoretical and numeric results. Nevertheless, I think the response does not fully address my concerns and the added stuff may not be even technically sound, which I will elaborate as follow.}

\begin{enumerate}
    \item \textit{My first comment in the previous review concerns with the applicability of the proposed framework on Bernoulli setting (hypergraph). The authors introduce the Section IV-C to discuss the initialization strategy for dense Bernoulli data (sub-algorithm 3) with guarantee (Proposition 1).}

    \textbf{Response:} There seem to be two sub-questions in this comment; we answer them one-by-one. 

    \begin{enumerate}[wide]
        \item \textit{The proposed strategy relies on the square-unfolding of tensors (i.e., $\Mat_{sq}$). Since this is not a common tensor operation, the authors should provide the mathematical definition. My major concern is the soundness of Proposition 1, which is proved using one line by applying the result by Gao et al. (2018). The referenced paper is a benchmark work on matrix dSBM (so that the input matrix Y in their context is an exact adjacency matrix generated by dSBM). I doubt that, under tensor dSBM, is $\Mat_{sq}(\tY)$ a valid adjacency matrix? From my perspective, the $\Mat_{sq}(\tY)$ may not be a squared matrix and the independent noises may also derive from the setting studied by Gao et al. (2018). In any event, the authors should be more rigours on proving their mathematical statement instead of just saying “Following ...,”.}

        \textbf{Response:} We thank reviewer for the suggestions and pointing out the issues in the proof of Proposition~\ref{prop:ber}. 
        
        First, we have added the definition of operation $\Mat_{sq}$ and the interpretation of the matricization $\Mat_{sq}(\tY)$. The matrix  $\Mat_{sq}(\tY)$ is not square and symmetric, but we are able to interpret $\Mat_{sq}(\tY)$ as a valid adjacency matrix for a bipartite network with connections between two groups of nodes. We quote the corresponding revision in Section~\ref{subsec:ber}:
        
        \begin{quote}
            ``...
            One possible remedy is to apply singular value decomposition to the \emph{square unfolding}~\citep{mu2014square}, $\Mat_{sq}(\cdot)$, of Bernoulli tensor $\tY \in \{0,1\}^{p_1 \times \cdots \times p_K}$. Specifically, the square matricization $\mat_{sq} (\tY) \in \{0,1\}^{p^{\floor{K/2}} \times p^{\ceil{K/2}} }$ has entries $[\Mat_{sq}(\tY)](j_1, j_2) = \tY(i_1, \ldots, i_K)$, where
\begin{align}
    j_1 &= i_1 + p_1(i_2 - 1) + \cdots + p_1 \cdots p_{\floor{K/2}-1} (i_{\floor{K/2}} - 1),\\
    j_2 &= i_{\ceil{K/2}} + p_{ \ceil{K/2}}(i_{\ceil{K/2} + 1} - 1) + \cdots + p_{ \ceil{K/2}} \cdot p_{K-1} (i_K-1).
\end{align}
The matrix $\Mat_{sq}(\tY)$ is asymmetric. We are able to interpret $\Mat_{sq}(\tY)$ as the adjacency matrix for a bipartite network with connections between two groups of node. The two groups of nodes in the bipartite network have $p_1 \cdots p_{\floor{K/2}}$ and $ p_{\ceil{K/2}} \cdots p_K$ nodes, respectively. The entry $[\Mat_{sq}(\tY)](j_1, j_2)$ refers to the presence of connection between the nodes indexed by combinations $(i_1, \ldots, i_{\floor{K/2}})$ and $(i_{\ceil{K/2}}, \ldots, i_K)$. We summarize the procedure in Sub-algorithm 3.
 ... "
        \end{quote}
        
        
        Second, we agree that \citet[Lemma 7]{gao2018community} is non-applicable in our case due to the asymmetry and the different estimation procedure to obtain $\hat \tX'$. Instead, we have applied a new lemma for low-rank matrix estimation to upper bound the estimation error via square unfolding. The conclusion in Proposition~\ref{prop:ber} still holds with the new lemma. We have revised the proof more rigorously without ``following..." terms and put the proof to Appendix~\ref{sec:initial_prove} for a more detailed presentation. 
        
        For self-consistency, we attach the Proposition~\ref{prop:ber} with proof and the new lemma here:
        \begin{quote}
            `` ...
            \begin{customprop}{1}[Error for Bernoulli initialization] Consider the Bernoulli dTBM in the parameter space $\tP$ and Assumption~\ref{assmp:min_gap} holds. Assume that $\mtheta$ is balanced and $\min_{i\in[p]}\theta(i) \geq c$ for some constant $c>0$. Let $ z^{(0)}$ denote the output of Sub-algorithm~3. With probability going to 1, we have
\begin{equation}\label{eq:ini_b}
 \ell(z^{(0)}, z) \lesssim \frac{r^K p^{- \floor{K/2} }}{\text{SNR}}, \quad \text{and} \quad L(z^{(0)}, z) \lesssim  {\sigma^2 r^K p^{-\floor{K/2}}}.
\end{equation}
            \end{customprop}
... "
        \end{quote}
        
        \begin{quote}
            
``... \begin{proof}[Proof of Proposition~\ref{prop:ber}] Sub-algorithm~3 shares the same algorithm strategy as Sub-algorithm~\hyperref[alg:main]{1} but with a different estimation of the mean tensor, $\hat \tX'$. Hence, the proof of Proposition~\ref{prop:ber} follows the same proof idea with the proof of Theorem~\ref{thm:initial}. Replacing the estimation $\hat \tX$ by $\hat \tX'$ in the proof of Theorem~\ref{thm:initial}, we have 
\begin{align}
        \min_{\pi \in \Pi} \sum_{i : z^{(0)}(i) \neq \pi(z(i))} \theta(i)^2  &\lesssim \of{\sum_{i \in S} \onormSize{}{\mX_{i:}}^2 + \sum_{i \in S_0} \onormSize{}{\mX_{i:}}^2  } p^{-(K-1)} r^{K-1}. \label{eq:binary_theta}
    \end{align}
By inequalities~\eqref{eq:lem1} and \eqref{eq:s0}, we have 
\begin{align}
   \sum_{i \in S} \onormSize{}{\mX_{i:}}^2 &\leq  \of{ \frac{16(1 + \eta)}{c_0^2 \Delta_{\min}^2 } + 2 }\onormSize{}{\hat \tX' - \tX}_F^2, \label{eq:binary_theta1}  \\
   \sum_{i \in S_0} \onormSize{}{\mX_{i:}}^2 &\leq  \onormSize{}{\hat \tX' - \tX}_F^2. \label{eq:binary_theta2}
\end{align}
Hence, it suffices to find the upper bound of the estimation error $\onormSize{}{\hat \tX' - \tX}_F^2$ to complete our proof. Note that the matricization $\Mat_{sq}(\tX) \in \bbR^{p^{\floor{K/2}} \times p^{\ceil{K/2}}}$ has $\text{rank}(\Mat_{sq}(\tX)) \leq r^{\ceil{K/2}}$, and Bernoulli random variables follow the sub-Gaussian distribution with bounded variance $\sigma^2 = 1/4$. Apply Lemma~\ref{lem:lowrank} to $\mY = \Mat_{sq}(\tY), \mX = \Mat_{sq}(\tX)$, and $\hat \mX = \Mat_{sq}(\hat \tX')$. Then, with probability tending to 1 as $ p \rightarrow \infty$, we have 
        \begin{equation}\label{eq:binary_est}
            \onormSize{}{\hat \tX' - \tX}_F^2 = \onormSize{}{\Mat_{sq}(\hat \tX') - \Mat_{sq}(\tX)}_F^2 \lesssim p^{\ceil{K/2}}.
        \end{equation}
Combining the estimation error~\eqref{eq:binary_est} with inequalities~\eqref{eq:binary_theta1}, \eqref{eq:binary_theta2}, and \eqref{eq:binary_theta}, we obtain 
\begin{equation}\label{eq:binary_theta_sum}
    \min_{\pi \in \Pi} \sum_{i : z^{(0)}(i) \neq \pi(z(i))} \theta(i)^2 \lesssim  \frac{\sigma^2 r^{K-1}}{ \Delta_{\min}^2  p^{K-1} } p^{\ceil{K/2}}.
\end{equation}
Replace the inequality~\eqref{eq:theta_sum} in the proof of Theorem~\ref{thm:initial} by inequality~\eqref{eq:binary_theta_sum}. With the the same procedures to obtain $\ell(\hat z^{(0)}, z)$ and $L(\hat z^{(0)}, z)$ for Theorem~\ref{thm:initial} , we finish the proof of Proposition~\ref{prop:ber}.
\end{proof}

\begin{customlemma}{9}[Low-rank matrix estimation]  Let $\mY = \mX + \mE \in \bbR^{m \times n}$, where $\mE$ contains independent mean-zero sub-Gaussian entries with bounded variance $\sigma^2$. Suppose $\text{rank}(\mX) = r$. Consider the least square estimator 
        \begin{equation}
            \hat \mX = \argmin_{\mX' \in \bbR^{m \times n}, \text{rank}(\mX') \leq r} \onormSize{}{\mX' - \mY}_F^2.
        \end{equation}
        There exist positive constant $C_1, C_2$ such that
        \begin{equation}
            \onormSize{}{\hat \mX - \mX}_F^2 \leq C_1 \sigma^2 nr,
        \end{equation}
        with probability at least $1 - \exp(-C_2 nr)$.
\end{customlemma}


\begin{proof}[Proof of Lemma~\ref{lem:lowrank}] Note that $\onormSize{}{\hat \mX - \mY}_F^2 \leq \onormSize{}{\mX - \mY}_F^2$ by the definition of least square estimator. We have 
        \begin{align}
            \onormSize{}{\hat \mX - \mX}_F^2 & \leq 2 \ang{ \hat \mX - \mX, \mY - \mX} \\ & \leq 2 \onormSize{}{\hat \mX - \mX}_F \sup_{\mT \in \bbR^{m \times n}, \text{rank}(\mT) \leq 2r, \onormSize{}{\mT}_F = 1} \ang{\mT, \mY - \mX} \\
            & \leq C \sigma  \onormSize{}{\hat \mX - \mX}_F \sqrt{nr},
        \end{align}
        with probability at least $1 - \exp(-C_2 nr)$, where the second inequality follows by re-arrangement, and the last inequality follows from \citet[Lemma E5]{han2022optimal} under the matrix case. "
\end{proof}
        \end{quote}
    
        
%         We attach our revision in Section IV-C here:
%         \begin{quote}
%         ``...
        
%         One possible remedy is to apply singular value decomposition to the \emph{square unfolding}~\citep{mu2014square}, $\Mat_{sq}(\cdot)$, of Bernoulli tensor $\tY \in \{0,1\}^{p_1 \times \cdots \times p_K}$. Specifically, we have $\mat_{sq} (\tY) \in \{0,1\}^{p^{\floor{K/2}} \times p^{\ceil{K/2}} }$ with entries $[\Mat_{sq}(\tY)](j_1, j_2) = \tY(i_1, \ldots, i_K)$, where
% \begin{align}
%     j_1 &= i_1 + p_1(i_2 - 1) + \cdots + p_1 \cdots p_{\floor{K/2}-1} (i_{\floor{K/2}} - 1),\\
%     j_2 &= i_{\ceil{K/2}} + p_{ \ceil{K/2}}(i_{\ceil{K/2}} - 1) + \cdots + p_{ \ceil{K/2}} \cdot p_{K-1} (i_K-1).
% \end{align}
% The square matricization $\Mat_{sq}(\tY)$ is an asymmetric matrix. We can interpret $\Mat_{sq}(\tY)$ as the adjacency matrix for a bipartite network with connections between two groups of node. The first and second groups have $p_1 \cdots p_{\floor{K/2}}$ and $ p_{\floor{K/2}} \cdots p_K$ nodes, respectively. The entry $[\Mat_{sq}(\tY)](j_1, j_2)$ with $j_1, j_2$ in above equations refer to the presence of connections between the nodes corresponding to combinations $(i_1, \ldots, i_{\floor{K/2}})$ and $(i_{\ceil{K/2}}, \ldots, i_K)$. We summarize the procedure in Sub-algorithm 3.

% \begin{algorithm}[h!]
% \caption*{\bf Sub-algorithm 3: Weighted higher-order initialization for Bernoulli observation}
% \vspace{.15cm}
% \begin{algorithmic}[1] 
% \INPUT Bernoulli tensor $\tY \in \{0,1\}^{p\times \cdots \times p}$, cluster number $r$, relaxation factor $\eta > 1$ in $k$-means clustering.
% \State  Let the matrix $\mat_{sq} (\tY) \in \{0,1\}^{p^{\floor{K/2}} \times p^{\ceil{K/2}} }$ denote the nearly square unfolded tensor. Compute the estimate $\tX'$, where
% \begin{equation}\label{eq:matrixsvd}
%     \hat \tX' = \argmin_{\text{rank}(\mat_{sq}(\tX)) \leq r^{\ceil{K/2}}} \onormSize{}{ \mat_{sq}(\tX) -  \mat_{sq}(\tY)}_F^2.
% \end{equation}
% \State Implement lines 3-5 of Sub-algorithm~\hyperref[alg:main]{1} with $\hat \tX$ replaced by $\hat \tX'$ in~\eqref{eq:matrixsvd}.
% \OUTPUT Initial clustering $z^{(0)} \leftarrow \hat z$.
% \end{algorithmic}
% \end{algorithm}

% \begin{prop}[Error for Bernoulli initialization]\label{cor:ber} Consider the Bernoulli dTBM in the parameter space $\tP$ and Assumption~\ref{assmp:min_gap} holds. Assume that $\mtheta$ is balanced and $\min_{i\in[p]}\theta(i) \geq c$ for some constant $c>0$. Let $ z^{(0)}$ denote the output of Sub-algorithm~3. With probability going to 1, we have
% \begin{equation}\label{eq:ini_b}
%  \ell(z^{(0)}, z) \lesssim \frac{r^K p^{- \floor{K/2} }}{\text{SNR}}, \quad \text{and} \quad L(z^{(0)}, z) \lesssim  {\sigma^2 r^K p^{-\floor{K/2}}}.
% \end{equation}
% \end{prop}

%  \begin{proof}[Proof of Proposition~\ref{cor:ber}] Note that $\Mat_{sq}(\tX) \in \bbR^{p^{\floor{K/2}} \times p^{\ceil{K/2}}}$ and $\text{rank}(\Mat_{sq}(\tX)) \leq r^{\ceil{K/2}}$. Apply Lemma~\ref{lem:lowrank_binary} with $\mY = \Mat_{sq}(\tY), \mX = \Mat_{sq}(\tX)$, and $\hat \mX = \Mat_{sq}(\hat \tX')$. With probability tending to 1 as $ p \rightarrow \infty$, we have 
%         \begin{equation}
%             \onormSize{}{\hat \tX' - \tX}_F^2 = \onormSize{}{\Mat_{sq}(\hat \tX') - \Mat_{sq}(\tX)}_F^2 \lesssim p^{\ceil{K/2}}.
%         \end{equation}
%         We then obtain the upper bound by using $\hat \tX'$ in place of $\hat \tX$ in Theorem~\ref{thm:initial}.\fixme{Miaoyan}{you did not mention anywhere in the proof where the assumptions of $\theta$ is used...Change ``we then obtain the upper bound....'' by the full proof.
%         }
%         \end{proof}

% \begin{lem}[Low-rank binary matrix estimation] \label{lem:lowrank_binary} Let $\mY \in \{0, 1\}^{m \times n}$ denote a binary matrix with independent Bernoulli entries. \fixme{Miaoyan}{remove binary. The proof does not use binary at all. Every mathematical Lemma should mention only *minimal* necessary conditions.  Never, never put redundant and useless conditions.} Suppose that $n \geq m$ and $\bbE[\mY] = \mX \in [0,1]^{m \times n}$ with fixed rank $\text{rank}(\mX) \leq r$. Consider the least square estimator 
%         \begin{equation}
%             \hat \mX = \argmin_{\mX' \in \bbR^{m \times n}, \text{rank}(\mX') \leq r} \onormSize{}{\mX' - \mY}_F^2.
%         \end{equation}
%         There exist positive constant $C_1, C_2$ such that
%         \begin{equation}
%             \onormSize{}{\hat \mX - \mX}_F^2 \leq C_1 nr,
%         \end{equation}
%         with probability at least $1 - \exp(-C_2 nr)$.
%         \end{lem}
%  "
%         \end{quote}
        
%         For self-consistency, we also attach the proof for Lemma~\ref{lem:lowrank_binary} in Appendix C here:
%         \begin{quote}
%         ``
%             \begin{proof}[Proof of Lemma~\ref{lem:lowrank_binary}] Note that $\onormSize{}{\hat \mX - \mY}_F^2 \leq \onormSize{}{\mX - \mY}_F^2$ by the definition of least square estimator. Re-arranging the terms, we have 
%         \begin{align}
%             \onormSize{}{\hat \mX - \mX}_F^2 & \leq 2 \ang{ \hat \mX - \mX, \mY - \mX} \\ & \leq 2 \onormSize{}{\hat \mX - \mX}_F \sup_{\mT \in \bbR^{m \times n}, \text{rank}(\mT) \leq 2r, \onormSize{}{\mT}_F = 1} \ang{\mT, \mY - \mX} \\
%             & \leq C  \onormSize{}{\hat \mX - \mX}_F \sqrt{nr}
%         \end{align}
%         with probability at least $1 - \exp(-C_2 nr)$ for some positive constant $C_2$, where the second inequality follows from \citet[Lemma 1]{wang2019multiway}\fixme{Miaoyan}{provide a full proof for self-containness.}, and the last inequality follows from \citet[Lemma E5]{han2022optimal} \fixme{Miaoyan}{Not reader friendly! This is a simple results in matrix theory. Do not over-complicate things by involving a harder lemma.
%         Find a textbook for easy formulation for reference. }under the matrix case and the fact that the Orlicz norm of Bernoulli noise $\onormSize{}{\mY_{ij} - \mX_{ij}}_{\varphi_2} = \sup_{q \geq 1} \bbE[|\mY_{ij} - \mX_{ij}|^q]^{1/q}/q^{1/2} \leq 1$ since $|Y_{ij} - X_{ij}| \leq 1$ for all $i \in [m], j \in [n]$.
%         \end{proof} "
%         \end{quote}

        \item \textit{Also, Comparison in Remark 3 seems unfair. While the final exponential error rate developed in this paper is tight, it requires quite stronger SNR for a warm initialization. So, it does not make sense to compare the final rate without talking about under what SNR conditions these rates are achievable. Could the authors compare the SNR conditions under which both methods (the proposed one and Ke et al., 2019) achieve exact recovery?}

        \textbf{Response:} It seems that reviewer intends to comment the Remark 8, which compares the error rates between our algorithm and \cite{ke2019community}, rather than the Remark 3 for degree assumption, which is irrelevant to algorithm performance. We answer this question with focus on the Remark 8.
        
        The method in \cite{ke2019community} adopts a signal notion based on the singular gap of the core tensor, denoted as $\Delta_{\rm singular}$. However, we are not able to infer the exact recovery of the spectral method by our angle-base SNR condition. There exists a case in which our algorithm achieves exact recovery but the spectral method in \cite{ke2019community} fails. Hence, for fair comparison, we compare the best performance of our algorithm and \cite{ke2019community} under the strongest signal setting of each model. We have added a new remark in Section~\ref{subsec:ber} and elaborated the comparison with \cite{ke2019community} with SNR consideration:
        \begin{quote}
        ``...
            \begin{customrmk}{10}[Comparison with previous methods]   Previous work \citep{ke2019community} develops a spectral clustering method for Bernoulli dTBM. \cite{ke2019community} adopts a different signal notion based on the singular gap in the core tensor, denoted as $\Delta_{\rm singular}$. By \citet[Theorem 1]{ke2019community}, the spectral method achieves exact recovery with $\Delta_{\rm singular} \gtrsim p^{-1/2}$. However, we are not able to infer the exact recovery of spectral method by our angle-base SNR condition. Consider an order-2 dTBM with $p > 2, \sigma^2 = 1$, $\mtheta = \mathbf{1}$, equal size assignment $|z^{-1}(a)| = p/r$ for all $ a \in [r]$, and core matrix $\mS = \mI_2$. The singular gap under this setting is $\Delta_{\rm singular} = \min\{ \lambda_1 - \lambda_2, \lambda_2 \} = 0$, where $\lambda_1 \geq \lambda_2$ are singular values of $\mS$. In contrast, our angle gap $\Delta_{\min}^2 = 2$ satisfies the SNR condition in Theorem~\ref{thm:refinement}. Then, our algorithm achieves the exact recovery, but the spectral method in \cite{ke2019community} fails.
            
Hence, for fair comparison, we compare the best performance of our algorithm and \cite{ke2019community} under the strongest signal setting of each model. Since both methods contain an iteration procedure, we set the iteration number to infinity to avoid the computational error. Considering the largest angle-based SNR $\asymp 1$ in Theorem~\ref{thm:refinement}, our Bernoulli clustering achieves exponential error rate of order $\exp(-p^{(K-1)})$; considering the largest singular gap $\Delta_{\rm singular} \asymp 1$ in Theorem 1 of \cite{ke2019community}, the spectral clustering has a polynomial error rate of order $p^{-2}$. Our algorithm still shows a better theoretical accuracy than the competitive work for Bernoulli observations.   "
            \end{customrmk}

        \end{quote}
        
        % In Remark 8, we compare the best performances that our algorithm and method in \cite{ke2019community} can achieve. Both methods contain an iteration procedure, and \cite{ke2019community} adopts a singular gap signal notion different with our angle gap notion. To obtain the best performance, we set the number of iterations to infinity for both methods to avoid computational error and consider the error rates under the strongest angle or singular based SNR, respectively. 
        
        % We consider the algorithm error rates of both methods with infinity number of iterations and with the strongest angle and singular SNR
        
        
        % Since both methods contain an iteration procedure, we set both iteration numbers to infinity to avoid the computational error; we 

    \end{enumerate}
    
    \item \textit{My second concern was on the imposed identifiability conditions and I think the authors have fully addressed it.}
    
    \item \textit{My last comment surrounding the two lower bounds is the major one, but I don’t think the authors have addressed it appropriately. }
    
    \textbf{Response:} This comment concerns the proving hardness of the lower bound statements in dTBM and the flaws in the proof of Theorem~\ref{thm:stats}. We separate the comment by these two aspects and address each aspect one-by-one.
    
    \begin{enumerate}[wide]
        \item \textit{The authors argued that ``Our results show the similar conclusion but under different conditions.”. I agree that $\Delta_{\rm ang}$ and $\Delta_{\rm Euc}$ (using the notation is the response) are different quantities and $\Delta_{\rm ang}$ can be much smaller than $\Delta_{\rm Euc}$ in the proposed setting. However, that does NOT affect the hardness in proving the lower bound statement. For simplicity, I will just show that for statistical lower bound, which claims that}
    \begin{equation}
        \inf_{\hat z} \sup_{(z,S, \theta) \in P(\gamma)} \bbE[p\ell(\hat z, z )] \geq 1.
    \end{equation}
    \textit{Here $P(\gamma)$ refers to the parameter space where the core tensor S satisfies $\Delta_{\rm ang}^2 /\sigma^2 \leq p^{\gamma}$  and $z, \theta$ satisfy some regularity conditions (e.g., balance). To prove the lower bound, it suffices to fix particular $\theta$ and $S$ and take supreme over the community variable $z$. Then, a trivial selection is to set $\theta_i = 1$ (homogeneous degree) and some S such that $\Delta_{\rm ang}/\Delta_{\rm Euc} = 1 + o(1)$. Such $S$ can be constructed randomly: $S_{ijk} = 1 + \epsilon_{ijk}$ where $\epsilon_{ijk}$  are independent mean-zero Gaussian with variance tends to zero. After constructing such $\theta_i$ and $S$, the heterogeneity issue vanishes and $\Delta_{\rm Euc}$ and $\Delta_{\rm ang}$ becomes equivalent.}
    
    \textit{The lower bounds arguments for TBM and dTBM become really different if the author considers the following stronger statement:}
    \begin{equation}
        \inf_{S \in P(\gamma)} \inf_{\hat z} \sup_{(z, \theta)} \bbE[p \ell(\hat z, z)] \geq 1
    \end{equation}
    \textit{This statement is stronger than the previous one, as it suggests statistical impossibility whenever the SNR condition is not met, while the current one actually suggests statistical impossibility for at least a specific $S$ that does not meet SNR requirement.}
    
    \textbf{Response:} 
    %The former part of the comment concerns the proving hardness of lower bound statements in dTBM compared with TBM. 
    We thank reviewer for the suggestions. In short, we have provided a stronger statistical impossibility statement for dTBM, inspired by the reviewer. With the new statement, we argue that dTBM-specific techniques are required to obtain our statistical and computational impossibilities, though the proof idea shares the same spirit with reviewer's proof in comment. We have made several revisions in Section~\ref{sec:limits}, Section~\ref{sec:tbm}, Section~\ref{sec:mainproof}, and Appendix~\ref{sec:statprove1} to address the comment. We attach each revision with explanations below.   
    
    First, we have provided a new impossibility result in Theorem~\ref{thm:stats}. The new Theorem~\ref{thm:stats} suggests the statistical impossibility of dTBM whenever the core tensor $\tS$ leads to a SNR with signal exponent $\gamma < -(K-1)$. This new impossibility statement is stronger than previous one which suggests the worst case impossibility for a particular $\tS$. We quote the revised theorem and discussion in Section~\ref{sec:limits} here:
    
    \begin{quote}
    ``
    \begin{customthm}{2}[Statistical critical value]  Consider general Gaussian dTBMs with parameter space $\tP(\gamma)$ with $K\geq 1$. Then, we have the following statistical phase transition. 
    \begin{itemize}[wide]
    \item 
    \textbf{Impossibility.} Assume $r\lesssim p^{1/3}$. Let $\tP_{\tS}(\gamma) \coloneqq \{ \tS: c_3\leq \onormSize{}{\text{Mat}(\tS)_{a:}}\leq c_4, a\in[r]\} \cap \{ \tS: \Delta_{\min}^2 = p^{\gamma}\}$ denote the space for valid $\tS$ satisfying SNR condition~\eqref{eq:gamma}, and $\tP_{z ,\mtheta} \coloneqq \{ \mtheta\in\mathbb{R}^p_{+},\ 
{c_1p\over r}\leq |z^{-1}(a)|\leq {c_2 p\over r}, \onorm{\mtheta_{z^{-1}(a)}}_1=|z^{-1}(a)|, a \in [r] \}$ denote the space for valid $(z, \mtheta)$,  where $c_1, c_2, c_3, c_4$ are the constants in parameter space \eqref{eq:family}. If the signal exponent satisfies $\gamma < -(K-1)$, then, for any true core tensor $\tS \in \tP_{\tS}(\gamma)$, no estimator $\hat z_{\text{stat}}$ achieves exact recovery in expectation; that is,
    \begin{equation}
  \gamma< -(K-1) \quad \Rightarrow  \quad \liminf_{p \rightarrow \infty}\inf_{\tS \in  \tP_{\tS}(\gamma)}  \inf_{\hat z_{\text{stat}}}\sup_{ (z, \mtheta) \in \tP_{z, \mtheta}} \bbE \left[ p\ell(\hat z_{\text{stat}}, z) \right]\geq 1.
\end{equation}
\end{itemize}
    \end{customthm} ... "
    \end{quote}
    
    \begin{quote}
        `` ...
        The proofs for the two parts in Theorem~\ref{thm:stats} are in the Appendix II, Section~\ref{sec:statprove1} and Section~\ref{sec:statprove2}, respectively. The first part of Theorem~\ref{thm:stats} demonstrates impossibility of exact recovery whenever the core tensor $\tS$ satisfies SNR condition~\eqref{eq:gamma} with exponent $\gamma < -(K-1)$. The proof is information-theoretical, and therefore the results apply to all statistical estimators, including but not limited to MLE and trace maximization~\citep{ghoshdastidar2017uniform}. The minimax bound~\eqref{eq:minminmax} indicates the worst case impossibility for a particular core tensor $\tS$ with signal exponent $\gamma < -(K-1)$; i.e., under the assumptions of Theorem~\ref{thm:stats}, 
\begin{equation}
    \gamma < -(K-1) \quad \Rightarrow \quad \liminf_{p \rightarrow \infty}  \inf_{\hat z_{\text{stat}}}\sup_{ (z,\tS, \mtheta) \in \tP(\gamma)} \bbE \left[ p\ell(\hat z_{\text{stat}}, z) \right]\geq 1.
\end{equation}
Such worst case impossibility is frequently studied in related works \citep{han2020exact, gao2018community} while our lower bound~\eqref{eq:minminmax} provides a stronger impossibility statement for arbitrary core tensor with weak signal.   ..."
    \end{quote}
    
    Second, we have added proof sketches for the impossibility results to better highlight the dTBM-specific techniques we used. In fact, our proof idea is common in minimax analysis and shares the same spirit with reviewer's proof idea in the comment: find the lower bound of minimax error by constructing a particular set of true parameters. However, angle-based signal notion brings extra difficulties in parameter constructions, and the previous TBM construction is no longer applicable to address the arbitrariness of core tensor. We quote the new proof sketches in new Section~\ref{subsec:impossible} here:
    
    \begin{quote}
    `` ...
    
    The proofs of impossibility in Theorems~\ref{thm:stats} and \ref{thm:comp} share the same proof idea with \citet[Theorems 6 and 7]{han2020exact} and \citet[Theorem 2]{gao2018community}. In both proofs of statistical and computational impossibilities, the key idea is to construct a particular set of parameters to lower bound the minimax rate. Specifically, for statistical impossibility in Theorem~\ref{thm:stats}, we construct a particular $(z^*_{\rm stats}, \mtheta^*_{\rm stats})  \in \tP_{z, \mtheta} $ such that for all $\tS^* \in \tP_{\tS}(\gamma)$
\begin{equation}
    \inf_{\hat z_{\rm stats}} \sup_{(z, \mtheta) \in \tP_{z, \mtheta}} \bbE[p \ell(\hat z_{\rm stat}, z)] \geq \inf_{\hat z_{\rm stats}} \bbE[p \ell(\hat z_{\rm stat}, z_{\rm stats}^*)| (z^*_{\rm stats}, \tS^*,\mtheta^*_{\rm stats})] \geq 1;
\end{equation}
for computational impossibility in Theorem~\ref{thm:comp}, we construct a particular $(z^*_{\rm comp}, \tS^*_{\rm comp}, \mtheta^*_{\rm comp})$ $\in \tP(\gamma)$ such that 
\begin{equation}
    \inf_{\hat z_{\rm comp}} \sup_{(z, \tS, \mtheta) \in \tP(\gamma)} \bbE[p \ell(\hat z_{\rm comp}, z)] \geq \inf_{\hat z_{\rm comp}} \bbE[p \ell(\hat z_{\rm comp}, z_{\rm comp}^*)| (z^*_{\rm comp}, \tS^*_{\rm comp},\mtheta^*_{\rm comp})] \geq 1.
\end{equation}

The constructions of $(z^*_{\rm stats}, \mtheta^*_{\rm stats})$ and $(z^*_{\rm comp}, \tS^*_{\rm comp}, \mtheta^*_{\rm comp})$ are the most critical steps. With good constructions, the lower bound ``$\geq 1$" can be verified by classical statistical conclusions (e.g.\ Neyman-Pearson Lemma) or prior work (e.g.\ HPC Conjecture). 

A notable detail in the proof of statistical impossibility is the arbitrariness of $\tS^*$. The first infimum over $ \tP_{\tS}(\gamma)$ in the minimax rate~\eqref{eq:minminmax} requires that the lower bound~\eqref{eq:stats_impo_sketch} holds for any $\tS^* \in \tP_{\tS}(\gamma)$. The arbitrary choice of $\tS^*$ brings extra difficulties in the parameter construction, and consequently a non-trivial $\mtheta^*_{\rm stats} \neq \mathbf{1}$ is chosen to address the arbitrariness. Previous TBM construction in the proof of \citet[Theorem 6]{han2020exact} with $\mtheta^*_{\rm stats} = \mathbf{1}$ is no longer applicable in our case. Meanwhile, our construction $(z^*_{\rm comp}, \tS^*_{\rm comp}, \mtheta^*_{\rm comp})$ leads to a rank-2 mean tensor to relate the HPC Conjecture while TBM \citet[Theorem 7]{han2020exact} constructs a rank-1 mean tensor. Hence, we emphasize that dTBM-specific techniques are required to obtain our impossibility results, though the proof idea is common for minimax lower bound analysis. ... "
    \end{quote}
    
    Here are two details differ with reviewer's comment. First, reviewer's construction with $\mtheta^* = \mathbf{1}$ is non-applicable for our new statistical impossibility. The TBM construction with $\mtheta^*_{\rm stats} = \mathbf{1}$ is not able to handle the arbitrary choice of $\tS^*$, because the Euclidean gap condition $\Delta_{\rm Euc}^2 \leq p^{\gamma}$ is not guaranteed with arbitrary $\tS^*$ satisfying $\Delta_{\rm ang}^2 \leq p^{\gamma}$; see Example \ref{example:euc_alg} in our main text. In consequence, we design a special $\mtheta^*_{\rm stats} \neq \mathbf{1}$ in the proof.
    %Because the core tensor $\tS$ with angle gap $\Delta_{\rm ang}^2 \leq p^{\gamma}$ is not guaranteed to have Euclidean gap $\Delta_{\rm Euc}^2 \leq p^{\gamma}$; see Example \ref{example:euc_alg} in our main text. In consequence, we construct a special $\mtheta^*_{\rm stats} \neq \mathbf{1}$ to address the arbitrariness of $\tS^*$ with angle gap requirement. 
    Second, we consider one particular core tensor $\tS^*$ at a time, rather than the random tensor with entries $\tS_{ijk}^* = 1 + \epsilon_{ijk}, \epsilon_{ijk} \sim N(0, \delta^2)$ for some $\delta \rightarrow 0$ as $p \rightarrow \infty$. The expectation in minimax error is taken with respect to the noise in observation, rather than the randomness in the true parameters. 
    
    We have also revised the discussion in Section~\ref{sec:tbm} based on the new theorem and proof sketches:
    
    \begin{quote}
    ``...
    
    Third, from the perspective of proofs, we develop new dTBM-specific techniques to handle the extra degree heterogeneity. In our Theorem~\ref{thm:stats}, we construct a special non-trivial degree heterogeneity to establish the lower bound for arbitrary core tensor with small angle gap, while, TBM~\citep{han2020exact} considers the constructions without degree parameter.
%we consider the profile MLE by maximizing out the nuisance degree parameter, while TBM~\citep{han2020exact} considers the usual MLE without degree parameter.
In our Theorem~\ref{thm:comp}, we construct a rank-2 tensor to relate HPC conjecture to $\Delta^2_{\text{ang}}$, while TBM~\citep{han2020exact} constructs a rank-1 tensor to relate HPC conjecture to $\Delta^2_{\text{Euc}}$. The asymptotic non-equivalence between $\Delta^2_{\text{ang}}$ and $\Delta^2_{\text{Euc}}$ renders our proof technically more involved. ..."
    \end{quote}
    
    Last, we have added the comparison with TBM in terms of the stronger statistical impossibility. Though the presentation in the Theorem 6 of \cite{han2020exact} suggests a weaker worst case impossibility for TBM, the proof of Theorem 6 indeed implies a stronger statistical impossibility for arbitrary core tensor with small signal. Both models show similar conclusions under different signal notions again, but we emphasize that different techniques are required to address the different signal notions. We quote the new discussion in Section~\ref{sec:tbm} here:
    
    \begin{quote}
        `` ...
        
         Last, we discuss the statistical impossibility statements. Our Theorem~\ref{thm:stats} implies the statistical impossibility whenever the core tensor $\tS$ leads to an angle-based SNR below the critical threshold. The Theorem 6 in \cite{han2020exact} implies the worst case statistical impossibility for a particular core tensor $\tS$ with Euclidean-based SNR below the statistical limit. Hence, our Theorem~\ref{thm:stats} shows a stronger statistical impossibility for dTBM than that presented in TBM \citet[Theorem 6]{han2020exact}. However, inspecting the proof of \cite{han2020exact}, the proof of Theorem 6 indeed implies a stronger TBM impossibility statement for arbitrary core tensor; i.e.,
 \begin{equation}
     \gamma < -(K-1) \quad \Rightarrow \quad \liminf_{p \rightarrow \infty} \inf_{ \tS \in \tP_{\tS, {\rm TBM}} \cap \{\Delta_{\rm Euc}^2 = p^{\gamma} \} }\inf_{\hat z_{\rm stats}}  \sup_{z \in \tP_{z, {\rm TBM}}}  \bbE[p \ell(\hat z_{\rm stats}, z)] \geq 1,
 \end{equation}
 where $\tP_{\tS, {\rm TBM}}$ and $\tP_{z, {\rm TBM}}$ refer to the space for core tensor $\tS$ and assignment $z$ under TBM, respectively. Again, in terms of the strong statistical impossibility, both models show similar conclusions but under different conditions. Since two impossibilities consider different core tensor regimes with non-equivalent $\Delta^2_{\text{ang}}$ and $\Delta^2_{\text{Euc}}$, we emphasize that different proof techniques are required to obtain these similar conclusions.  See our proof sketch in Section~\ref{subsec:impossible}, Appendices~\ref{sec:statprove1} and \ref{sec:compprove1} for detail technical differences. "
    \end{quote}
    
    For self-consistency, we also attach the proof of the new impossibility theorem in Appendix~\ref{sec:statprove1}:
    
    \begin{quote}
    ``...
        \begin{proof}[Proof of Theorem~\ref{thm:stats} (Impossibility)]Consider the general asymetric dTBM~\eqref{eq:general_dtbm} in the special case that $p_k = p$ and $r_k = r$ for all $ k\in [K]$. For simplicity, we show the minimax rate for the estimation on the first mode $\hat z_1$; the proof for other modes are essentially the same. 
   
   To prove the minimax rate~\eqref{eq:minminmax}, it suffices to take an arbitrary $\tS^* \in  \tP_{\tS}(\gamma)$ wih $\gamma < -(K-1)$ and construct $(z^*_k, \mtheta^*_k)$ such that 
   \begin{equation}
       \inf_{\hat z_1} \bbE \left[ p\ell(\hat z_1, z_1^*) | (z^*_k, \mtheta^*_k, \tS^*)  \right]\geq 1.
   \end{equation}
   
   We first define a subset of indices $T_k \subset [p_k], k \in [K]$ in order to avoid the complication of label permutation. Based on \citet[Proof of Theorem 6]{han2020exact}, we consider the restricted family of $\hat z_k$'s for which the following three conditions are satisfied:
   \begin{align}
       &\text{(a)}\ \hat z_k(i)=z_k(i) \text{ for all }i\in T_k; \quad \text{(b)} \ |T^c_k|\asymp {p\over r}; \\
       &\text{(c)}\ \min_{\pi\in \Pi}\sum_{i\in[p]}\ind\{\hat z_k(i) \neq \pi\circ z_k (i)\} = \sum_{i\in[p]}\ind\{\hat z_k(i) \neq  z_k (i)\},
   \end{align}
for all $k \in [K]$.
   Now, we consider the construction:
   \begin{enumerate}
       \item[(i)] $\{z_k^*\}$ satisfies properties (a)-(c) with misclassification sets $T_k^c$ for all $k \in [K]$;
       \item [(ii)] $\{\mtheta_k^*\}$ such that $\mtheta_k^*(i) \leq \sigma r^{(K-1)/2} p^{-(K-1)/2}$ for all $i \in T_k^c, k \in [K]$ and $\max_{k \in [K], a \in [r]} $ $\onormSize{}{\mtheta_{k, z^{*, -1}_k(a)}}^2_2 \asymp p/r$.
   \end{enumerate}
   % There exists a $(z^*, \mtheta^*) \in \tP_{z, \mtheta}$ satisfying the property (i) and (ii) when $r < p$; otherwise, when $r = p$, we must have $\mtheta^*(i) = 1$ due to the $\ell_1$ norm constraint and we can not satisfy the property (ii).
   
   Combining the inequalities (39) and (40) in the proof of Theorem 2 in \cite{gao2018community}, we have 
   \begin{align}
        & \inf_{\hat z_1 } \bbE \left[ \ell(\hat z_1, z_1^*) | (z^*_k, \mtheta^*_k, \tS^*)  \right] \geq  \\
        & \quad  \frac{C}{r^3 |T_1^c|} \sum_{i \in T_1^c} \inf_{\hat z_1  (i)} \{ \bbP[\hat z_1(i) = 1| z_1^*(i) = 2, z_k^*, \mtheta_k^*, \tS^*] +   \bbP[\hat z_1(i) = 2| z_1^*(i) = 1, z_k^*, \mtheta_k^*, \tS^*] \},
   \end{align}
   where $C$ is some positive constant,  $\hat z_1$ on the left hand side denote the generic assignment functions in $\tP(\gamma)$, and the infimum on the right hand side is taken over the generic assignment function family of $\hat z_1(i)$ for all nodes $i \in T_1^c$. Here, the factor $r^3=r\cdot r^2$ in~\eqref{eq:inf_lower} comes from two sources: $r^2\asymp {r\choose 2}$ comes from the multiple testing burden for all pairwise comparisons among $r$ clusters; and another $r$ comes from the number of elements $|T^c_k|\asymp p/r$ to be clustered. 
   
   Next, we need to find the lower bound of the rightmost side in~\eqref{eq:inf_lower}. 
   %For simplicity, we show the bound for the mode-1 case $k = 1$ only. We drop the subscripts $1$ in $z_1, T_1, \mS_1, \mtheta_1$ and omit the repeated procedures for the cases of $k = 2,\ldots, K$. 
We consider the hypothesis test based on model~\eqref{eq:general_dtbm}. First, we reparameterize the model under the construction (i)-(ii).
\begin{equation}
    \mx_a^* = \off{\mat_1 \of{ \tS^*\times_2 \mTheta_2^* \mM_2^*\times_3\cdots \times_K \mTheta^*_K \mM_K^* }}_{a:}, \quad \text{for all} \ a \in [r],
\end{equation}
where $\mx_a^*$'s are centroids in $\bbR^{p^{K-1}}$. Without loss of generality, we consider the lower bound for the summand in~\eqref{eq:inf_lower} for $i=1$. The analysis for other $i\in T^c_1$ are similar. For notational simplicity, we suppress the subscript $i$ and write $\my, \theta^*, z$ in place of $\my_1, \mtheta_1^*(1)$ and $z_1(1)$, respectively. The equivalent vector problem for assessing the summand in~\eqref{eq:inf_lower} is
\begin{equation}
\my=\theta^* \mx_{z}^*+\me,
\end{equation}
where $z\in \{1,2\}$ is an unknown parameter, $\theta^* \in \bbR_+$ is the given heterogeneity degree, $\mx_1^*,\mx_2^*\in\bbR^{p^{K-1}}$ are given centroids, and $\me\in\bbR^{p^{K-1}}$ consists of i.i.d.\ $N(0,\sigma^2)$ entries.  Then, we consider the hypothesis testing under the model~\eqref{eq:z}:
\begin{equation}
 H_0: z = 1 \text{ and } \my = \theta^* \mx_1^* + \me \quad \leftrightarrow \quad H_1: z = 2 \text{ and } \my = \theta^* \mx_2^* + \me,
\end{equation}
   
%   For an arbitrary $i \in T_1^c$, the right hand side in \eqref{eq:inf_lower} corresponds to the following hypothesis test:
%   \begin{equation}\label{eq:hypo}
%       H_0: \my = \mtheta_1^*(i) \mx_1^* + \me \quad \leftrightarrow \quad H_1: \my = \mtheta_1^*(i) \mx_2^* + \me,
%   \end{equation}
%   where $\my = [\mat_1(\tY)]_{i:}$, $\mx_a^* = [\mat_1( \tS^* \times_2 \mTheta^* \mM_2^* \times_3 \cdots \times_K \mTheta^* \mM_K^*)]_{a:}$, and $\me$ consists of i.i.d.\ $N(0,\sigma^2)$ entries. 
   The hypothesis testing~\eqref{eq:test} is a simple versus simple testing, since the assignment $z$ is the only unknown parameter in the test. 
   By Neyman-Pearson lemma, the likelihood ratio test is optimal with minimal Type I + II error. Under Gaussian model, the likelihood ratio test of \eqref{eq:test} is equivalent to the least square estimator $\hat z_{LS} = \argmin_{a = \{1,2\}} \onormSize{}{\my - \theta^* \mx_a^*}_F^2$. 
   
   Let $\mS = \mat_1(\tS)$. Note that 
   \begin{align}
       \onormSize{}{ \theta^* \mx_1^*  - \theta^* \mx_2^*}_F &\leq  \theta^* \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F \prod_{k = 2}^K \lambda_{\max}(\mTheta_k^*\mM_k^*)  \\
       & \leq \theta^* \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F   \max_{k \in [K]/\{1\}, a \in [r]} \onormSize{}{\mtheta_{k, z^{*, -1}_k(a)}}_2^{K-1} \\
       & \leq  \sigma r^{(K-1)/2} p^{-(K-1)/2} 2 c_4 p^{(K-1)/2} r^{-(K-1)/2}\\
       & \leq 2 c_4 \sigma, 
   \end{align}
%   \begin{align}
%       \onormSize{}{ \mtheta_1^*(i) \mx_1^*  - \mtheta_1^*(i) \mx_2^*}_F &\leq  \mtheta_1^*(i) \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F \prod_{k = 2}^K \lambda_{\max}(\mTheta_k^*\mM_k^*)  \\
%       & \leq \mtheta_1^*(i) \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F   \max_{k \in [K]/\{1\}, a \in [r]} \onormSize{}{\mtheta_{k, z^{*, -1}_k(a)}}_2^{K-1} \\
%       & \leq  \sigma r^{(K-1)/2} p^{-(K-1)/2} 2 c_4 p^{(K-1)/2} r^{-(K-1)/2}\\
%       & \leq 2 c_4 \sigma, \label{eq:bound}
%   \end{align}
   where $\lambda_{\max}$ denote the maximal singular value, the second inequality follows from Lemma~\ref{lem:singular_thetam}, and the third inequality follows from property (ii) and the boundedness constraint in $\tP_{\tS}(\gamma)$ such that $\onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F  \leq \onormSize{}{\mS_{1:}^*}_F + \onormSize{}{\mS_{2:}^*}_F \leq 2c_4$.

   
   Hence, we have 
   \begin{align}
       & \inf_{\hat z_1(1)} \{ \bbP[\hat z_1(1) = 1| z_1^*(1) = 2, z^*_k,  \mtheta_k^*, \tS^*] +   \bbP[\hat z_1(1) = 2| z_1^*(1) = 1, z^*_k,  \mtheta_k^*, \tS^*] \} \\
       & \quad = 2 \bbP[ \hat z_{LS} = 1 |  z_1^*(1) = 2, z^*_k,  \mtheta_k^*, \tS^* ] \\
      & \quad  = 2 \bbP[ \onormSize{}{\my - \theta^* \mx_1^*}_F^2 \leq \onormSize{}{\my - \theta^* \mx_2^*}_F^2 | z_1^*(1) = 2, z^*_k , \mtheta_k^*, \tS^*   ] \\
      & \quad = 2 \bbP \off{ 2 \ang{\me, \theta^* \mx_1^* - \theta^*\mx_2^* } \geq  \onormSize{}{\theta^* \mx_1^* - \theta^*\mx_2^*}_F^2 }\\
       & \quad =  2 \bbP[ N(0,1) \geq \theta^* \onormSize{}{\mx_1^* - \mx_2^*}_F /(2\sigma) ] \\
       & \quad \geq 2 \bbP[ N(0,1) \geq c_4 ]   \geq c,
   \end{align}

where the first equation holds by symmetry, the third equation holds by rearrangement, the fourth equation holds by the fact that $\ang{\me, \theta^* \mx_1^* - \theta^*\mx_2^* } \sim N(0, \sigma \onormSize{}{\theta^* \mx_1^* - \theta^*\mx_2^*}_F)$, and $c$ is some positive constant in the last inequality.
   %for some positive constant $c$, where the calculation for the last equation follows from the calculation in equation (102) in \cite{han2020exact}.
   
   Plugging the inequality~\eqref{eq:inf} into the inequality~\eqref{eq:inf_lower} for all $i \in T_1^c$, then, we have 
   \begin{equation}
       \liminf_{p \rightarrow \infty}  \inf_{\hat z_1 } \bbE \left[ p \ell(\hat z_1, z_1^*) | z^*_k, \mtheta^*_k, \tS^*  \right]  \geq \liminf_{p \rightarrow \infty} \frac{ C cp }{r^3} \geq C c,
   \end{equation}
   where the last inequality follows by the condition $r = o(p^{1/3})$. By the discrete nature of the misclassification error, we obtain our conclusion
   \begin{equation}
       \liminf_{p \rightarrow \infty} \inf_{\tS^* \in  \tP_{\tS}(\gamma)}  \inf_{\hat z_{\text{stat}} }\sup_{ (z^*, \mtheta^*) \in \tP_{z, \mtheta}} \bbE \left[ p\ell(\hat z_{\text{stat}}, z) \right]  \geq 1. 
   \end{equation}
"
   \end{proof}
    \end{quote}

    
    % In fact, our proof idea of minimax lower bound statements (impossibility in Theorems~\ref{thm:stats} and \ref{thm:comp}) shares the same spirit with reviewer's comment. We briefly describe the proof idea of impossibility below:
    
    % Let $\tP_z$ denote the space for all possible (polynomial-time) community assignment estimation $\hat z$ and $\tP(\gamma) = \tP \cap \{ \Delta_{\rm ang}^2/\sigma^2 = p^{\gamma}\}$ denote the dTBM parameter space with SNR condition. We prove the minimax lower bound by constructing fixed true parameters $(z^*, \mtheta^*, \tS^*) \in \tP(\gamma)$ such that 
    % \begin{equation}\label{eq:impossible_prove}
    %     \inf_{\hat z \in \tP_z} \sup_{(z, \mtheta, \tS) \in \tP(\gamma)} \bbE[p \ell(\hat z, z)] \geq  \inf_{\hat z \in \tP_z} \bbE[p \ell(\hat z, z^*)| (z^*, \mtheta^*, \tS^*)] \geq 1.
    % \end{equation}
    % Here are two details differ with reviewer's comment. First, the SNR condition in $\tP(\gamma)$ is $\{ \Delta_{\rm ang}^2/\sigma^2 = p^{\gamma}\}$ rather than $\{ \Delta_{\rm ang}^2/\sigma^2 \leq p^{\gamma}\}$; otherwise, for any $\gamma \leq 0$, we can always take $\tS^* \in \{\Delta_{\rm ang}^2 = 0\}$ to make dTBM non-identifiable and thereof theoretically impossible. Second, we consider one particular core tensor $\tS^*$ rather than the random tensor with entries $\tS_{ijk} = 1 + \epsilon_{ijk}, \epsilon_{ijk} \sim N(0, \delta^2)$ for some $\delta \rightarrow 0$ as $p \rightarrow \infty$. The expectation in minimax error~\eqref{eq:impossible_prove} is taken with respect to the noise in observation, rather than the randomness in the true parameters. 
    
    % The construction of $(z^*, \mtheta^*, \tS^*) \in \tP(\gamma)$ is the key step in the proof. With a good construction, the second inequality in~\eqref{eq:impossible_prove} can be easily verified with prior works. For statistical lower bound, we take one $(z^*, \mtheta^*, \tS^*)$ with $\mtheta^* = \mathbf{1}$ and $\Delta_{\rm Euc}^2(\tS^*) = p^{\gamma}$; for computational lower bound, we construct a rank-2 $\tS^*$ to relate the HPC conjecture. The inequality~\eqref{eq:impossible_prove} then holds with \citet[Proof of Theorems 6 and 7]{han2020exact}.
    
    % From the perspective of proof, we agree that the minimax lower bound statements can be obtained by exploiting prior results via proof idea~\eqref{eq:impossible_prove}. However, we argue that \textbf{none of our lower bound results are corollaries of \cite{han2020exact} and dTBM-specific techniques are required.} More importantly, we want to emphasize that the proof of dTBM achievability is more difficult than that for TBM due to the extra heterogeneity. 
    
    % In the revision, we have proved a stronger lower bound statement for statistical impossibility as following comment suggested and added more discussion in Section~\ref{sec:tbm}.
    
    % Define $\tP_z \coloneqq \offf{ z:  c_1 p/r \leq |z^{-1}(a)| \leq c_2 p/r, a \in [r]}$, where $c_1, c_2$ are the positive constants in dTBM parameter space~\eqref{eq:family}. The space $\tP_z$ collects all possible community assignment $z$ under the dTBM or TBM. Let $\tP_{\rm dTBM}(\gamma) \coloneqq \tP_{\rm dTBM} \cap \{\Delta_{\rm ang}^2 = p^{\gamma}\}$ refer to the space $\tP(\gamma)$ in our main text. We prove the minimax lower bound by constructing fixed true parameters $(z^*, \mtheta^*, \tS^*) \in \tP_{\rm dTBM}(\gamma)$ such that 
    % \begin{equation}
    %     \inf_{\hat z \in \tP_z} \sup_{(z, \mtheta, \tS) \in \tP_{\rm dTBM}(\gamma)} \bbE[p \ell(\hat z, z)] \geq  \inf_{\hat z \in \tP_z} \bbE[p \ell(\hat z, z^*)| (z^*, \mtheta^*, \tS^*)] \geq 1.
    % \end{equation}
    
    
    % Let $\tP_{\rm TBM}(\gamma) \coloneqq \tP_{\rm TBM} \cap \{\Delta_{\rm Euc}^2 = p^{\gamma}\}$ and $\tP_{\rm dTBM}(\gamma) \coloneqq \tP_{\rm dTBM} \cap \{\Delta_{\rm ang}^2 = p^{\gamma}\}$ denote the parameter space for TBM and dTBM with SNR requirement, where $\tP_{\rm TBM}(\gamma)$ refers to the equation (26) in \cite{han2020exact} and $\tP_{\rm dTBM}(\gamma)$ refers to the space $\tP(\gamma)$ in our main text. 
    
  
    
%     \textbf{Response:} We thank reviewer for the suggestions. Inspired by the reviewer, we have proved a stronger statement for statistical impossibility. The new statement suggests the dTBM statistical impossibility whenever the core tensor $\tS$ leads to a week SNR with exponent $\gamma < -(K-1)$. The proof of our new statement relies on special construction of heterogeneity $\mtheta^*$ in the worst case, and previous conclusions for TBM is non-applicable under the construction. We attach our new statement in Theorem~\ref{thm:stats}, Section~\ref{sec:limits}:

    
%     For self-consistency, we attach the proof of the new impossibility result in Section~\ref{sec:statprove1}:
    
%     \begin{quote}
%       ``
%       \begin{proof}[Proof of Theorem~\ref{thm:stats} (Impossibility)]Consider the general asymetric dTBM~\eqref{eq:general_dtbm} in the special case that $p_k = p$ and $r_k = r$ for all $ k\in [K]$. For simplicity, we show the minimax rate for the estimation on the first mode $\hat z_1$; the proof for other modes are essentially the same. 
   
%   To prove the minimax rate~\eqref{eq:minminmax}, it suffices to take an arbitrary $\tS^* \in  \tP_{\tS}(\gamma)$ wih $\gamma < -(K-1)$ and construct $(z^*_k, \mtheta^*_k)$ such that 
%   \begin{equation}
%       \inf_{\hat z_1} \bbE \left[ p\ell(\hat z_1, z_1^*) | (z^*_k, \mtheta^*_k, \tS^*)  \right]\geq 1.
%   \end{equation}
   
%   We first define a subset of indices $T_k \subset [p_k], k \in [K]$ in order to avoid the complication of label permutation. Based on \citet[Proof of Theorem 6]{han2020exact}, we consider the restricted family of $\hat z_k$'s for which the following three conditions are satisfied:
%   \begin{align}
%         &\text{(a)}\ \hat z_k(i)=z_k(i) \text{ for all }i\in T_k; \quad \text{(b)} \ |T^c_k|\asymp {p\over r}; \\
%         &\text{(c)}\ \min_{\pi\in \Pi}\sum_{i\in[p]}\ind\{\hat z_k(i) \neq \pi\circ z_k (i)\} = \sum_{i\in[p]}\ind\{\hat z_k(i) \neq  z_k (i)\},
%   \end{align}
% for all $k \in [K]$.
%   Now, we consider the construction:
%   \begin{enumerate}
%       \item[(i)] $\{z_k^*\}$ satisfies properties (a)-(c) with misclassification sets $T_k^c$ for all $k \in [K]$;
%       \item [(ii)] $\{\mtheta_k^*\}$ such that $\mtheta_k^*(i) \leq \sigma r^{(K-1)/2} p^{-(K-1)/2}$ for all $i \in T_k^c, k \in [K]$ and $\max_{k \in [K], a \in [r]} \onormSize{}{\mtheta_{k, z^{*, -1}_k(a)}}^2_2 \asymp p/r$.
%   \end{enumerate}
%   % There exists a $(z^*, \mtheta^*) \in \tP_{z, \mtheta}$ satisfying the property (i) and (ii) when $r < p$; otherwise, when $r = p$, we must have $\mtheta^*(i) = 1$ due to the $\ell_1$ norm constraint and we can not satisfy the property (ii).
   
%   Then, following the proof of \cite{gao2018community}, we have 
%   \begin{align}
%         & \inf_{\hat z_1 } \bbE \left[ \ell(\hat z_1, z_1^*) | (z^*_k, \mtheta^*_k, \tS^*)  \right] \geq  \\
%         &\frac{C}{r^3 |T_1^c|} \sum_{i \in T_1^c} \inf_{\hat z_1  (i)} \{ \bbP[\hat z_1(i) = 1| z_1^*(i) = 2, z_k^*, \mtheta_k^*, \tS^*] +   \bbP[\hat z_1(i) = 2| z_1^*(i) = 1, z_k^*, \mtheta_k^*, \tS^*] \},
%   \end{align}
%   where $C$ is some positive constant,  $\hat z_1$ on the left hand side denote the generic assignment functions in $\tP(\gamma)$, and the infimum on the right hand side is taken over the generic assignment function family of $\hat z_1(i)$ for all node $i \in T_1^c$. Here, the factor $r^3=r\cdot r^2$ in~\eqref{eq:inf_lower} comes from two sources: $r^2\asymp {r\choose 2}$ comes from the multiple testing burden for all pairwise comparisons among $r$ clusters; and another $r$ comes from the number of elements $|T^c_k|\asymp p/r$ to be clustered. 
   
%   Next, we need to find the lower bound of the rightmost side in~\eqref{eq:inf_lower}. 
%   %For simplicity, we show the bound for the mode-1 case $k = 1$ only. We drop the subscripts $1$ in $z_1, T_1, \mS_1, \mtheta_1$ and omit the repeated procedures for the cases of $k = 2,\ldots, K$. 
% We consider the hypothesis test based on model~\eqref{eq:general_dtbm}. First, we reparameterize the model under the construction (i)-(ii).
% \begin{equation}
%     \mx_a^* = \off{\mat_1 \of{ \tS^*\times_2 \mTheta_2^* \mM_2^*\times_3\cdots \times_K \mTheta^* \mM_K^* }}_{a:}, \quad \text{for all} \ a \in [r],
% \end{equation}
% where $\mx_a^*$'s are centroids in $\bbR^{p^{K-1}}$. Without loss of generality, we consider the lower bound for the summand in~\eqref{eq:inf_lower} for $i=1$. The analysis for other $i\in T^c_1$ are similar. For notational simplicity, we suppress the subscript $i$ and write $\my, \theta^*, z$ in place of $\my_1, \mtheta_1^*(1)$ and $z_1(1)$, respectively. The equivalent vector problem for assessing the summand in~\eqref{eq:inf_lower} is
% \begin{equation}
% \my=\theta^* \mx_{z}^*+\me,
% \end{equation}
% where $z\in \{1,2\}$ is an unknown parameter, $\theta^* \in \bbR_+$ is the given heterogeneity degree, $\mx_1^*,\mx_2^*\in\bbR^{p^{K-1}}$ are given centroids, and $\me\in\bbR^{p^{K-1}}$ consists of i.i.d.\ $N(0,\sigma^2)$ entries.  Then, we consider the hypothesis testing under the model~\eqref{eq:z}:
% \begin{equation}
%  H_0: z = 1 \text{ and } \my = \theta^* \mx_1^* + \me \quad \leftrightarrow \quad H_1: z = 2 \text{ and } \my = \theta^* \mx_2^* + \me,
% \end{equation}
   
% %   For an arbitrary $i \in T_1^c$, the right hand side in \eqref{eq:inf_lower} corresponds to the following hypothesis test:
% %   \begin{equation}\label{eq:hypo}
% %       H_0: \my = \mtheta_1^*(i) \mx_1^* + \me \quad \leftrightarrow \quad H_1: \my = \mtheta_1^*(i) \mx_2^* + \me,
% %   \end{equation}
% %   where $\my = [\mat_1(\tY)]_{i:}$, $\mx_a^* = [\mat_1( \tS^* \times_2 \mTheta^* \mM_2^* \times_3 \cdots \times_K \mTheta^* \mM_K^*)]_{a:}$, and $\me$ consists of i.i.d.\ $N(0,\sigma^2)$ entries. 
%   The hypothesis test~\eqref{eq:test} is a simple versus simple testing, since the assignment $z$ is the only unknown parameter in the test. 
%   By Neyman-Pearson lemma, the likelihood ratio test is optimal with minimal Type I + II error. Under Gaussian model, the likelihood ratio test is equivalent to the least square estimator $\hat z_{LS} = \argmin_{a = \{1,2\}} \onormSize{}{\my - \theta^* \mx_a^*}_F^2$. 
   
%   Let $\mS = \mat_1(\tS)$. Note that 
%   \begin{align}
%       \onormSize{}{ \theta^* \mx_1^*  - \theta^* \mx_2^*}_F &\leq  \mtheta_1^*(i) \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F \prod_{k = 2}^K \lambda_{\max}(\mTheta_k^*\mM_k^*)  \\
%       & \leq \theta^* \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F   \max_{k \in [K]/\{1\}, a \in [r]} \onormSize{}{\mtheta_{k, z^{*, -1}_k(a)}}_2^{K-1} \\
%       & \leq  \sigma r^{(K-1)/2} p^{-(K-1)/2} 2 c_4 p^{(K-1)/2} r^{-(K-1)/2}\\
%       & \leq 2 c_4 \sigma, 
%   \end{align}
% %   \begin{align}
% %       \onormSize{}{ \mtheta_1^*(i) \mx_1^*  - \mtheta_1^*(i) \mx_2^*}_F &\leq  \mtheta_1^*(i) \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F \prod_{k = 2}^K \lambda_{\max}(\mTheta_k^*\mM_k^*)  \\
% %       & \leq \mtheta_1^*(i) \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F   \max_{k \in [K]/\{1\}, a \in [r]} \onormSize{}{\mtheta_{k, z^{*, -1}_k(a)}}_2^{K-1} \\
% %       & \leq  \sigma r^{(K-1)/2} p^{-(K-1)/2} 2 c_4 p^{(K-1)/2} r^{-(K-1)/2}\\
% %       & \leq 2 c_4 \sigma, \label{eq:bound}
% %   \end{align}
%   where the second inequality follows Lemma~\ref{lem:singular_thetam} and the third inequality follows from property (ii) and the boundedness constraint in $\tP_{\tS}(\gamma)$ such that $\onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F  \leq \onormSize{}{\mS_{1:}^*}_F + \onormSize{}{\mS_{2:}^*}_F \leq 2c_4$.

   
%   Hence, we have 
%   \begin{align}
%       & \inf_{\hat z_1(1)} \{ \bbP[\hat z_1(1) = 1| z_1^*(1) = 2, z^*_k,  \mtheta_k^*, \tS^*] +   \bbP[\hat z_1(1) = 2| z_1^*(1) = 1, z^*_k,  \mtheta_k^*, \tS^*] \} \\
%       & \quad = 2 \bbP[ \hat z_{LS} = 1 |  z_1^*(1) = 2, z^*_k,  \mtheta_k^*, \tS^* ] \\
%       & \quad  = 2 \bbP[ \onormSize{}{\my - \theta^* \mx_1^*}_F^2 \leq \onormSize{}{\my - \theta^* \mx_2^*}_F^2 | z_1^*(1) = 2, z^*_k , \mtheta_k^*, \tS^*   ] \\
%       & \quad = 2 \bbP \off{ 2 \ang{\me, \theta^* \mx_1^* - \theta^*\mx_2^* } \geq  \onormSize{}{\theta^* \mx_1^* - \theta^*\mx_2^*}_F^2 }\\
%       & \quad =  2 \bbP[ N(0,1) \geq \theta^* \onormSize{}{\mx_1^* - \mx_2^*}_F /(2\sigma) ] \\
%       & \quad \geq 2 \bbP[ N(0,1) \geq c_4 ]   \geq c,
%   \end{align}
% %   \begin{align}
% %       & \inf_{\hat z_1(1)} \{ \bbP[\hat z_1(1) = 1| z_1^*(i) = 2, z^*_k,  \mtheta_k^*, \tS^*] +   \bbP[\hat z_1(1) = 2| z_1^*(i) = 1, z^*_k,  \mtheta_k^*, \tS^*] \} \\
% %       & \quad = 2 \bbP[ \hat z_{LS}(i) = 1 |  z_1^*(i) = 2, z^*_k,  \mtheta_k^*, \tS^* ] \\
% %       & \quad  = 2 \bbP[ \onormSize{}{\my - \mtheta_1(i)^* \mx_1^*}_F^2 \leq \onormSize{}{\my - \mtheta_1(i)^* \mx_2^*}_F^2 | z_1^*(i) = 2, z^*_k , \mtheta_k^*, \tS^*   ] \\
% %       & \quad =  2 \bbP[ N(0,1) \geq \mtheta_1(i)^* \onormSize{}{\mx_1^* - \mx_2^*}_F /\sigma ] \\
% %       & \quad \geq 2 \bbP[ N(0,1) \geq 2 c_4 ]   \geq c, \label{eq:inf}
% %   \end{align}
% where the first equation holds by symmetry, the third equation holds by rearrangement, the fourth equation holds by the fact that $\ang{\me, \theta^* \mx_1^* - \theta^*\mx_2^* } \sim N(0, \sigma \onormSize{}{\theta^* \mx_1^* - \theta^*\mx_2^*}_F)$, and $c$ is some positive constant in the last inequality.
%   %for some positive constant $c$, where the calculation for the last equation follows from the calculation in equation (102) in \cite{han2020exact}.
   
%   Plugging the inequality~\eqref{eq:inf} to inequality~\eqref{eq:inf_lower}, then, we have 
%   \begin{equation}
%       \liminf_{p \rightarrow \infty}  \inf_{\hat z_1 } \bbE \left[ p \ell(\hat z_1, z_1^*) | z^*_k, \mtheta^*_k, \tS^*  \right]  \geq \liminf_{p \rightarrow \infty} \frac{ C cp }{r^3} \geq C c,
%   \end{equation}
%   where the last inequality follows by the condition $r = o(p^{1/3})$. By the discrete nature of the misclassification error, we have 
%   \begin{equation}
%       \liminf_{p \rightarrow \infty} \inf_{\tS^* \in  \tP_{\tS}(\gamma)}  \inf_{\hat z_{\text{stat}} }\sup_{ (z^*, \mtheta^*) \in \tP_{z, \mtheta}} \bbE \left[ p\ell(\hat z_{\text{stat}}, z) \right]  \geq 1. 
%   \end{equation}
% %   for any $r < p$ and $p$,  we have 
% %   \begin{equation}
% %         \inf_{\hat z_1 \in \tP_z} \bbE \left[ p \ell(\hat z_1, z_1^*) | z^*_k, \mtheta^*_k, \tS^*  \right] \geq \frac{ cp }{r^3} > 0,
% %   \end{equation}
% %   which indicates 
% %   \begin{equation}
% %         \inf_{\hat z_1 \in \tP_z} \bbE \left[ p \ell(\hat z_1, z_1^*) | z^*_k, \mtheta^*_k, \tS^*  \right] \geq 1, \quad \text{for every } p
% %   \end{equation}
% %   by the discrete nature of $p\ell(\hat z, z^*)$.
%   \end{proof} "
%     \end{quote}
    
    \item \textit{Finally, I checked the author’s proof towards the statement. I think the proofs need to be revised substantially as I found several flaws:} 
    
    \textbf{Response:} We thank reviewer for pointing out these issues. We have provided a new proof of Theorem~\ref{thm:stats} for the new statistical impossibility statement. We will reply to each issue based on the related parts in the new proof. 
    
    %Since we have proved a new stronger statement for statistical impossibility, most of following concerns no longer exist in the new proof of Theorem~\ref{thm:stats}. We still thank reviewer for pointing out these concerns and answer each sub-question for clarification.
    
    \begin{itemize}
        \item \textit{Page 32, Eqn (31) suggests that $\theta$ is fixed and the decision problems are reduced to $z$ from that point. But in latter argument $\theta$ is taken as unknown parameter.}
        
        \textbf{Response:} We agree that the inconsistent status of $\mtheta$ is a flaw in previous proof of Theorem~\ref{thm:stats}. Once we fix the true $\mtheta$ at the beginning (Eqn (31) of previous proof), we should take true $\mtheta$ as a known parameter in the rest of the proof. Specifically, in previous Eqn (33), the model of $\my$
        \begin{equation}
            \my = \theta \mx_z + \me
        \end{equation}
        should take $\theta$ as a known parameter. Because the distribution of observation $\my$ corresponds to the conditional probability $\bbP[\cdot | z_k(i)]$ in previous Eqn (31). When the true assignment $z_k(i)$ is known, the distribution of $\my$ should be known with true heterogeneity $\theta$ and true centroid $\mx_{z_k(i)}$. To avoid this flaw in the new proof, we use new notations $z^*_k, \mtheta^*_k, \tS^*_k$ for true parameters to better distinguish the known and unknown parameters. We also re-write the conditional probability as $\bbP[\cdot | z^*_k(i), z_k^*, \mtheta^*_k, \tS^*]$ for better expression.  We attach the related parts in the new proof in Appendix~\ref{sec:statprove1} here:
        
        \begin{quote}
            ``...
            To prove the minimax rate~\eqref{eq:minminmax}, it suffices to take an arbitrary $\tS^* \in  \tP_{\tS}(\gamma)$ wih $\gamma < -(K-1)$ and construct $(z^*_k, \mtheta^*_k)$ such that 
   \begin{equation}
       \inf_{\hat z_1} \bbE \left[ p\ell(\hat z_1, z_1^*) | (z^*_k, \mtheta^*_k, \tS^*)  \right]\geq 1.
   \end{equation}
   ..."
        \end{quote}
        
        \begin{quote}
            ``... we have 
   \begin{align}
        & \inf_{\hat z_1 } \bbE \left[ \ell(\hat z_1, z_1^*) | (z^*_k, \mtheta^*_k, \tS^*)  \right] \geq  \\
        & \frac{C}{r^3 |T_1^c|} \sum_{i \in T_1^c} \inf_{\hat z_1  (i)} \{ \bbP[\hat z_1(i) = 1| z_1^*(i) = 2, z_k^*, \mtheta_k^*, \tS^*] +   \bbP[\hat z_1(i) = 2| z_1^*(i) = 1, z_k^*, \mtheta_k^*, \tS^*] \},
   \end{align}
    where $C$ is some positive constant, ..."
        \end{quote}
        
        \begin{quote}
            `` ...  The equivalent vector problem for assessing the summand in~\eqref{eq:inf_lower} is
\begin{equation}
\my=\theta^* \mx_{z}^*+\me,
\end{equation}
where $z\in \{1,2\}$ is an unknown parameter, $\theta^* \in \bbR_+$ is the given heterogeneity degree, $\mx_1^*,\mx_2^*\in\bbR^{p^{K-1}}$ are given centroids, and $\me\in\bbR^{p^{K-1}}$ consists of i.i.d.\ $N(0,\sigma^2)$ entries.  ..."
        \end{quote}
        
        
        \item \textit{Page 33, Line 9: let’s say the authors may want to make $\theta$ unknown and involve it in the decision problem. The authors formulate a hypothesis testing for model (33). My question is, is this really a hypotheses testing? Eqn. (30) involves both $z$ and $\theta$ as unknown parameters. Under null ($z = 1$), the distribution of $y$ is still unknown.}
        
        \textbf{Response:} This is a follow-up question of last comment. We agree that involving $\mtheta$ as an unknown parameter in the model of $\my$ and hypothesis testing is in appropriate. Following our last response, we consider the true heterogeneity $\mtheta^*$ as known in the model of $\my$. Hence, we have a valid simple hypothesis testing problem, in which the assignment function $z$ is the only unknown parameter in the test. Under the null hypothesis $(z = 1)$, the observation $\my = \theta^* \mx_1^* + \me$ follows multivariate Gaussian distribution with known mean vector $\theta^* \mx_1^*$ and covariance matrix $\sigma^2 \mI$. For clarification, we re-write the hypothesis testing with the model of $\my$. We attach the related parts in the new proof in Appendix~\ref{sec:statprove1} here:
        
        \begin{quote}
            ``...
            The equivalent vector problem for assessing the summand in~\eqref{eq:inf_lower} is
\begin{equation}
\my=\theta^* \mx_{z}^*+\me,
\end{equation}
where $z\in \{1,2\}$ is an unknown parameter, $\theta^* \in \bbR_+$ is the given heterogeneity degree, $\mx_1^*,\mx_2^*\in\bbR^{p^{K-1}}$ are given centroids, and $\me\in\bbR^{p^{K-1}}$ consists of i.i.d.\ $N(0,\sigma^2)$ entries.  Then, we consider the hypothesis testing under the model~\eqref{eq:z}:
\begin{equation}
 H_0: z = 1 \text{ and } \my = \theta^* \mx_1^* + \me \quad \leftrightarrow \quad H_1: z = 2 \text{ and } \my = \theta^* \mx_2^* + \me,
\end{equation}
...
            "
        \end{quote}
        
        % Based on last sub-question, we agree that the $\theta$ should be considered as a known parameter in model (33). The hypothesis testing from model (33) is equivalent to
        % \begin{equation}\label{eq:hypo_revise}
        %     H_0: \my = \theta \mx_1 + \me \quad \text{v.s.} \quad H_1: \my = \theta \mx_2 + \me,
        % \end{equation}
        % where $\theta, \mx_1, \mx_2$ are known parameters, $\me$ consists of i.i.d.\ $N(0, \sigma^2)$ entries. Now, the null and alternative distri-0bution of $\my$ is known, and the test~\eqref{eq:hypo_revise} is a valid hypothesis testing problem.
        
        \item \textit{A follow-up question is why could you apply Neyman-Pearson Lemma to lower bound the decision risks? Apparently this is not a simple testing, and the authors should provide the justification on why the presented “profiled MLE” works (technically) in determining the lower bound.}
        
        \textbf{Response:} The revised hypothesis testing in new Eqn~\eqref{eq:test} is a simple $(z = 1)$ versus simple $(z = 2)$ testing problem. Because the assignment function $z$ is the only unknown parameter in test. Therefore, we are able to apply Neyman-Pearson Lemma to our testing problem. By Neyman-Pearson Lemma, the likelihood ratio test, which coincides to the least square estimator of $z$ under our Gaussian model, leads to the minimal Type I + II error. As $\theta^*$ is a known parameter in the new proof, we no longer use the ``profile MLE" to determine the lower bound. We attach the related parts in the new proof in Appendix~\ref{sec:statprove1} here:
        
        \begin{quote}
            ``
            ... The hypothesis testing~\eqref{eq:test} is a simple versus simple testing, since the assignment $z$ is the only unknown parameter in the test. By Neyman-Pearson lemma, the likelihood ratio test is optimal with minimal Type I + II error. Under Gaussian model, the likelihood ratio test of \eqref{eq:test} is equivalent to the least square estimator $\hat z_{LS} = \argmin_{a = \{1,2\}} \onormSize{}{\my - \theta^* \mx_a^*}_F^2$.  ...
            "
        \end{quote}
        
        \item \textit{Although I don’t understand why $\hat \theta_{\rm MLE}$ is introduced here, I feel its calculation is incorrect. Does the authors consider the fact that there is a pre-specified parameter space for $\theta$ and it cannot take all values on $\bbR^+$ ?}
        
        \textbf{Response:} In the new proof, we no longer introduce the estimator $\hat \theta_{\rm MLE}$ and $\hat z_{\rm MLE}$ defined in the previous proof. Instead, we consider the least square estimator by Neyman-Pearson Lemma. In general, we agree that the MLE of $(z, \mtheta, \tS)$ should lie in the pre-specified parameter space $\tP$ to guarantee the identifiability. We emphasize this point in the formal definition of MLE~\eqref{eq:mle} in the main text. We attach the definition in Section~\ref{sec:limits} here:
        \begin{quote}
            ``
            We consider the Gaussian MLE, denoted as $(\hat z_{\text{MLE}}, \hat \mtheta_{\text{MLE}}, \hat \tS_{\text{MLE}})$, over the estimation space $\tP$, where 
\begin{equation}
    (\hat z_{\text{MLE}}, \hat \mtheta_{\text{MLE}}, \hat \tS_{\text{MLE}}) = \argmin_{ (z, \mtheta, \tS) \in \tP} \onormSize{}{\tY - \tX(z,\mtheta, \tS)}_F^2.
\end{equation}
...
            "
        \end{quote}
        
        
    \end{itemize}
    \end{enumerate} 
    
    
\end{enumerate}

\newpage
\begin{center}
    \textbf{Point-by-point response to Reviewer 2}
\end{center}

\textit{I thank the authors for the signiﬁcant revision that clariﬁes the issues that I raised. I still suggest the authors to emphasise early that this paper focuses on dense regime. For instance, they could include sparsity in Table 1 and mention that Ahn et al (2018) proves exact recovery in the sparse regime (which this paper and few others in the list don't). This would provide a clear (unbiased) comparison of the results.}

\textbf{Response:} We thank reviewer for the reference. We have included the reference \cite{ahn2018hypergraph} in Table~\ref{tab:comp}:

\begin{quote}
    \begin{table*}[ht]
\resizebox{\textwidth}{!}{%
    \begin{tabular}{c|cccccc}
\hline
    & \cite{gao2018community}&  \cite{ahn2018hypergraph} &\cite{han2020exact}&  \cite{ghoshdastidar2017consistency} &\cite{ke2019community} & \textbf{Ours}\\
    \hline
      Allow tensors of arbitrary order & $\times$ & $\surd$&  $\surd$ & $\surd$ &$\surd$ & $\surd$  \\
        Allow degree heterogeneity &  $\surd$ & $\times$ & $\times$ & $\surd$ &$\surd$ & $\surd$ \\
        Singular-value gap-free clustering &  $\surd$ & $\surd$ & $\surd$ & $\times$ & $\times$ &$\surd$ \\
        Misclustering rate (for order $K^*$)& - & ${p^{-(K-1)} \alpha^{-1}_p}^{**}$ & $\exp(-p^{K/2})$ & $p^{-1}$& $p^{-2}$ &  $\exp(-p^{K/2})$\\
        Consider sparse observation & $\times$ & $\surd$ & $\times$ & $\times$ & $\times$ &$\times$\\
        \hline
    \end{tabular}
    }
    \caption{ Comparison between previous methods with our method. $^*$We list the result for order-K tensors with $K \geq 3$ and general number of communities $r = \tO(1)$. $^{**}$The parameter $\alpha = f(p) > 0$ denotes the sparsity level which is some function of dimension $p$. }
\end{table*}
\end{quote}

We have added a brief clarification in the related work, Section~\ref{sec:intro}:
\begin{quote}
    ``... Some works~\citep{ahn2018hypergraph} study the TBM with sparse observations, while, others~\citep{wang2019multiway, han2020exact} and our work focus on the dense regime. ..."
\end{quote}

We also added the reference in the extension to Bernoulli clustering, Section~\ref{subsec:ber}:
\begin{quote}
    ``The sparsity is often a popular feature in hypergraphs~\citep{florescu2016spectral,ke2019community, ahn2018hypergraph}.
    "
\end{quote}


\newpage
% \section{Notes}


% \textbf{Claim:} The statistical impossibility in dTBM can be easily obtained from the TBM paper. 
    
%     \textcolor{blue}{ Note that ``easily obtained" is not ``directly obtained". See following arguments.}
    
%     Let $\tP_{\rm dTBM}$ and $\tP_{\rm TBM}$ denote the true parameter spaces for dTBM and TBM models. There is no belonging relation between $\tP_{\rm dTBM}$ and $\tP_{\rm TBM}$. Examples: if $\mtheta \neq \mathbf{1}$, then we have $(z, \mtheta, \tS) \in \tP_{\rm dTBM}$ but $(z, \mtheta, \tS) \notin \tP_{\rm TBM}$; if $\tS$ has $\onormSize{}{\mS_{a:}}_F = 0$ and $\mtheta = \mathbf{1}$, then we have $(z, \mtheta, \tS) \in \tP_{\rm TBM}$ but $(z, \mtheta, \tS) \notin \tP_{\rm dTBM}$. Also, there is no belonging relation between $\tP_{\rm dTBM}(\gamma) \coloneqq \tP_{\rm dTBM} \cap \{ \Delta_{\rm ang}^2 \lesssim p^{\gamma} \}$ and $\tP_{\rm TBM}(\gamma)  \coloneqq \tP_{\rm TBM} \cap \{ \Delta_{\rm Euc}^2 \lesssim p^{\gamma} \}$.
    
%     Hence, for any estimator $\hat z$, we can not infer the worst case error from one case to another case; i.e.,
%     \begin{equation}
%         \sup_{(z, \mtheta, \tS) \in \tP_{\rm dTBM}(\gamma)} p\ell(\hat z, z)  >  1 \quad \nRightarrow \text{and} \nLeftarrow \quad  \sup_{(z, \mtheta, \tS) \in \tP_{\rm TBM}(\gamma)} p\ell(\hat z, z) > 1.
%     \end{equation}
    
%     That's why we can not ``\emph{directly} obtain" the dTBM impossibility from TBM impossibility, as previous Remark 1 said. But we can ``\emph{easily}" obtain dTBM result by using the proof of TBM impossibility. 
    
%     Note that there exists a set of parameters $(z^*, \mtheta^*, \tS^*)$ with $\mtheta^* = \mathbf{1}$ such that $(z^*, \mtheta^*, \tS^*) \in \tP_{\rm dTBM}(\gamma) \cap \tP_{\rm TBM}(\gamma)$. In the proof of TBM impossibility in \cite{han2020exact}, we eliminate the supreme by considering the special case with $(z^*, \mtheta^*, \tS^*)$ 
%     \begin{equation}
%         \inf_{\hat z: (\hat z, \hat \mtheta, \hat \tS) \in \tP_{\rm TBM}}  \sup_{(z, \mtheta, \tS) \in \tP_{\rm TBM}(\gamma)} p\ell(\hat z, z) \geq \inf_{\hat z: (\hat z, \hat \mtheta, \hat \tS) \in \tP_{\rm TBM}}    p\ell(\hat z, z^*)  > 1.
%     \end{equation}
%     Then, in dTBM, we also can eliminate the supreme by considering the same special case with $(z^*, \mtheta^*, \tS^*)$ 
%     \begin{equation}
%         \inf_{\hat z: (\hat z, \hat \mtheta, \hat \tS) \in \tP_{\rm dTBM}}  \sup_{(z, \mtheta, \tS) \in \tP_{\rm dTBM}(\gamma)} p\ell(\hat z, z) \geq \inf_{\hat z: (\hat z, \hat \mtheta, \hat \tS) \in \tP_{\rm dTBM}}    p\ell(\hat z, z^*)
%     \end{equation}
%     Notice that the infimum are taken under different regimes. We only need to show that 
%     \begin{equation}\label{eq:inf}
%         \inf_{\hat z: (\hat z, \hat \mtheta, \hat \tS) \in \tP_{\rm dTBM}}    p\ell(\hat z, z^*) = \inf_{\hat z: (\hat z, \hat \mtheta, \hat \tS) \in \tP_{\rm TBM}} p\ell(\hat z, z^*). 
%     \end{equation}
%     The equation~\eqref{eq:inf} is true. Because under $(z^*, \mtheta^*, \tS^*)$ with $\mtheta^* = \mathbf{1}$, it is equivalent to consider the UMP of the vector hypothesis test problem
%     \begin{equation}
%         H_0: \my = \mx_1 + \me \quad vs \quad H_1: \my = \mx_2 + \me,
%     \end{equation}
%     where $\mx_a$ are known centroids and $\me$ consists of i.i.d.\ $N(0,\sigma^2)$. By Neyman-Pearson Lemma, the UMP is the non-degree least square estimator $\hat z_{\rm UMP} = \argmin_{a \in \{1,2\}} \onormSize{}{\my - \mx_a}^2_F$, which is independent with the estimation $\hat \mtheta$. Then, we finish the proof
%     \begin{equation}
%         \inf_{\hat z: (\hat z, \hat \mtheta, \tS) \in \tP_{\rm dTBM}}    p\ell(\hat z, z^*) = p\ell(\hat z_{\rm UMP}, z^*) = \inf_{\hat z: (\hat z, \hat \mtheta, \tS) \in \tP_{\rm TBM}} p\ell(\hat z, z^*)  > 1
%     \end{equation}
    

%  \textbf{Claim:} The minimax errors over $\hat z$ and over $(\hat z, \hat \mtheta)$ are the same in our case; i.e.,
%     \begin{equation}
%         \inf_{\hat z: (\hat z, \hat \mtheta, \hat \tS) \in \tP_{\rm dTBM}} \sup_{(z^*, \mtheta^*, \tS^*) \in \tP_{\rm dTBM}(\gamma)} \ell(\hat z, z^*) =  \inf_{(\hat z, \hat \mtheta): (\hat z, \hat \mtheta, \hat \tS) \in \tP_{\rm dTBM}} \sup_{(z^*, \mtheta^*, \tS^*) \in \tP_{\rm dTBM}(\gamma)} \ell(\hat z, z^*).
%     \end{equation}
    
%     \textbf{Proof:} Note that the parts after the infimum are the same. Let $f(\hat z) = \sup_{(z^*, \mtheta^*, \tS^*) \in \tP_{\rm dTBM}(\gamma)} \ell(\hat z, z^*)$. Also note that for some parameters $x,y$, function $f$, and area $\mathbb{A}$ we use the notation
%     \begin{equation}
%         \{ x: f(x,y) \in \mathbb{A}\} \coloneqq \{ x: \text{there exists a } y \text{ such that } f(x,y) \in \mathbb{A}\} = \{x: \exists y \text{ s.t.} f(x,y) \in \mathbb{A} \}.
%     \end{equation}
%     It is equivalent to consider
%     \begin{equation}
%         \inf_{\hat z: \exists (\hat \mtheta, \hat \tS) \text{ s.t.} (\hat z, \hat \mtheta, \hat \tS) \in \tP_{\rm dTBM}}  f(\hat z) \quad \text{and} \quad \inf_{(\hat z, \hat \mtheta): \exists \hat \tS \text{ s.t.}  (\hat z, \hat \mtheta, \hat \tS) \in \tP_{\rm dTBM}} f(\hat z).
%     \end{equation}
%     Then, we have
%     \begin{align}
%          \inf_{(\hat z, \hat \mtheta): \exists \hat \tS \text{ s.t.}  (\hat z, \hat \mtheta, \hat \tS) \in \tP_{\rm dTBM}} f(\hat z) &=  \inf_{\hat z': \exists (\hat \mtheta, \hat \tS) \text{ s.t.} (\hat z', \hat \mtheta, \hat \tS) \in \tP_{\rm dTBM}}  \inf_{  \hat \mtheta': \exists \hat \tS \text{ s.t.} (\hat z', \hat \mtheta', \hat \tS) \in  \tP_{\rm dTBM} } f(\hat z')  \\
%          & =  \inf_{\hat z': \exists (\hat \mtheta, \hat \tS) \text{ s.t.} (\hat z', \hat \mtheta, \hat \tS) \in \tP_{\rm dTBM}} f(\hat z'),
%     \end{align}
%     where the last equation follows from the fact that $f(\hat z)$ is independent with $\hat \mtheta$. Therefore, we have proven the claim. 
    
    

% \textbf{Claim:} Let $(z^*, \mtheta^*, \tS^*)$ denote the true parameters in Eqn (31). The hypothesis test corresponds to Eqn (32) should be 
%     \begin{equation}
%         H_0: \my = \theta^* \mx_1^* + \me, \quad vs \quad H_1: \my = \theta^* \mx_2^* + \me,
%     \end{equation}
%     where $\mx_a^* = [\Mat_1(\tS^* \times_2 \mM_2^* \times_2 \cdots \times_K \mM_k^*)]_{a:}$.
    
%     \textbf{Proof:} In all impossibility proofs (Eqn(32) in our proof, \cite{han2020exact}, and \cite{gao2018community}), we want to lower bound the following probability
%     \begin{equation}\label{eq:hypo}
%         \inf_{\hat z \in \tP} \{ \bbP(\hat z = 1| z^* = 2) + \bbP(\hat z = 2|z^* = 1) \},
%     \end{equation}
%     where the $\tP$ is the estimation space of $\hat z$ and $z^*$ is the true assignment. In our dTBM, we have $\tP = \{\hat z: (\hat z, \hat \mtheta, \hat \tS) \in \tP_{\rm dTBM}\}$, and the $\bbP(\cdot | z^*)$ should refer to the distribution of observation $\my$ with true parameter $z^*, \mtheta^*, \tS^*$, denoted as $\bbP(\cdot | z^*, \mtheta^*, \tS^*)$. Then, the hypothesis test corresponds to the probability~\eqref{eq:hypo} is 
%     \begin{equation}
%         H_0: \my = \theta^* \mx^*_1 + \me \quad vs \quad H_1: \my = \theta^* \mx_2^* + \me.
%     \end{equation}
%     The hypothesis test should not be affected by the estimation procedure of $\hat z$. 
    
%     If we write the hypothesis test with unknown $\theta$
%     \begin{equation}
%         H_0: \my = \theta \mx^*_1 + \me \quad vs \quad H_1: \my = \theta \mx_2^* + \me,
%     \end{equation}
%     then we do not have a valid null/alternative distribution. In addition, from the estimation perspective, parameter $\mx_a$ is also unknown in dTBM problem. 
    
%     \textbf{Claim:} Both \cite{han2020exact} and \cite{gao2018community} proves minimax bound
%     \begin{equation}\label{eq:minimax}
%         \inf_{\hat z} \sup_{(z, \tS, \mtheta) \in \tP(\gamma)} \bbE[p \ell(\hat z, z)] \geq 1,
%     \end{equation}
%     rather than the minimin bound
%     \begin{equation}\label{eq:minimin}
%          \inf_{\tS \in \tP(\gamma)} \inf_{\hat z} \sup_{(z, \mtheta)} \bbE[p \ell(\hat z, z)] \geq 1.
%     \end{equation}
    
%     \textbf{Proof:} 
    
%     - For TBM \cite{han2020exact}, the parameter space is defined as 
%     \begin{equation}\label{eq:space}
%         \tP_{\rm TBM}(\gamma) = \tP_{\rm TBM} \cap \{\Delta_{\rm Euc}^2 \gtrsim p^{\gamma}\}.
%     \end{equation}
%     To prove the minimax rate~\eqref{eq:minimax}, we consider the boundary case $\tS^*$ such that $\Delta_{\rm Euc}^2(\tS^*) = p^{\gamma}$. 
    
%     For minimin bound~\eqref{eq:minimin}, with any $\gamma$, we can always have 
%     \begin{equation}
%          \inf_{\tS \in \tP_{\rm TBM}(\gamma)} \inf_{\hat z} \sup_{z} \bbE[p \ell(\hat z, z)] \leq \inf_{\hat z} \sup_{z^*} \bbE[p \ell(\hat z, z^*)] \leq 0,
%     \end{equation}
%     where the inequality holds with true parameter $(z^*, \tS^*)$ which lies in the strong signal regime to exact recovery. 
    
%   - For DCBM \cite{gao2018community}, we have parameter space 
%   \begin{equation}\label{eq:space_dcbm}
%       \tP_{\rm DCBM}(\gamma) = \tP_{\rm DCBM} \cap \{ \min_u B_{uu} - \max_{u \neq v} B_{uv} \geq p-q = \gamma \},
%   \end{equation}
%   where $\min_u B_{uu} - \max_{u \neq v} B_{uv}$ denotes the minimal difference between the diagonal and off-diagonal elements in the mean matrix, and $p-q$ can be considered as the SNR in this case. 
   
%   To prove the minimax rate~\eqref{eq:minimax}, \cite{gao2018community} considers the boundary case where $\min_u B^*_{uu} - \max_{u \neq v} B^*_{uv} = p^* - q^*$. 
   
%   For minimin bound~\eqref{eq:minimin}, with any $\gamma \in [0,1]$, ww can always have 
%   \begin{equation}
%       \inf_{B \in \tP_{\rm DCBM}(\gamma)} \inf_{\hat z} \sup_{z, \mtheta} \bbE[p \ell(\hat z, z)]  \leq \inf_{\hat z} \sup_{z^*, \mtheta^*} \bbE[p \ell(\hat z, z^*)] \leq 0,
%   \end{equation}
%   where the inequality holds with true parameters $B^*$ such that $p^* - q^*$ lies in the strong signal regime. 
   
%   \textit{Therefore, the minimin bound~\eqref{eq:minimin} is trivial or meaningless under the TBM or DCBM case with parameter spaces~\eqref{eq:space} or \eqref{eq:space_dcbm}.}
   
%   - In our case, we have parameter space 
%   \begin{equation}
%       \tP_{\rm dTBM}(\gamma) = \tP_{\rm dTBM} \cap \{ \Delta_{\rm ang }^2 = p^{\gamma}\},
%   \end{equation}
%   For the minimax rate~\eqref{eq:minimax}, we consider the boundary case $(z^*, S^*, \mtheta^*) \in \tP_{\rm dTBM}(\gamma) \cap \tP_{\rm TBM}(\gamma)$.
   
%   However, the minimin rate~\eqref{eq:minimin} is not trivial for dTBM since $\tP_{\rm dTBM}(\gamma)$ does not include the $\tS^*$ that leads to arbitrarily large signal. If we revise the space as $ \tP_{\rm dTBM}'(\gamma) = \tP_{\rm dTBM} \cap \{ \Delta_{\rm ang }^2 \geq p^{\gamma}\}$, then the minimin rate is trivial as TBM and DCBM. This definition revision from $\tP_{\rm dTBM}$ to $\tP'_{\rm dTBM}$ won't affect results in old submissions but lead to an easier defense against the minimin rate comment.
   
%   \newpage 
   
%   \textbf{Minimax vs Minimin rate.} The reviewer mention the minimax and minimin rate under the dTBM:
%   \begin{align}
%       {\rm Minimax:} \quad  \inf_{\hat z} \sup_{(z, \theta, S) \in \tP(\gamma)} \bbE[p l(\hat z,  z)] \geq 1\quad \quad 
%       {\rm Minimin:} \quad \inf_{S \in P(\gamma)} \inf_{\hat z} \sup_{(z, \theta) \in \tP(\gamma)} \bbE[p l(\hat z,  z)]\geq 1.
%   \end{align}
%   To analyze the minimax and minimin rate rigorously, we firstly recall and define several spaces for the estimator and true parameters.
%   \begin{enumerate}[wide]
%       \item $\tP_{\rm dTBM}, \tP_{\rm TBM}, \tP_{\rm DCBM}$: general parameter spaces for our dTBM, TBM \cite{han2020exact}, and matrix DCBM \cite{gao2018community};
%       \item $\tP_{\rm dTBM} (\gamma) = \tP_{\rm dTBM} \cap \{ \Delta_{\rm ang}^2 = p^{\gamma} \}$: dTBM parameter space with angle gap equal to $p^{\gamma}$;
%       \item $\tP_{\rm TBM} (\gamma) = \tP_{\rm TBM} \cap \{ \Delta_{\rm Euc}^2 = p^{\gamma} \}$: TBM parameter space with Euclidean gap equal to $p^{\gamma}$;
%       \item $\tP_{\rm DCBM} (\gamma) = \tP_{\rm TBM} \cap \{ \min_u B_{uu} - \max_{u \neq v} B_{uv} = \gamma \}$: DCBM parameter space with diagonal-offdiagonal gap equal to $\gamma$;
%       \item $\tP'_{\rm dTBM}(\gamma) = \tP_{\rm dTBM} \cap \{ \Delta_{\rm ang}^2 \geq  p^{\gamma} \}$:  dTBM parameter space with angle gap equal or \textbf{larger} than $p^{\gamma}$; similar for $\tP'_{\rm TBM}(\gamma)$ and $\tP'_{\rm DCBM}(\gamma)$;
%       \item $\tP_{z} = \{ z: c_1 p/r |z^{-1}(a)| \leq c_2 p/r \}$: space for all possible assignment satisfying the group size balance assumption; we omit the subscript $k$ for simplicity; $\tP_z$ should be the estimation space for $\hat z$ in dTBM, TBM, and DCBM, because all three models share the same balance constraint on $z$. 
%       \item $\tP_{\mtheta, {\rm dTBM}} = \{ \mtheta: \text{there exists } (z, \tS) \text{ such that } (z, \mtheta, \tS) \in \tP_{\rm dTBM}\}$: the space for all $\mtheta$ satisfying the dTBM requirement; similar to replace the subscript by $\tS$ or $(z,\mtheta)$ and similar to define $\tP_{\mtheta, {\rm dTBM}}(\gamma)$ and with other models.
%     \end{enumerate}
%   Equipped with these spaces, we re-state the minimax and minimin rates rigorously:
%   \begin{align}
%       {\rm Minimax:}& \quad  \inf_{\hat z \in \tP_z} \sup_{(z, \mtheta, \tS) \in \tP_{\rm dTBM}(\gamma)} \bbE[p \ell(\hat z,  z)] \geq 1 \label{eq:minimax} \\ 
%       {\rm Minimin:}& \quad \inf_{\tS \in \tP_{\tS, {\rm dTBM}}(\gamma)} \inf_{\hat z \in \tP_z} \sup_{(z, \mtheta) \in \tP_{(z,\mtheta), \rm dTBM}(\gamma)} \bbE[p \ell(\hat z,  z)]\geq 1. \label{eq:minimin}
%   \end{align}
%   Moreover, the space $\tP(\gamma)$ under the supreme can be replaced by the space $\tP'(\gamma)$, and these rates also holds for TBM and DCBM. 
   
%   The comparison between minimax and minimin rates depends on spaces related to the signal; the minimax rates are invariant to the choice of $\tP(\gamma)$ and $\tP'(\gamma)$ while the minimin results are quite different with different spaces. The minimax rates under all models have been studied, so we focus on the minimin results here.
   
%   \begin{itemize}
%   \item \textbf{Choose $\tP'(\gamma).$} In \cite{han2020exact} and \cite{gao2018community}, we are actually using $\tP'(\gamma)$ rather than $\tP(\gamma)$. The minimin lower bound becomes meaningless under this case for all dTBM, TBM, and DCBM. We take TBM for detailed illustration. 
   
%   \textbf{Claim:} For any $\gamma \leq 0$, 
%   \begin{equation}
%       \inf_{\tS \in \tP'_{\tS, {\rm TBM}}(\gamma)} \inf_{\hat z \in \tP_z} \sup_{z \in \tP'_{z, \rm TBM}(\gamma)} \bbE[p \ell(\hat z,  z)] = 0.
%   \end{equation}
   
%   The intuition is that we need to consider all $\tS^*$ with large signal in $\tP'(\gamma)$ due to the first infimum.
   
%   \textbf{Proof:} For any $\gamma \leq 0$, we always have $\tS^* \in \tP'_{\tS, {\rm TBM}}(\gamma)$ such that $\Delta_{\rm Euc}^2 (\tS^*) = c$ for some positive constant $c$. Then, we have 
%   \begin{align}
%       \inf_{\tS \in \tP'_{\tS, {\rm TBM}}(\gamma)} \inf_{\hat z \in \tP_z} \sup_{z \in \tP'_{z, \rm TBM}(\gamma)} \bbE[p \ell(\hat z,  z)] & \leq  \inf_{\hat z \in \tP_z} \sup_{z \in \tP'_{z, \rm TBM}(\gamma)} \bbE[p \ell(\hat z, z)| \Delta_{\rm Euc}^2(\tS^*) = c ]  \\
%       & \leq \sup_{z^* \in \tP'_{z, \rm TBM}(\gamma)} \bbE[p \ell(\hat z_{\rm alg}, z^*)| \Delta_{\rm Euc}^2(\tS^*) = c ]\\
%       & \leq 0,
%   \end{align}
%   where $\hat z_{\rm alg}$ refers to the algorithm output in \cite{han2020exact}, and the last inequality holds since the algorithm guarantee Theorem 3 or 4 hold with arbitrary $z^* \in \tP_z$ and $\tP'_{z, \rm TBM}(\gamma) = \tP_z$. 
   
%       \item \textbf{Choose $\tP(\gamma).$} Our dTBM shows different conclusion with TBM and DCBM in this case. 
       
%     %   \textbf{Claim:} For TBM and DCBM, the minimax is equal to the minimin rate; i.e., for example, 
%     %   \begin{equation}
%     %       \inf_{\hat z \in \tP_z} \sup_{(z, \tS) \in \tP_{z, \rm TBM}(\gamma)} \bbE[p \ell(\hat z,  z)]  =  \inf_{\tS \in \tP_{\tS, {\rm TBM}}(\gamma)} \inf_{\hat z \in \tP_z} \sup_{z \in \tP_{z, \rm TBM}(\gamma)} \bbE[p \ell(\hat z,  z)]. 
%     %   \end{equation}
       
%     %   \textbf{Proof:} 
    
%   \end{itemize}
   
%   \textbf{Minimax rate of dTBM.} We consider the equivalent statement of the minimin/minimax rate under dTBM. We omit the subscript dTBM in $\tP$ here.
   
   
%   \begin{align}
%       \inf_{\tS \in \tP_{\tS}(\gamma)} \inf_{\hat z \in \tP_z} \sup_{(z, \mtheta) \in \tP_{z, \mtheta}(\gamma)} \bbE[p \ell(\hat z, z)] \geq 1  \Leftrightarrow  \text{for all } \tS^* \text{ s.t. } \Delta_{\rm ang}(\tS^*)^2 = p^{\gamma}, \inf_{\hat z \in \tP_z} \sup_{(z, \mtheta) \in \tP_{z, \mtheta}(\gamma)} \bbE[p \ell(\hat z, z)| \tS^*] \geq 1,
%   \end{align}
%   and similarly, 
%     \begin{align}
%       \inf_{(\mtheta, \tS) \in \tP_{\mtheta, \tS}(\gamma)} \inf_{\hat z \in \tP_z} \sup_{z \in \tP_{z}(\gamma)} \bbE[p \ell(\hat z, z)] \geq 1  \Leftrightarrow  \text{for all } (\mtheta^*, \tS^*) \in \tP_{\mtheta, \tS}(\gamma), \inf_{\hat z \in \tP_z} \sup_{z \in \tP_{z}(\gamma)} \bbE[p \ell(\hat z, z)| \mtheta^*, \tS^*] \geq 1.
%   \end{align}
   
%   The key step to prove the minimin or minimax rate is to lower bound error with given true parameters $(z^*, \mtheta^*, \tS^*)$ 
%   \begin{equation}
%       \inf_{\hat z \in \tP_z} \bbE[p \ell(\hat z, z)| z^*, \mtheta^*, \tS^*] \gtrsim \frac{1}{|T^c|} \sum_{i \in T^c}\inf_{\hat z \in \tP_Z} \bbP(\hat z(i) = 1| z^*(i) = 2, \mtheta^*, \tS^*) +  \bbP(\hat z(i) = 2| z^*(i) = 1, \mtheta^*, \tS^*),
%   \end{equation}
%   where $T^c$ is a set defined in \cite{han2020exact} and $|T^c| \asymp c_2$ for some constant $c_2$.
   
%   For any $i \in T^c$, the probability on the right hand side corresponding to the hypothesis test
%   \begin{equation}
%       H_0: \my = \mtheta^*(i) \mx^*_1 + \me, \quad \leftrightarrow \quad H_1: \my = \mtheta^*(i) \mx^*_2 + \me,
%   \end{equation}
%   and the best estimator is $\hat z_{LS}(i) = \argmin_{a = 1,2} \onormSize{}{\my - \mtheta^*(i) \mx^*_a} _F^2$ by Neyman-Pearson Lemma, where $\mx^*_a = \mat_1(\tS^* \times_2 \mTheta^* \mM^* \times \cdots \times_K \mTheta^* \mM^*)_{a:}$.
   
%   By the calculation in equation (102) in \cite{han2020exact}, we have 
%   \begin{equation}
%       \bbP(\hat z(i) = 1| z^*(i) = 2, \mtheta^*, \tS^*) +  \bbP(\hat z(i) = 2| z^*(i) = 1, \mtheta^*, \tS^*) \geq 2 \bbP \of{ N(0,1) \geq \frac{\onormSize{}{\mtheta^*(i) \mx^*_1 - \mtheta^*(i) \mx^*_2}_F}{\sigma}}.
%   \end{equation}
%   Due to the summation over $T^c$ and the number $(1,2)$ can be replace by any $(a,b), a \neq b$, we define $f(z, \mtheta, \tS) = \min_{i \in T^c, a \neq b}\onormSize{}{\mtheta(i) \mx_a - \mtheta(i) \mx_b}_F$. Then, we can lower bounded the rate $\inf_{\hat z \in \tP_z} \bbE[p \ell(\hat z, z)| z^*, \mtheta^*, \tS^*]$ by a constant when $f(z^*, \mtheta^*, \tS^*) \lesssim \sigma$. Hence, we re-state the minimax rates with $f.$
   
%   \begin{align}
%       &(1) \inf_{\hat \in \tP_z} \sup_{(z, \mtheta, \tS) \in \tP(\gamma)} \bbE[p \ell(\hat z, z)| \mtheta^*, \tS^*] \geq 1  \Leftrightarrow  \text{there exists a }(z^*, \mtheta^*, \tS^*)  \in \tP(\gamma) \text{ s.t. } f(z^*, \mtheta^*, \tS^*) \lesssim \sigma \\
%         &(2) \inf_{\tS \in \tP_{\tS}(\gamma)} \inf_{\hat z \in \tP_z} \sup_{(z, \mtheta) \in \tP_{z, \mtheta}(\gamma)} \bbE[p \ell(\hat z, z)| \mtheta^*, \tS^*] \geq 1   \\
%         & \quad \quad \Leftrightarrow  \text{for all } \tS^* \in \tP_{\tS}(\gamma), \text{ there exists a }(z^*, \mtheta^*)  \in \tP_{z, \mtheta}(\gamma) \text{ s.t. } f(z^*, \mtheta^*, \tS^*) \lesssim \sigma \\
%         &(3) \inf_{(\mtheta, \tS) \in \tP_{\mtheta, \tS}(\gamma)} \inf_{\hat z \in \tP_z} \sup_{z \in \tP_{z}(\gamma)}  \bbE[p \ell(\hat z, z)| \mtheta^*, \tS^*] \geq 1   \\
%         & \quad \quad \Leftrightarrow  \text{for all } (\mtheta^*, \tS^*) \in \tP_{\mtheta, \tS} (\gamma), \text{ there exists a }z^*  \in \tP_{z}(\gamma) \text{ s.t. } f(z^*, \mtheta^*, \tS^*) \lesssim \sigma.
%   \end{align}
   
   
   
%   (1) is easy to prove under both TBM and dTBM, since we only need to design a specific $(z^*, \mtheta^*, \tS^*)$ such that $   f(z^*, \mtheta^*, \tS^*) \leq \sigma$.
   
%   (2) is easy in TBM because $f(z, \mtheta, \tS) = \min \onormSize{}{\mx_a - \mx_b}_F \leq p^{(K-1)/2} \Delta_{Euc}$, and thus $f(z, \mtheta, \tS) \lesssim \sigma $ for any $\tS$ such that $\Delta_{Euc}^2(\tS) \leq p^{-(K-1)} \sigma^2$.  
   
%   For (2) under dTBM, we consider the upper bound
%   \begin{align}
%       f(z^*, \mtheta^*, \tS^*) & = \min_{i \in T^c} \mtheta^*(i) \min_{a \neq b }\onormSize{}{\mx_a^* - \mx_b^*}_F \\
%       & \leq  \min_{i \in T^c} \mtheta^*(i) \min_{a \neq b } \onormSize{}{\mS^*_{a:} - \mS^*_{b:}}_F \lambda_{\max}^{K-1}(\mTheta^* \mM^*)   \\
%       & \leq \min_{i \in T^c} \mtheta^*(i) \min_{a \neq b } \onormSize{}{\mS^*_{a:} - \mS^*_{b:}}_F (\sqrt{\max_a \onormSize{}{\mtheta_{z^{-1}(a)}^* }^2})^{(K-1)}
%   \end{align}
%   where the last inequality follows from Lemma 6. 
   
%   \newpage
   
%   \section{New minimax results}
   
%   Let $\tP_{\tS} \coloneqq \{ \tS: c_3\leq \onormSize{}{\text{Mat}(\tS)_{a:}}\leq c_4, a\in[r]\}$ denote space for core tensor $\tS$ under dTBM;  let $\tP_{z} \coloneqq \{ z: {c_1p\over r}\leq |z^{-1}(a)|\leq {c_2 p\over r}, a \in [r]\}$ denote the space for assignment $z$ under dTBM; let $\tP_{z ,\mtheta} \coloneqq \{ \mtheta\in\mathbb{R}^p_{+},\ 
% {c_1p\over r}\leq |z^{-1}(a)|\leq {c_2 p\over r}, \onorm{\mtheta_{z^{-1}(a)}}_1=|z^{-1}(a)|, a \in [r] \}$ denote the space for $(z, \mtheta)$ under dTBM,  where $c_1, c_2, c_3, c_4$ are constants in $\tP$.
   
%   \begin{lem}[Trivial statistical impossibility]\label{lem:trivial}  Consider general Gaussian dTBMs with parameter space $\tP(\gamma)$ with $K\geq 1$. Assume $ r =  o(p^{1/3})$. For any true core tensors $\tS^* \in \tP_{\tS} $ such that $\Delta_{\min}^2 (\tS^*)\leq 2$ (or equivalently $\gamma = 0$), no estimator $\hat z_{\text{stat}}$ achieves exact recovery in expectation under the worst case; i.e.,
%   \begin{equation}\label{eq:minminmax}
%   \liminf_{p \rightarrow \infty}\inf_{\tS^* \in  \tP_{\tS} \cap \{\Delta_{\min}^2(\tS^*) \leq 2\}}  \inf_{\hat z_{\text{stat}} \in \tP_z}\sup_{ (z^*, \mtheta^*) \in \tP_{z, \mtheta}} \bbE \left[ p\ell(\hat z_{\text{stat}}, z) \right]\geq 1.
% \end{equation}
%   \end{lem}
   
%   \begin{rmk}The Lemma~\ref{lem:trivial} indicates that for any possible $\tS^*$ with   $\Delta_{\min}^2 (\tS^*)\leq 2$ there exist parameters $(z^*, \mtheta^*)$ such that no estimator is able to exactly recover the assignment under the dTBM with true parameter $(z^*, \tS^*,\mtheta^*)$. Note that $\onormSize{}{\mv_1^s - \mv_2^s}^2_F \leq 2$ for arbitrary vectors $\mv_1, \mv_2$. Then, the regime $ \tP_{\tS} \cap \{\Delta_{\min}^2 (\tS^*)\leq 2\} = \tP_{\tS}$ includes all possible core tensor $\tS^*$ under dTBM. Hence, we say this minimax lower bound~\eqref{eq:minminmax} is trivial.
%   \end{rmk}
   
%   \begin{cor}[Statistical impossibility]\label{cor:impossible} Consider general Gaussian dTBMs with parameter space $\tP(\gamma)$ with $K\geq 1$.  Assume $ r < c p^{1/3}$ for some positive constant $c$. If the signal exponent satisfies $\gamma < -(K-1)$, then, for any true core tensor $\tS^* \in \tP_{\tS}$ such that $\Delta_{\min}^2 (\tS^*) = p^{\gamma}$, no estimator $\hat z_{\text{stat}}$ achieves exact recovery in expectation under the worst case; i.e.,
%   \begin{equation}
%   \inf_{\tS^* \in \tP_{\tS} \cap \{\Delta_{\min}^2(\tS^*) = p^{\gamma} \} }  \inf_{\hat z_{\text{stat}} \in \tP_z }\sup_{ (z^*, \mtheta^*) \in \tP_{z, \mtheta}} \bbE \left[ p\ell(\hat z_{\text{stat}}, z) \right]\geq 1.
% \end{equation}
%   \end{cor}
   
%   \begin{proof}[Proof of Corollary~\ref{cor:impossible}] Note that for any $\tS^*$
%   \begin{equation}
%       \{ (z, \mtheta): (z, \tS^*, \mtheta) \in \tP(\gamma) \} = \{ (z, \mtheta):  \mtheta\in\mathbb{R}^p_{+},\ 
% {c_1p\over r}\leq |z^{-1}(a)|\leq {c_2 p\over r}, \onorm{\mtheta_{z^{-1}(a)}}_1=|z^{-1}(a)|, a\in[r] \},
%   \end{equation}
%           which is independent with $\gamma$. Then, we have $\{ (z, \mtheta): (z, \tS^*, \mtheta) \in \tP(\gamma) \} = \{ (z, \mtheta): (z, \tS^*, \mtheta) \in \tP(0) \}$. 
%           Also note that $ \tP_{\tS} \cap \{\Delta_{\min}^2(\tS^*) = p^{\gamma} \} \subset  \tP_{\tS} \cap \{\Delta_{\min}^2(\tS^*) \leq 2\}$. Hence, we have 
%           \begin{align}
%                 & \inf_{\tS^* \in \tP_{\tS} \cap \{\Delta_{\min}^2(\tS^*) = p^{\gamma} \} }  \inf_{\hat z_{\text{stat}} \in \tP_z }\sup_{ (z^*, \mtheta^*): (z^*, \tS^*, \mtheta^*) \in \tP(\gamma)} \bbE \left[ p\ell(\hat z_{\text{stat}}, z) \right] \\
%                 & \quad \quad \geq \inf_{\tS^* \in \tP_{\tS} \cap \{\Delta_{\min}^2(\tS^*) \leq 2\}}  \inf_{\hat z_{\text{stat}} \in \tP_z}\sup_{ (z^*, \mtheta^*): (z^*, \tS^*, \mtheta^*) \in \tP(0)} \bbE \left[ p\ell(\hat z_{\text{stat}}, z) \right]\geq 1.
%           \end{align}
%   \end{proof}
   
%   \begin{proof}[Proof of Lemma~\ref{lem:trivial}] Consider the general asymetric dTBM~\eqref{eq:general_dtbm} in the special case that $p_k = p$ and $r_k = r$ for all $ k\in [K]$. For simplicity, we show the minimax rate for the estimation on the first mode $\hat z_1$; the proof for other modes are essentially the same. 
   
%   To prove the minimax rate~\eqref{eq:minminmax}, it suffices to take an arbitrary $\tS^* \in  \tP_{\tS} \cap \{\Delta_{\min}^2(\tS^*) \leq 2\}$ and construct $(z^*, \mtheta^*)$ such that 
%   \begin{equation}
%       \inf_{\hat z_1 \in \tP_z} \bbE \left[ p\ell(\hat z_1, z_1^*) | z^*_k, \mtheta^*_k, \tS^*  \right]\geq 1.
%   \end{equation}
   
%   We consider the following construction:
%   \begin{enumerate}
%       \item[(i)] $\{z_k^*\}$ the same in \citet[Proof of Theorem 6]{han2020exact} with misclassification set $T^c_k \subset [p]$ and $|T^c_k| \asymp p/r$ for all $k \in [K]$.
%       \item [(ii)] $\{\mtheta_k^*\}$ such that $\mtheta_1^*(i) \leq \sigma r^{(K-1)/2} p^{-(K-1)/2}$ for all $i \in T_1^c$ and $\max_{k \in [K]/\{1\}, a \in [r]} \onormSize{}{\mtheta_{k, z^{*, -1}_k(a)}}^2_2 \asymp p/r$.
%   \end{enumerate}
%   % There exists a $(z^*, \mtheta^*) \in \tP_{z, \mtheta}$ satisfying the property (i) and (ii) when $r < p$; otherwise, when $r = p$, we must have $\mtheta^*(i) = 1$ due to the $\ell_1$ norm constraint and we can not satisfy the property (ii).
   
%   Then, following the proof of \cite{gao2018community}, we have 
%   \begin{equation}\label{eq:inf_lower}
%         \inf_{\hat z_1 \in \tP_z} \bbE \left[ \ell(\hat z_1, z_1^*) | z^*_k, \mtheta^*_k, \tS^*  \right] \geq \frac{1}{r^3 |T_1^c|} \sum_{i \in T_1^c} \inf_{\hat z_1 \in \tP_1} \{ \bbP[\hat z_1(i) = 1| z_1^*(i) = 2, \mtheta_k^*, \tS^*] +   \bbP[\hat z_1(i) = 2| z_1^*(i) = 1, \mtheta_k^*, \tS^*] \}. 
%   \end{equation}
   
%   The right hand side corresponds to the following hypothesis test for vectors: for an arbitrary $i \in T_1^c$, 
%   \begin{equation}\label{eq:hypo}
%       H_0: \my = \mtheta_1^*(i) \mx_1^* + \me \quad \leftrightarrow \quad H_1: \my = \mtheta_1^*(i) \mx_2^* + \me,
%   \end{equation}
%   where $\my = [\mat_1(\tY)]_{i:}$, $\mx_a^* = [\mat_1( \tS^* \times_2 \mTheta^* \mM_2^* \times_3 \cdots \times_K \mTheta^* \mM_K^*)]_{a:}$, and $\me$ consists of i.i.d.\ $N(0,\sigma^2)$ entries. By Neyman-Pearson lemma, the least square estimator is the best test for above hypothesis testing, where $\hat z_{LS}(i) = \argmin_{a = 1,2} \onormSize{}{\my - \mtheta_1(i)^* \mx_a^*}_F^2$. 
   
%   Note that 
%   \begin{align}
%       \onormSize{}{ \mtheta_1^*(i) \mx_1^*  - \mtheta_1^*(i) \mx_2^*}_F &\leq  \mtheta_1^*(i) \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F \prod_{k = 2}^K \lambda_{\max}(\mTheta_k^*\mM_k^*)  \\
%       & \leq \mtheta_1^*(i) \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F   \max_{k \in [K]/\{1\}, a \in [r]} \onormSize{}{\mtheta_{k, z^{*, -1}_k(a)}}_2^{K-1} \\
%       & \leq  \sigma r^{(K-1)/2} p^{-(K-1)/2} 2 c_4 p^{(K-1)/2} r^{-(K-1)/2}\\
%       & \leq 2 c_4 \sigma, \label{eq:bound}
%   \end{align}
%   where the second inequality follows Lemma 6, the third inequality follows from property (ii) and the boundedness constraint in $\tP_{\tS}$ such that $\onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F  \leq \onormSize{}{\mS_{1:}^*}_F + \onormSize{}{\mS_{2:}^*}_F \leq 2c_4$.

   
%   Hence, we have 
%   \begin{align}
%       & \inf_{\hat z_1 \in \tP_1} \{ \bbP[\hat z_1(i) = 1| z_1^*(i) = 2, \mtheta_k^*, \tS^*] +   \bbP[\hat z_1(i) = 2| z_1^*(i) = 1, \mtheta_k^*, \tS^*] \} \\
%       & \quad = 2 \bbP[ \hat z_{LS}(i) = 1 |  z_1^*(i) = 2, \mtheta_k^*, \tS^* ] \\
%       & \quad  = 2 \bbP[ \onormSize{}{\my - \mtheta_1(i)^* \mx_1^*}_F^2 \leq \onormSize{}{\my - \mtheta_1(i)^* \mx_2^*}_F^2 | z_1^*(i) = 2, \mtheta_k^*, \tS^*   ] \\
%       & \quad =  2 \bbP[ N(0,1) \geq \mtheta_1(i)^* \onormSize{}{\mx_1^* - \mx_2^*}_F /\sigma ] \\
%       & \quad \geq 2 \bbP[ N(0,1) \geq 2 c_4 ]   \geq c, \label{eq:inf}
%   \end{align}
%   for some positive constant $c$, where the calculation for the last equation follows from the calculation in equation (102) in \cite{han2020exact}.
   
%   Plugging the inequality~\eqref{eq:inf} to inequality~\eqref{eq:inf_lower}, then, we have 
%   \begin{equation}
%       \liminf_{p \rightarrow \infty}  \inf_{\hat z_1 \in \tP_z} \bbE \left[ p \ell(\hat z_1, z_1^*) | z^*_k, \mtheta^*_k, \tS^*  \right]  \geq \liminf_{p \rightarrow \infty} \frac{ cp }{r^3} \geq c,
%   \end{equation}
%   where the last inequality follows by the condition $r = o(p^{1/3})$. By the discrete nature of the misclassification error, we have 
%   \begin{equation}
%       \liminf_{p \rightarrow \infty} \inf_{\tS^* \in  \tP_{\tS} \cap \{\Delta_{\min}^2(\tS^*) \leq 2\}}  \inf_{\hat z_{\text{stat}} \in \tP_z}\sup_{ (z^*, \mtheta^*) \in \tP_{z, \mtheta}} \bbE \left[ p\ell(\hat z_{\text{stat}}, z) \right]  \geq 1. 
%   \end{equation}
% %   for any $r < p$ and $p$,  we have 
% %   \begin{equation}
% %         \inf_{\hat z_1 \in \tP_z} \bbE \left[ p \ell(\hat z_1, z_1^*) | z^*_k, \mtheta^*_k, \tS^*  \right] \geq \frac{ cp }{r^3} > 0,
% %   \end{equation}
% %   which indicates 
% %   \begin{equation}
% %         \inf_{\hat z_1 \in \tP_z} \bbE \left[ p \ell(\hat z_1, z_1^*) | z^*_k, \mtheta^*_k, \tS^*  \right] \geq 1, \quad \text{for every } p
% %   \end{equation}
% %   by the discrete nature of $p\ell(\hat z, z^*)$.
%   \end{proof}
   
%   \begin{rmk}[Where the triviality comes from]
%   The proof of Lemma~\ref{lem:trivial} does not use the angle gap of $\tS^*$ but use the Euclidean gap of $\tS^*$ when bound the key quantity $ \onormSize{}{ \mtheta_1^*(i) \mx_1^*  - \mtheta_1^*(i) \mx_2^*}_F$ in \eqref{eq:bound}. By the boundedness constraint in $\tP_{\tS}$, we have $\onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F  \leq 2 c_4$. Then we can construct a small $\mtheta^*(i)$ to eliminate the constant Euclidean gap and make the classification problem very hard.  That's why the minimax bound~\eqref{eq:minminmax} is trivial with respect to the angle gap.
%   \end{rmk}
   
%   \begin{rmk}[Strongest position of $\mtheta$]
%   The proof of Lemma~\ref{lem:trivial} also indicates the strongest position of $\mtheta^*$ should be under the sup. Note that the key quantity in \eqref{eq:bound} relies on the Euclidean gap of $\tS^*$; i.e., 
%   \begin{equation}
%      \mtheta_1^*(i) \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F \prod_{k = 2}^K \lambda_{\min}(\mTheta_k^*\mM_k^*)   \leq  \onormSize{}{ \mtheta_1^*(i) \mx_1^*  - \mtheta_1^*(i) \mx_2^*}_F \leq  \mtheta_1^*(i) \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F \prod_{k = 2}^K \lambda_{\max}(\mTheta_k^*\mM_k^*).
%   \end{equation}
%   We can not infer the Euclidean gap of $\tS^*$ from the angle gap of $\tS^*$. Therefore, we need to design a specific $\mtheta^*$ to eliminate the gap $\onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F$ to make a hard case to solve the hypothesis test~\eqref{eq:hypo}. 
   
%   If we take $\mtheta$ under the inf, then we need to consider arbitrary combination of $\mtheta^*$ and $\tS^*$. Take $(\mtheta^*, \tS^*)$ such that $\onormSize{}{ \mtheta_1^*(i) \mx_1^*  - \mtheta_1^*(i) \mx_2^*}_F  \geq  \mtheta_1^*(i) \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F \prod_{k = 2}^K \lambda_{\min}(\mTheta_k^*\mM_k^*)   \geq c \sigma p^{(K-1)/2} r^{-(K-1)/2}$. Plugging into the inequality~\eqref{eq:inf}, we have 
%   $ \inf_{\hat z_1 \in \tP_1} \{ \bbP[\hat z_1(i) = 1| z_1^*(i) = 2, \mtheta_k^*, \tS^*] +   \bbP[\hat z_1(i) = 2| z_1^*(i) = 1, \mtheta_k^*, \tS^*] \} =  0$ and thus $\inf_{\hat z_1 \in \tP_z} \bbE \left[ p\ell(\hat z_1, z_1^*) | z^*_k, \mtheta^*_k, \tS^*  \right]  \geq 0$, which is meaningless because the misclassification error always $\geq 0$. 
%   \end{rmk}
   
%   \begin{rmk}[Comparison with TBM] In TBM, the statistical impossibility is not trivial. We will have following results:
   
%   Assume $ r \lesssim p^{1/3}$. If the signal exponent satisfies $\gamma < -(K-1)$, then, for any true core tensor $\tS^* \in \tP_{\tS}$ such that $\Delta_{\rm Euc}^2 (\tS^*) = p^{\gamma}$, no estimator $\hat z_{\text{stat}}$ achieves exact recovery in expectation under the worst case; i.e.,
   
%   \begin{equation}
%   \inf_{\tS^* \in \tP_{TBM, \tS} \cap \{\Delta_{\rm Euc}^2(\tS^*) = p^{\gamma} \} }  \inf_{\hat z_{\text{stat}} \in \tP_z }\sup_{ (z^*, \mtheta^*): (z^*, \tS^*, \mtheta^*) \in \tP_{TBM}(\gamma)} \bbE \left[ p\ell(\hat z_{\text{stat}}, z) \right]\geq 1.
% \end{equation}
% The prove of above result is exactly the Proof of Lemma~\ref{lem:trivial} after replacing $\mtheta^* = \mathbf{1}$. Particularly, the inequality~\eqref{eq:bound} becomes 
% \begin{equation}
%      \onormSize{}{  \mx_1^*  -  \mx_2^*}_F  \leq \onormSize{}{ \mS^*_{1:} - \mS^*_{2:} }_F p^{(K-1)/2} r^{-(K-1)/2},
% \end{equation}
% and thus we need the Euclidean gap $\Delta_{\rm Euc}^2 (\tS^*) \leq \sigma^2  p^{-(K-1)} r^{K-1}$ to have $ \onormSize{}{  \mx_1^*  -  \mx_2^*}_F  \leq \sigma$. That's the why the TBM minimax rate is not trivial.
%   \end{rmk}

\bibliography{tensor_wang}
\bibliographystyle{apalike}

\end{document}
 