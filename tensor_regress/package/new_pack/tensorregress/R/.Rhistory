C = X_covar3%*%W3
core=update_core(tsr,G,A,B,C,core_shape,cons,lambda,alpha,solver,dist)
G=core$G
lglk=core$lglk
violate=core$violate
for(n in 1:niter){
## parameter from previous step
W10 = W1 ; W20 = W2 ; W30 = W3 ; G0=G; A0=A;B0=B;C0=C;lglk0=tail(lglk,1);
###### update W1
G_BC = ttl(G, list(B,C), ms = c(2,3))
G_BC1 = unfold(G_BC, row_idx = 1, col_idx = c(2,3))@data
if(un_m1) {re = glm_mat(t(Y_1),t(G_BC1),dist=dist) ## no covariate
} else {re = glm_two(Y = Y_1, X1 = X_covar1, X2 = G_BC1, dist=dist)}
if(dim(re[[1]])[1]==1) W1=t(re[[1]]) else W1 = as.matrix(re[[1]])
lglk = c(lglk,re[[2]])
# replace NA -> 0, and send warning
if(sum(is.na(W1)) > 0){
warning("the input rank is higher than the data could fit.
Estimated factors are not reliable because of infinitely many solutions.
Estimated coefficient tensor is still reliable.",immediate. = T)
W1[is.na(W1)] = 0
}
## orthogonal W1*
qr_res=qr(W1)
W1=qr.Q(qr_res)
G=ttm(G,qr.R(qr_res),1)
#print("W1 Done------------------")
##### calculate A
A = X_covar1%*%W1;
##### update W2
G_AC = ttl(G, list(A,C), ms = c(1,3))
G_AC2 = unfold(G_AC, row_idx = 2, col_idx = c(1,3))@data
if(un_m2) {re = glm_mat(t(Y_2),t(G_AC2),dist=dist)
} else {re = glm_two(Y_2, X_covar2, G_AC2, dist=dist)}
if(dim(re[[1]])[1]==1) W2=t(re[[1]]) else W2 = as.matrix(re[[1]])
lglk = c(lglk,re[[2]])
# replace NA -> 0, and send warning
if(sum(is.na(W2)) > 0){
warning("the input rank is higher than the data could fit.
Estimated factors are not reliable because of infinitely many solutions.
Estimated coefficient tensor is still reliable.",immediate. = T)
W2[is.na(W2)] = 0
}
## orthogonal W2*
qr_res=qr(W2)
W2=qr.Q(qr_res)
G=ttm(G,qr.R(qr_res),2)
#print("W2 Done------------------")
##### calculate B
B = X_covar2%*%W2;
###### update W3
G_AB = ttl(G, list(A,B), ms = c(1,2))
G_AB3 = unfold(G_AB, row_idx = 3, col_idx = c(1,2))@data
if(un_m3) {re = glm_mat(t(Y_3),t(G_AB3),dist=dist)
} else {re = glm_two(Y_3, X_covar3, G_AB3,dist=dist)}
if(dim(re[[1]])[1]==1) W3=t(re[[1]]) else W3 = as.matrix(re[[1]])
lglk = c(lglk,re[[2]])
# replace NA -> 0, and send warning
if(sum(is.na(W3)) > 0){
warning("the input rank is higher than the data could fit.
Estimated factors are not reliable because of infinitely many solutions.
Estimated coefficient tensor is still reliable.",immediate. = T)
W3[is.na(W3)] = 0
}
## orthogonal W3*
qr_res=qr(W3)
W3=qr.Q(qr_res)
G=ttm(G,qr.R(qr_res),3)
#print("W3 Done------------------")
##### calculate C
C = X_covar3%*%W3;
#########-----------------------------------------------
###  obtain core tensor under constraint
core=update_core(tsr,G,A,B,C,core_shape,cons,lambda,alpha,solver,dist)
G=core$G
lglk=c(lglk,core$lglk)
violate=c(violate,core$violate)
#print("G Done------------------")
message(paste(n,"-th  iteration -- when dimension is",d1,d2,d3,"- rank is ",r1,r2,r3," -----------------"))
#print(paste(n,"-th  iteration"))
if((traj_long==T)&(n < 8)){
n = n+1
next
}
if ((tail(lglk,1)-lglk0)/abs(lglk0)<= 0.0001 & tail(lglk,1)>= lglk0 ){
message(paste(n,"-th iteration: convergence"))
break
} else if (tail(lglk,1)-lglk0 < 0) {
W1 = W10 ; W2 = W20 ; W3 = W30; G=G0; lglk=lglk[-c((length(lglk)-3):length(lglk))];
A=A0;B=B0;C=C0;
break
}
}
U=ttl(G,list(A,B,C),ms = c(1,2,3))@data
sigma_est=mean((tsr@data-U_to_mean(U,dist))^2)
return(list(W = list(W1 = W1,W2 = W2,W3 = W3),G = G@data,U=U, C_ts=ttl(G,list(W1,W2,W3),ms = c(1,2,3))@data,lglk = lglk, sigma=sigma_est,violate = violate))
}
### supervised
sup_res = tensor_regress(Y, X_covar1=X,X_covar2=NULL,X_covar3=NULL,c(2,2,2),niter=10,cons="non",dist="normal", initial = "QR_tucker")
sup_res1 = tensor_regress(Y, X_covar1=X,X_covar2=NULL,X_covar3=NULL,c(2,2,2),niter=10,cons="non",dist="normal", initial = "random")
sup_res$C_ts
library(plot.matrix)
library(RColorBrewer)
plot(sup_res$U[,,1], col=brewer.pal(n = 11, name = "RdBu"), breaks = seq(-1, 1, length.out=12),border = NA)
plot(sup_res1$U[,,1], col=brewer.pal(n = 11, name = "RdBu"), breaks = seq(-1, 1, length.out=12),border = NA)
plot(U[,,1], col=brewer.pal(n = 11, name = "RdBu"), breaks = seq(-1, 1, length.out=12),border = NA)
angle_mat(X%*%sup_res$W$W1, A0[,-c(1,2)])
source('~/Documents/GitHub/research_Jiaxin/tensor_regress/package/new_pack/tensorregress/R/bricks.R', echo=TRUE)
library(rTensor)
library(plot.matrix)
library(RColorBrewer)
######## orthogonal
set.seed(0)
d = 10
Core = array(4,dim = c(4,4,4))
Core[1:2,1:2,1:2] = 0.5
A0 = randortho(10)
B0 = randortho(10)
C0 = randortho(10)
A = A0[,1:4]
B = B0[,1:4]
C = C0[,1:4]
U = ttl(as.tensor(Core), list(A,B,C), c(1,2,3))@data
X = A[,1:2]
Y = U + array(rnorm(d^3, sd = 0), dim = c(d,d,d))
un_res = tensor_regress(Y, X_covar1=NULL,X_covar2=NULL,X_covar3=NULL,c(2,2,2),cons="non",dist="normal", initial = "QR_tucker")
tensor_regress = function(tsr,X_covar1 = NULL, X_covar2 = NULL,X_covar3 = NULL, core_shape, niter=20, cons = c("non","vanilla","penalty"), lambda = 0.1, alpha = 1,
solver ="CG",dist = c("binary", "poisson","normal"),traj_long=FALSE, initial = c("random","QR_tucker")){
# initial: "random" for random initialization; "QR_tucker" for QR-based tucker initialization
tsr = as.tensor(tsr)
Y_1 = unfold(tsr, row_idx = 1, col_idx = c(2,3))@data
Y_2 = unfold(tsr, row_idx = 2, col_idx = c(1,3))@data
Y_3 = unfold(tsr, row_idx = 3, col_idx = c(1,2))@data
d1 = dim(tsr)[1] ; d2 = dim(tsr)[2] ; d3 = dim(tsr)[3]
r1 = core_shape[1] ; r2 = core_shape[2] ; r3 = core_shape[3]
###  check whether unsupervised on each mode
un_m1 = FALSE ; un_m2 = FALSE ; un_m3 = FALSE
if(is.null(X_covar1)|(identical(X_covar1,diag(d1)))) {X_covar1 = diag(d1) ; un_m1 = TRUE}
if(is.null(X_covar2)|(identical(X_covar2,diag(d2)))) {X_covar2 = diag(d2) ; un_m2 = TRUE}
if(is.null(X_covar3)|(identical(X_covar3,diag(d3)))) {X_covar3 = diag(d3) ; un_m3 = TRUE}
p1 = dim(X_covar1)[2] ; p2 = dim(X_covar2)[2] ; p3 = dim(X_covar3)[2]
if(dist=="binary"){
tsr.transform=as.tensor(2*tsr@data-1)
}else if(dist=="poisson"){
tsr.transform=as.tensor(log(tsr@data+0.5)) # change from 0.1 to 0.5
}else if (dist=="normal"){
tsr.transform=tsr
}
if(initial == "random"){ # random initialization
C_ts=ttl(tsr.transform,list(ginv(X_covar1),ginv(X_covar2),ginv(X_covar3)),ms=c(1,2,3))
mr1 = qr(unfold(C_ts,row_idx = 1, col_idx = c(2,3))@data)$rank
mr2 = qr(unfold(C_ts,row_idx = 2, col_idx = c(1,3))@data)$rank
mr3 = qr(unfold(C_ts,row_idx = 3, col_idx = c(1,2))@data)$rank
if(mr1 < r1|mr2 < r2|mr3 < r3){
warning("the input rank is higher than the data could fit.
Estimated factors are not reliable because of infinitely many solutions.
Estimated coefficient tensor is still reliable.",immediate. = T)
}
W1=randortho(p1)[,1:core_shape[1]];W2=randortho(p2)[,1:core_shape[2]];W3=randortho(p3)[,1:core_shape[3]]
G=ttl(C_ts,list(t(W1),t(W2),t(W3)),ms=1:3)
}else if(initial == "QR_tucker"){ # QR based tucker initialization
# tckr = tucker(C_ts, ranks = core_shape)
# W1 = tckr$U[[1]] ; W2 = tckr$U[[2]] ; W3 = tckr$U[[3]] ## tucker factors
# G = tckr$Z
# use new QR generalization
qr1 = qr(X_covar1); qr2 = qr(X_covar2); qr3 = qr(X_covar3)
Q1 = qr.Q(qr1); Q2 = qr.Q(qr2); Q3 = qr.Q(qr3)
R1 = qr.R(qr1); R2 = qr.R(qr2); R3 = qr.R(qr3)
new_y = ttl(tsr.transform, list_mat = list(t(Q1), t(Q2),t(Q3)), c(1,2,3)) # Y \times Q^T = B \times R
res_un = tucker(new_y,ranks = core_shape) # HOOI, not random
G = res_un$Z@data
W1 = solve(R1)%*%res_un$U[[1]]; W2 = solve(R2)%*%res_un$U[[2]]; W3 = solve(R3)%*%res_un$U[[3]]
if(dist == "normal"){ # normal
C_ts=ttl(as.tensor(G),list(W1,W2,W3),ms = c(1,2,3))
U = ttl(C_ts, list(X_covar1, X_covar2, X_covar3),c(1,2,3))
lglk = loglike(tsr@data,U@data,dist)
sigma_est=mean((tsr@data-U_to_mean(U@data,dist))^2)
violate = 0
return(list(W = list(W1 = W1,W2 = W2,W3 = W3),G = G,U=U@data, C_ts = C_ts@data,lglk = lglk, sigma=sigma_est,violate = violate))
}
}
A = X_covar1%*%W1
B = X_covar2%*%W2
C = X_covar3%*%W3
core=update_core(tsr,G,A,B,C,core_shape,cons,lambda,alpha,solver,dist)
G=core$G
lglk=core$lglk
violate=core$violate
for(n in 1:niter){
## parameter from previous step
W10 = W1 ; W20 = W2 ; W30 = W3 ; G0=G; A0=A;B0=B;C0=C;lglk0=tail(lglk,1);
###### update W1
G_BC = ttl(G, list(B,C), ms = c(2,3))
G_BC1 = unfold(G_BC, row_idx = 1, col_idx = c(2,3))@data
if(un_m1) {re = glm_mat(t(Y_1),t(G_BC1),dist=dist) ## no covariate
} else {re = glm_two(Y = Y_1, X1 = X_covar1, X2 = G_BC1, dist=dist)}
if(dim(re[[1]])[1]==1) W1=t(re[[1]]) else W1 = as.matrix(re[[1]])
lglk = c(lglk,re[[2]])
# replace NA -> 0
W1[is.na(W1)] = 0
## orthogonal W1*
qr_res=qr(W1)
W1=qr.Q(qr_res)
G=ttm(G,qr.R(qr_res),1)
#print("W1 Done------------------")
##### calculate A
A = X_covar1%*%W1;
##### update W2
G_AC = ttl(G, list(A,C), ms = c(1,3))
G_AC2 = unfold(G_AC, row_idx = 2, col_idx = c(1,3))@data
if(un_m2) {re = glm_mat(t(Y_2),t(G_AC2),dist=dist)
} else {re = glm_two(Y_2, X_covar2, G_AC2, dist=dist)}
if(dim(re[[1]])[1]==1) W2=t(re[[1]]) else W2 = as.matrix(re[[1]])
lglk = c(lglk,re[[2]])
# replace NA -> 0
W2[is.na(W2)] = 0
## orthogonal W2*
qr_res=qr(W2)
W2=qr.Q(qr_res)
G=ttm(G,qr.R(qr_res),2)
#print("W2 Done------------------")
##### calculate B
B = X_covar2%*%W2;
###### update W3
G_AB = ttl(G, list(A,B), ms = c(1,2))
G_AB3 = unfold(G_AB, row_idx = 3, col_idx = c(1,2))@data
if(un_m3) {re = glm_mat(t(Y_3),t(G_AB3),dist=dist)
} else {re = glm_two(Y_3, X_covar3, G_AB3,dist=dist)}
if(dim(re[[1]])[1]==1) W3=t(re[[1]]) else W3 = as.matrix(re[[1]])
lglk = c(lglk,re[[2]])
# replace NA -> 0
W3[is.na(W3)] = 0
## orthogonal W3*
qr_res=qr(W3)
W3=qr.Q(qr_res)
G=ttm(G,qr.R(qr_res),3)
#print("W3 Done------------------")
##### calculate C
C = X_covar3%*%W3;
#########-----------------------------------------------
###  obtain core tensor under constraint
core=update_core(tsr,G,A,B,C,core_shape,cons,lambda,alpha,solver,dist)
G=core$G
lglk=c(lglk,core$lglk)
violate=c(violate,core$violate)
#print("G Done------------------")
message(paste(n,"-th  iteration -- when dimension is",d1,d2,d3,"- rank is ",r1,r2,r3," -----------------"))
#print(paste(n,"-th  iteration"))
if((traj_long==T)&(n < 8)){
n = n+1
next
}
if ((tail(lglk,1)-lglk0)/abs(lglk0)<= 0.0001 & tail(lglk,1)>= lglk0 ){
message(paste(n,"-th iteration: convergence"))
break
} else if (tail(lglk,1)-lglk0 < 0) {
W1 = W10 ; W2 = W20 ; W3 = W30; G=G0; lglk=lglk[-c((length(lglk)-3):length(lglk))];
A=A0;B=B0;C=C0;
break
}
}
U=ttl(G,list(A,B,C),ms = c(1,2,3))@data
sigma_est=mean((tsr@data-U_to_mean(U,dist))^2)
return(list(W = list(W1 = W1,W2 = W2,W3 = W3),G = G@data,U=U, C_ts=ttl(G,list(W1,W2,W3),ms = c(1,2,3))@data,lglk = lglk, sigma=sigma_est,violate = violate))
}
un_res = tensor_regress(Y, X_covar1=NULL,X_covar2=NULL,X_covar3=NULL,c(2,2,2),cons="non",dist="normal", initial = "QR_tucker")
### supervised
sup_res = tensor_regress(Y, X_covar1=X,X_covar2=NULL,X_covar3=NULL,c(2,2,2),niter=10,cons="non",dist="normal", initial = "QR_tucker")
sup_res1 = tensor_regress(Y, X_covar1=X,X_covar2=NULL,X_covar3=NULL,c(2,2,2),niter=10,cons="non",dist="normal", initial = "random")
tensor_regress = function(tsr,X_covar1 = NULL, X_covar2 = NULL,X_covar3 = NULL, core_shape, niter=20, cons = c("non","vanilla","penalty"), lambda = 0.1, alpha = 1,
solver ="CG",dist = c("binary", "poisson","normal"),traj_long=FALSE, initial = c("random","QR_tucker")){
# initial: "random" for random initialization; "QR_tucker" for QR-based tucker initialization
tsr = as.tensor(tsr)
Y_1 = unfold(tsr, row_idx = 1, col_idx = c(2,3))@data
Y_2 = unfold(tsr, row_idx = 2, col_idx = c(1,3))@data
Y_3 = unfold(tsr, row_idx = 3, col_idx = c(1,2))@data
d1 = dim(tsr)[1] ; d2 = dim(tsr)[2] ; d3 = dim(tsr)[3]
r1 = core_shape[1] ; r2 = core_shape[2] ; r3 = core_shape[3]
###  check whether unsupervised on each mode
un_m1 = FALSE ; un_m2 = FALSE ; un_m3 = FALSE
if(is.null(X_covar1)|(identical(X_covar1,diag(d1)))) {X_covar1 = diag(d1) ; un_m1 = TRUE}
if(is.null(X_covar2)|(identical(X_covar2,diag(d2)))) {X_covar2 = diag(d2) ; un_m2 = TRUE}
if(is.null(X_covar3)|(identical(X_covar3,diag(d3)))) {X_covar3 = diag(d3) ; un_m3 = TRUE}
p1 = dim(X_covar1)[2] ; p2 = dim(X_covar2)[2] ; p3 = dim(X_covar3)[2]
if(dist=="binary"){
tsr.transform=as.tensor(2*tsr@data-1)
}else if(dist=="poisson"){
tsr.transform=as.tensor(log(tsr@data+0.5)) # change from 0.1 to 0.5
}else if (dist=="normal"){
tsr.transform=tsr
}
if(initial == "random"){ # random initialization
C_ts=ttl(tsr.transform,list(ginv(X_covar1),ginv(X_covar2),ginv(X_covar3)),ms=c(1,2,3))
mr1 = qr(unfold(C_ts,row_idx = 1, col_idx = c(2,3))@data)$rank
mr2 = qr(unfold(C_ts,row_idx = 2, col_idx = c(1,3))@data)$rank
mr3 = qr(unfold(C_ts,row_idx = 3, col_idx = c(1,2))@data)$rank
if(mr1 < r1|mr2 < r2|mr3 < r3){
warning("the input rank is higher than the data could fit. Estimated factors are not reliable because of infinitely many solutions. Estimated coefficient tensor is still reliable.",immediate. = T)
}
W1=randortho(p1)[,1:core_shape[1]];W2=randortho(p2)[,1:core_shape[2]];W3=randortho(p3)[,1:core_shape[3]]
G=ttl(C_ts,list(t(W1),t(W2),t(W3)),ms=1:3)
}else if(initial == "QR_tucker"){ # QR based tucker initialization
# tckr = tucker(C_ts, ranks = core_shape)
# W1 = tckr$U[[1]] ; W2 = tckr$U[[2]] ; W3 = tckr$U[[3]] ## tucker factors
# G = tckr$Z
# use new QR generalization
qr1 = qr(X_covar1); qr2 = qr(X_covar2); qr3 = qr(X_covar3)
Q1 = qr.Q(qr1); Q2 = qr.Q(qr2); Q3 = qr.Q(qr3)
R1 = qr.R(qr1); R2 = qr.R(qr2); R3 = qr.R(qr3)
new_y = ttl(tsr.transform, list_mat = list(t(Q1), t(Q2),t(Q3)), c(1,2,3)) # Y \times Q^T = B \times R
res_un = tucker(new_y,ranks = core_shape) # HOOI, not random
G = res_un$Z@data
W1 = solve(R1)%*%res_un$U[[1]]; W2 = solve(R2)%*%res_un$U[[2]]; W3 = solve(R3)%*%res_un$U[[3]]
if(dist == "normal"){ # normal
C_ts=ttl(as.tensor(G),list(W1,W2,W3),ms = c(1,2,3))
U = ttl(C_ts, list(X_covar1, X_covar2, X_covar3),c(1,2,3))
lglk = loglike(tsr@data,U@data,dist)
sigma_est=mean((tsr@data-U_to_mean(U@data,dist))^2)
violate = 0
return(list(W = list(W1 = W1,W2 = W2,W3 = W3),G = G,U=U@data, C_ts = C_ts@data,lglk = lglk, sigma=sigma_est,violate = violate))
}
}
A = X_covar1%*%W1
B = X_covar2%*%W2
C = X_covar3%*%W3
core=update_core(tsr,G,A,B,C,core_shape,cons,lambda,alpha,solver,dist)
G=core$G
lglk=core$lglk
violate=core$violate
for(n in 1:niter){
## parameter from previous step
W10 = W1 ; W20 = W2 ; W30 = W3 ; G0=G; A0=A;B0=B;C0=C;lglk0=tail(lglk,1);
###### update W1
G_BC = ttl(G, list(B,C), ms = c(2,3))
G_BC1 = unfold(G_BC, row_idx = 1, col_idx = c(2,3))@data
if(un_m1) {re = glm_mat(t(Y_1),t(G_BC1),dist=dist) ## no covariate
} else {re = glm_two(Y = Y_1, X1 = X_covar1, X2 = G_BC1, dist=dist)}
if(dim(re[[1]])[1]==1) W1=t(re[[1]]) else W1 = as.matrix(re[[1]])
lglk = c(lglk,re[[2]])
# replace NA -> 0
W1[is.na(W1)] = 0
## orthogonal W1*
qr_res=qr(W1)
W1=qr.Q(qr_res)
G=ttm(G,qr.R(qr_res),1)
#print("W1 Done------------------")
##### calculate A
A = X_covar1%*%W1;
##### update W2
G_AC = ttl(G, list(A,C), ms = c(1,3))
G_AC2 = unfold(G_AC, row_idx = 2, col_idx = c(1,3))@data
if(un_m2) {re = glm_mat(t(Y_2),t(G_AC2),dist=dist)
} else {re = glm_two(Y_2, X_covar2, G_AC2, dist=dist)}
if(dim(re[[1]])[1]==1) W2=t(re[[1]]) else W2 = as.matrix(re[[1]])
lglk = c(lglk,re[[2]])
# replace NA -> 0
W2[is.na(W2)] = 0
## orthogonal W2*
qr_res=qr(W2)
W2=qr.Q(qr_res)
G=ttm(G,qr.R(qr_res),2)
#print("W2 Done------------------")
##### calculate B
B = X_covar2%*%W2;
###### update W3
G_AB = ttl(G, list(A,B), ms = c(1,2))
G_AB3 = unfold(G_AB, row_idx = 3, col_idx = c(1,2))@data
if(un_m3) {re = glm_mat(t(Y_3),t(G_AB3),dist=dist)
} else {re = glm_two(Y_3, X_covar3, G_AB3,dist=dist)}
if(dim(re[[1]])[1]==1) W3=t(re[[1]]) else W3 = as.matrix(re[[1]])
lglk = c(lglk,re[[2]])
# replace NA -> 0
W3[is.na(W3)] = 0
## orthogonal W3*
qr_res=qr(W3)
W3=qr.Q(qr_res)
G=ttm(G,qr.R(qr_res),3)
#print("W3 Done------------------")
##### calculate C
C = X_covar3%*%W3;
#########-----------------------------------------------
###  obtain core tensor under constraint
core=update_core(tsr,G,A,B,C,core_shape,cons,lambda,alpha,solver,dist)
G=core$G
lglk=c(lglk,core$lglk)
violate=c(violate,core$violate)
#print("G Done------------------")
message(paste(n,"-th  iteration -- when dimension is",d1,d2,d3,"- rank is ",r1,r2,r3," -----------------"))
#print(paste(n,"-th  iteration"))
if((traj_long==T)&(n < 8)){
n = n+1
next
}
if ((tail(lglk,1)-lglk0)/abs(lglk0)<= 0.0001 & tail(lglk,1)>= lglk0 ){
message(paste(n,"-th iteration: convergence"))
break
} else if (tail(lglk,1)-lglk0 < 0) {
W1 = W10 ; W2 = W20 ; W3 = W30; G=G0; lglk=lglk[-c((length(lglk)-3):length(lglk))];
A=A0;B=B0;C=C0;
break
}
}
U=ttl(G,list(A,B,C),ms = c(1,2,3))@data
sigma_est=mean((tsr@data-U_to_mean(U,dist))^2)
return(list(W = list(W1 = W1,W2 = W2,W3 = W3),G = G@data,U=U, C_ts=ttl(G,list(W1,W2,W3),ms = c(1,2,3))@data,lglk = lglk, sigma=sigma_est,violate = violate))
}
sup_res1 = tensor_regress(Y, X_covar1=X,X_covar2=NULL,X_covar3=NULL,c(2,2,2),niter=10,cons="non",dist="normal", initial = "random")
un_res = tensor_regress(Y, X_covar1=NULL,X_covar2=NULL,X_covar3=NULL,c(2,2,2),cons="non",dist="normal", initial = "random")
sup_res1 = tensor_regress(Y, X_covar1=X,X_covar2=NULL,X_covar3=NULL,c(2,2,2),niter=10,cons="non",dist="normal", initial = "random")
### supervised
sup_res = tensor_regress(Y, X_covar1=X,X_covar2=NULL,X_covar3=NULL,c(2,2,2),niter=10,cons="non",dist="normal", initial = "QR_tucker")
sup_res1 = tensor_regress(Y, X_covar1=X,X_covar2=NULL,X_covar3=NULL,c(2,2,2),niter=10,cons="non",dist="normal", initial = "random")
plot(sup_res$U[,,1], col=brewer.pal(n = 11, name = "RdBu"), breaks = seq(-1, 1, length.out=12),border = NA)
plot(sup_res1$U[,,1], col=brewer.pal(n = 11, name = "RdBu"), breaks = seq(-1, 1, length.out=12),border = NA)
plot(U[,,1], col=brewer.pal(n = 11, name = "RdBu"), breaks = seq(-1, 1, length.out=12),border = NA)
source('~/Documents/GitHub/research_Jiaxin/tensor_regress/package/new_pack/tensorregress/R/bricks.R', echo=TRUE)
source('~/Documents/GitHub/research_Jiaxin/tensor_regress/package/new_pack/tensorregress/R/tensor_regress.R', echo=TRUE)
library(pracma)
angle_mat = function(A,B_null) {
# sin(A,B) = || A^T B^{\perp} || (max singular value)
mat = t(A) %*% B_null
sin_theta = svd(mat)$d[1]
return(sin_theta)
}
set.seed(0)
d = 10
Core = array(4,dim = c(4,4,4))
Core[1:2,1:2,1:2] = 0.5
A0 = randortho(10)
B0 = randortho(10)
C0 = randortho(10)
A = A0[,1:4]
B = B0[,1:4]
C = C0[,1:4]
U = ttl(as.tensor(Core), list(A,B,C), c(1,2,3))@data
X = A[,1:2]
Y = U + array(rnorm(d^3, sd = 0), dim = c(d,d,d))
library(rTensor)
set.seed(0)
d = 10
Core = array(4,dim = c(4,4,4))
Core[1:2,1:2,1:2] = 0.5
A0 = randortho(10)
B0 = randortho(10)
C0 = randortho(10)
A = A0[,1:4]
B = B0[,1:4]
C = C0[,1:4]
U = ttl(as.tensor(Core), list(A,B,C), c(1,2,3))@data
X = A[,1:2]
Y = U + array(rnorm(d^3, sd = 0), dim = c(d,d,d))
### unsupervised
library(tensorregress)
un_res = tensor_regress(Y, X_covar1=NULL,X_covar2=NULL,X_covar3=NULL,c(2,2,2),cons="non",dist="normal", initial = "QR_tucker")
#un_res = tensor_regress(Y, X_covar1=NULL,X_covar2=NULL,X_covar3=NULL,c(2,2,2),cons="non",dist="normal", initial = "random")
angle_mat(un_res$W$W1, A0[,-c(1,2,3,4)])
angle_mat(un_res$W$W1, A0[,-c(3,4)])
angle_mat(un_res$W$W2, B0[,-c(1,2)])
angle_mat(un_res$W$W3, C0[,-c(1,2)])
### supervised
sup_res = tensor_regress(Y, X_covar1=X,X_covar2=NULL,X_covar3=NULL,c(2,2,2),niter=10,cons="non",dist="normal", initial = "QR_tucker")
angle_mat(X%*%sup_res$W$W1, A0[,-c(1,2)])
angle_mat(sup_res$W$W2, B0[,-c(1,2)])
angle_mat(sup_res$W$W3, C0[,-c(1,2,3,4)])
##########  overparameter
set.seed(0)
d = 10; alpha = 3
core = array(runif(2^3,-alpha,alpha), dim = c(2,2,2))
W1 = randortho(d)[,1:2]
W2 = randortho(d)[,1:2]
W3 = randortho(2)[,1:2]
X0 = randortho(d)
X = X0[,1:2]
B = ttl(as.tensor(core), list(W1,W2,W3), c(1,2,3))
U = ttm(B, X, 3)@data
Y = U + array(rnorm(d^3, sd = 0.5), dim = c(d,d,d))
Y = U + array(rnorm(d^3, sd = 0.1), dim = c(d,d,d))
# regular
res = tensor_regress(Y, X_covar1 = NULL,X_covar2=NULL,X_covar3=X,c(2,2,2),niter = 10,cons="non",dist="normal")
# regular
res = tensor_regress(Y, X_covar1 = NULL,X_covar2=NULL,X_covar3=X,c(2,2,2),niter = 10,cons="non",dist="normal", initial = "random")
# overparameterizaiton
X1 = X0[,1:3]
res1 = tensor_regress(Y, X_covar1=X,X_covar2=NULL,X_covar3=NULL,c(2,2,3),niter = 10,cons="non",dist="normal", initial = "random")
res$C_ts[,,1]
res1$C_ts[,,1]
# overparameterizaiton
X1 = X0[,1:3]
res1 = tensor_regress(Y, X_covar1=NULL,X_covar2=NULL,X_covar3=X1,c(2,2,3),niter = 10,cons="non",dist="normal", initial = "random")
res1$C_ts[,,1]
res$C_ts[,,1]
mean((res$C_ts[,,2] - res1$C_ts[,,2])^2, na.rm=FALSE)
mean((res$C_ts[,,1] - res1$C_ts[,,1])^2, na.rm=FALSE)
install.packages("/Users/March/Documents/GitHub/research_Jiaxin/tensor_regress/package/new_pack/tensorregress_4.0.tar.gz", repos = NULL, type="source")
library(tensorregress)
library(devtools)
document()
build()
remove.packages("tensorregress")
library(devtools)
build()
remove.packages("tensorregress")
install.packages("/Users/March/Documents/GitHub/research_Jiaxin/tensor_regress/package/new_pack/tensorregress_4.0.tar.gz", repos = NULL, type="source")
library(devtools)
document()
build()
remove.packages("tensorregress")
library(devtools)
document()
build()
remove.packages("tensorregress")
install.packages("/Users/March/Documents/GitHub/research_Jiaxin/tensor_regress/package/new_pack/tensorregress_4.0.tar.gz", repos = NULL, type="source")
