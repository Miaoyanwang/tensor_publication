\documentclass[11pt]{article}

\usepackage{fancybox}


\usepackage{color}
\usepackage{url}
\usepackage[margin=1in]{geometry}

\setlength\parindent{0pt}
\renewcommand{\textfraction}{0.0}
\renewcommand{\topfraction}{1.0}
%\renewcommand{\textfloatsep}{5mm}

\usepackage{comment}
% Definitions of handy macros can go here
\usepackage{amsmath,amssymb,amsthm,bm,mathtools}
\usepackage{multirow}
\usepackage{dsfont,multirow,hyperref,setspace,natbib,enumerate}
%\usepackage{dsfont,multirow,hyperref,setspace,enumerate}
\hypersetup{colorlinks,linkcolor={blue},citecolor={blue},urlcolor={red}} 
\usepackage{algpseudocode,algorithm}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}

\mathtoolsset{showonlyrefs=true}

\newcommand{\of}[1]{\left(#1\right)}
\newcommand{\off}[1]{\left[#1\right]}
\newcommand{\offf}[1]{\left\{#1\right\}}
\newcommand{\aabs}[1]{\left|#1\right|}
\usepackage{colonequals}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{assumption}{Assumption}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}


\setcounter{figure}{0}   
\setcounter{table}{0}  



\newcommand{\cmt}[1]{{\leavevmode\color{red}{#1}}}

\usepackage{dsfont}
\usepackage{multirow}

\DeclareMathOperator*{\minimize}{minimize}



\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}
\usepackage{xr}

\input macros.tex



\title{Future Improvement}

\date{\today}
\author{%
Jiaxin Hu
}



\begin{document}

% Makes the title and author information appear.

\maketitle


% Abstracts are required.
\section{Unsolved Review Summary}

\subsection{Comparisons}
\begin{enumerate}
	\item Add comparison on tensor-on-tensor regression~\citep{lock2018tensor,llosa2018tensor,gahrooei2020multiple,raskutti2015convex}. {\bf NIPS R3}.
	\item Add comparison on scalar-on-tensor regression~\citep{chen2019non, zhou2013tensor}. {\bf ICML R1}.
	\item Add comparison on HOLRR~\citep{rabusseau2016low}. Though HOLRR is a special case of the proposed model, it is helpful to show the benefit of the incorporation of covariate on distinct modes, when all covariates are vectorized in a long vector in HOLRR. {\bf AISTAT R3}.
	\item Add comparison on HOPLS~\citep{zhao2012higher} and TPG \citep{yu2016learning} from the perspective of the model. {\bf AISTAT R1}.
	\item Add comparison on paper~\citep{smith2015positive}. {\bf NIPS R4}.
\end{enumerate}

\subsection{Experiments}
\begin{enumerate}
	\item Add comparisons on other scalar-on-tensor and tensor-on-tensor regressions in both simulations and real data analysis. {\bf NIPS R2, R3}.
	\item Add numerical results on large-scale or higher-order response. {\bf NIPS R2, AISTAT R3}.
	\item Show how many iterations and time are needed to converge. {\bf NIPS R2}.
\end{enumerate}

\subsection{Explanations}
\begin{enumerate}
	\item Add explanations on hyperparameter selection for our method and other methods (e.g.\ HOLRR, TPG, HOPLS) we want to compare. {\bf NIPS R2,R4}.
	\item Add explanations on rank selection, including relative supplements. Explain why not use the proposed rank selection strategy in simulations. Explain the computational issues of grid search. {\bf NIPS R3, ICML R1, R3}.
	\item Discuss the existence of $\tB_{true}$ and discuss the mismatching loss $\sum \hat \tB_{ijk} \neq {\tB_{true}}_{ijk}$. {\bf NIPS R4}.
	\item Compare the computational complexity with other methods. {\bf NIPS R2}.
	\item Explain how the 136 subjects in HCP are selected from thousands of subjects in the original data. {\bf NIPS R3}.
\end{enumerate}


\section{Model Equivalence}

\subsection{Our method}
Let $\tY = \entry{\tY_{i_1,...,i_k}} \in \bbR^{d_1,...,d_K}$ be the tensor observation, $\mX_k \in \bbR^{d_k \times p_k}, k \in [K]$ be the matrix covariates. Our tensor regression model is of form
\begin{equation}\label{ourmodel}
	g \of{ \bbE[\tY |\mX_1 ,...,\mX_K ] } = \tB \times_1 \mX_1 \times_2 \cdots \times_K \mX_K,
\end{equation}
where $g(\cdot)$ is the inverse link function, $\tB \in \bbR^{p_1 \times \cdots \times p_K}$ is the order-K tensor-valued coefficient. Reformulating the model~\eqref{ourmodel} into $\prod_{i=1}^K d_k$ univariate regressions, we have
\begin{equation}\label{ourmodel1}
	g \of{ \bbE[\tY_{i_1,...,i_K} |\mX_1 ,...,\mX_K ] }  = \langle \tB, \mX_{1i_1} \circ \cdots \circ \mX_{K i_K}  \rangle, \quad \text{for } i_k \in [d_k], k\in[K],
\end{equation}
where $\mX_{k i_k}$ refers to the $i_k$-th row of $\mX_k$, for all $k \in [K]$, $\langle \cdot, \cdot \rangle$ refers to the inner product of two tensors, and $\circ$ refers to the outer product of two vectors.

\subsection{Scalar-on-tensor regression}
Let $y \in \bbR$ denote the scalar response, $\tX \in \bbR^{p_1 \times \cdots \times p_K}$ denote the order-K tensor predictor, and $\mZ \in \bbR^{p_0}$ denote the vector-valued covariate. Given the i.i.d.\ sample set $\{y^{(i)}, \tX^{(i)}, \mZ^{(i)}\}_{i=1}^n$,  the scalar-on-tensor regression model is
\begin{equation}\label{scalar}
	g \of{\bbE[y^{(i)} | \tX^{(i)}]}  = \alpha + \gamma^T \mZ + \langle \tB, \tX^{(i)}  \rangle, \quad \text{for } i  = 1 ,...,n
\end{equation}

where $g(\cdot)$ is the inverse link function, $\alpha \in \bbR$ is the scalar coefficient, $\gamma \in \bbR^{p_0}$ is the vector-valued coefficient, and $\tB \in \bbR^{p_1 \times \cdots \times p_K}$ is the order-K tensor-valued coefficient.

Ignoring the scalar parameter $\alpha$ and the vector-valued covariate $\mZ$, our model~\eqref{ourmodel1} is equal to the scalar-on-tensor model~\eqref{scalar} by defining
\begin{equation}
	\bbE[y^{(i)} | \tX^{(i)}] \colonequals \bbE[\tY_{i_1,...,i_K} |\mX_1 ,...,\mX_K ] \quad \text{and} \quad \tX^{(i)} \colonequals \mX_{1i_1} \circ \cdots \circ \mX_{K i_K}, \quad \text{for } i = 1,..., n,
\end{equation}
where $n = \prod_{i=1}^K d_k$.

\subsection{Tensor-on-tensor regression}
Let $\mathbb{Y} \in \bbR^{ d_{K+1} \times \cdots \times d_{N}}$ be the tensor observation, $\mathbb{X} \in \bbR^{ d_1 \times \cdots \times d_K}$ be the tensor predictor. Given the i.i.d.\ sample set $\{\mathbb{Y}^{(i)}, \mathbb{X}^{(i)} \}_{i=1}^n$,  the tensor-on-tensor regression is of form
\begin{equation}\label{tensor}
	g \of{\bbE[\mathbb{Y}^{(i)} | \mathbb{X}^{(i)}]} = \langle \mathbb{X}^{(i)}, \mathbb{B}   \rangle_K, \quad \text{for } i = 1,...,n,
\end{equation}

where $g(\cdot)$ is the inverse link function, $\mathbb{B} \in \bbR^{d_1 \times \cdots \times d_N}$ is the order-K tensor-valued coefficient, $\langle \cdot, \cdot \rangle_K$ refers to the contracted tensor product; i.e., for two tensors $\mathbb{P} \in \bbR^{I_1 \times \cdots \times I_L \times d_1 \times \cdots \times d_K}$ and $\mathbb{Q} \in \bbR^{d_1 \times \cdots \times d_K \times J_1 \times \cdots \times J_M}$, the contracted tensor product $\langle \mathbb{P}, \mathbb{Q} \rangle_K \in \bbR^{I_1\times \cdots \times I_L \times J_1 \times \cdots \times J_M}$ and the $(i_1,...,i_L, j_1,...,j_M)$-the entry is
\begin{equation}
	(\langle \mathbb{P}, \mathbb{Q} \rangle_K)_{i_1,...,i_L, j_1,...,j_M} = \sum_{a_1,...,a_K}^{d_1,...,d_K} \mathbb{P}_{i_1,...,i_L, a_1,...,a_K} \mathbb{Q}_{a_1,...,a_K, j_1,...,j_M}.
\end{equation}

Let $N = K$. Then, the contracted product becomes usual inner product, and $\mathbb{Y}^{(i)}$ becomes a scalar for $i \in [n]$. Consequently, the model~\eqref{tensor} degenerates to the scalar-on-tensor regression~\eqref{scalar}, which is equal to our model.


\section{Iteration time table}

See Table~\ref{tab:time}.
\begin{table}[h]
\centering
%\resizebox{1\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
Setting &Gaussian& Bernoulli & Poisson  \\ \hline
$d = 30, r = 6$& 2.6 &  6.0& 5.7\\
$d = 30, r = 3$&  0.5 & 1.1&1.3\\
$d = 25, r = 6$&1.4 & 3.7& 4.0\\
$d = 25, r = 3$&0.3& 0.6& 0.6\\
\hline
\end{tabular}
%}
\caption{ Iteration time of Algorithm 1. Numerical values are the running time (seconds) required for one iteration in Algorithm 1 under different settings.  }\label{tab:time}
\end{table}


\bibliography{tensor_wang}
\bibliographystyle{apalike}












\end{document}