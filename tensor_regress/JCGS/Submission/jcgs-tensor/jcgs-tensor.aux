\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\citation{zhou2013tensor}
\citation{pmlr-v108-berthet20a,hoff2005bilinear}
\citation{nickel2011three,hu2015scalable}
\citation{zhang2018mapping,wang2018learning,wang2019common}
\citation{narita2012tensor,lock2018supervised,rai2014scalable}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\newlabel{sec:intro}{{1}{2}{Introduction}{section.1}{}}
\citation{de2000multilinear,kolda2009tensor,hong2020generalized,wang2017tensor,bi2018recom,chi2012tensors}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Examples of supervised tensor decomposition with interactive side information. (a) Network population model. (b) Spatio-temporal growth model.}}{3}{figure.1}}
\newlabel{fig:intro1}{{1}{3}{Examples of supervised tensor decomposition with interactive side information. (a) Network population model. (b) Spatio-temporal growth model}{figure.1}{}}
\citation{raskutti2019convex,lock2018tensor,gahrooei2020multiple}
\citation{lock2018tensor,lock2018supervised}
\citation{raskutti2019convex}
\citation{lock2018tensor,raskutti2019convex}
\citation{adragni2009sufficient}
\citation{narita2012tensor,song2019tensor,cao2016semi}
\citation{ioannidis2019coupled,farias2019learning}
\citation{cao2016semi,song2019tensor}
\citation{kolda2009tensor}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{5}{section.2}}
\newlabel{sec:pre}{{2}{5}{Preliminaries}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Motivation and model}{6}{section.3}}
\newlabel{sec:model}{{3}{6}{Motivation and model}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}General framework for supervised tensor decomposition}{6}{subsection.3.1}}
\newlabel{eq:tensormodel}{{1}{6}{General framework for supervised tensor decomposition}{equation.3.1}{}}
\citation{adragni2009sufficient}
\newlabel{eq:factor}{{2}{7}{General framework for supervised tensor decomposition}{equation.3.2}{}}
\MT@newlabel{eq:tensormodel}
\MT@newlabel{eq:factor}
\newlabel{eq:decomp}{{3}{7}{General framework for supervised tensor decomposition}{equation.3.3}{}}
\MT@newlabel{eq:decomp}
\citation{gabriel1998generalised,srivastava2008models}
\citation{rabusseau2016low}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Three examples}{8}{subsection.3.2}}
\MT@newlabel{eq:decomp}
\newlabel{eq:time}{{4}{8}{Spatio-temporal growth model}{equation.3.4}{}}
\MT@newlabel{eq:time}
\MT@newlabel{eq:decomp}
\newlabel{example:brain}{{2}{8}{Network population model}{example.2}{}}
\newlabel{eq:network}{{5}{9}{Network population model}{equation.3.5}{}}
\MT@newlabel{eq:network}
\MT@newlabel{eq:network}
\citation{hoff2005bilinear}
\citation{de2000multilinear,hong2020generalized}
\newlabel{eq:edge}{{6}{10}{Dyadic data with node attributes}{equation.3.6}{}}
\MT@newlabel{eq:decomp}
\MT@newlabel{eq:edge}
\MT@newlabel{eq:decomp}
\MT@newlabel{eq:decomp}
\MT@newlabel{eq:decomp}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Connection to sufficient dimension reduction and tensor-on-tensor regression}{10}{subsection.3.3}}
\newlabel{sec:connection}{{3.3}{10}{Connection to sufficient dimension reduction and tensor-on-tensor regression}{subsection.3.3}{}}
\MT@newlabel{eq:decomp}
\citation{adragni2009sufficient}
\citation{raskutti2019convex,lock2018tensor,gahrooei2020multiple,hao2020sparse}
\MT@newlabel{eq:decomp}
\MT@newlabel{eq:decomp}
\newlabel{eq:interaction}{{7}{11}{Connection to sufficient dimension reduction and tensor-on-tensor regression}{equation.3.7}{}}
\MT@newlabel{eq:interaction}
\MT@newlabel{eq:interaction}
\@writefile{toc}{\contentsline {section}{\numberline {4}Estimation}{12}{section.4}}
\newlabel{sec:est}{{4}{12}{Estimation}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Rank-constrained M-estimator}{12}{subsection.4.1}}
\MT@newlabel{eq:decomp}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Canonical links for common distributions.}}{12}{table.1}}
\newlabel{table:link}{{1}{12}{Canonical links for common distributions}{table.1}{}}
\MT@newlabel{eq:decomp}
\newlabel{eq:loglikelihood}{{4.1}{12}{Rank-constrained M-estimator}{table.1}{}}
\newlabel{eq:MLE}{{8}{12}{Rank-constrained M-estimator}{equation.4.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Alternating optimization}{13}{subsection.4.2}}
\newlabel{subsec:alg}{{4.2}{13}{Alternating optimization}{subsection.4.2}{}}
\MT@newlabel{eq:MLE}
\MT@newlabel{eq:MLE}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Supervised Tensor Decomposition with Interactive Side Information}}{13}{algorithm.1}}
\newlabel{alg:B}{{1}{13}{Alternating optimization}{algorithm.1}{}}
\MT@newlabel{eq:MLE}
\newlabel{eq:equivalance}{{9}{14}{Equivalence class}{equation.4.9}{}}
\MT@newlabel{eq:factor}
\newlabel{prop:alg}{{4.1}{14}{Global convergence}{prop.4.1}{}}
\MT@newlabel{eq:equivalance}
\MT@newlabel{eq:decomp}
\MT@newlabel{eq:decomp}
\citation{kolda2009tensor}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Trajectory of the objective function with various dimension $d$ and rank $r$ under (a) Gaussian (b) Bernoulli (c) Poisson models. The dashed line represents the objective value at true parameters. }}{15}{figure.2}}
\newlabel{fig:loglike}{{2}{15}{Trajectory of the objective function with various dimension $d$ and rank $r$ under (a) Gaussian (b) Bernoulli (c) Poisson models. The dashed line represents the objective value at true parameters}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Rank selection and computational complexity}{15}{subsection.4.3}}
\newlabel{sec:tuning}{{4.3}{15}{Rank selection and computational complexity}{subsection.4.3}{}}
\newlabel{eq:BIC}{{10}{15}{Rank selection and computational complexity}{equation.4.10}{}}
\citation{raskutti2019convex}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Statistical properties}{16}{subsection.4.4}}
\newlabel{subsec:statprob}{{4.4}{16}{Statistical properties}{subsection.4.4}{}}
\MT@newlabel{eq:MLE}
\newlabel{ass}{{1}{16}{}{assumption.1}{}}
\MT@newlabel{eq:MLE}
\newlabel{thm:main}{{4.1}{17}{Statistical convergence}{thm.4.1}{}}
\MT@newlabel{eq:decomp}
\MT@newlabel{eq:MLE}
\newlabel{eq:bound}{{11}{17}{Statistical convergence}{equation.4.11}{}}
\newlabel{eq:sinebound}{{4.1}{17}{Statistical convergence}{equation.4.11}{}}
\MT@newlabel{eq:MLE}
\MT@newlabel{eq:bound}
\newlabel{thm:KL}{{4.1}{18}{Prediction error}{cor.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical experiments}{18}{section.5}}
\newlabel{sec:simulation}{{5}{18}{Numerical experiments}{section.5}{}}
\MT@newlabel{eq:decomp}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Rank selection via BIC. The estimated ranks are averaged across 30 simulation. Bold number indicates the ground truth is within two standard deviations of the estimate.}}{19}{table.2}}
\newlabel{tab:rank}{{2}{19}{Rank selection via BIC. The estimated ranks are averaged across 30 simulation. Bold number indicates the ground truth is within two standard deviations of the estimate}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Finite-sample performance}{19}{subsection.5.1}}
\MT@newlabel{eq:BIC}
\citation{abbe2017community}
\citation{zeng2019multiway}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Estimation error against effective sample size. The three panels plot the MSE when the response tensors are generated from (a) Gaussian (b) Poisson and (c) Bernoulli models. The dashed curves correspond to $\mathcal  {O}({1/d^2})$.}}{20}{figure.3}}
\newlabel{fig:dim}{{3}{20}{Estimation error against effective sample size. The three panels plot the MSE when the response tensors are generated from (a) Gaussian (b) Poisson and (c) Bernoulli models. The dashed curves correspond to $\tO ({1/d^2})$}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Comparison with GLMs under stochastic block models}{20}{subsection.5.2}}
\citation{rabusseau2016low}
\citation{zhao2012higher}
\citation{yu2016learning}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance comparison under stochastic block models. The three panels plot the MSE when the response tensors are generated from (a) Gaussian (b) Poisson and (c) Bernoulli models. The $x$-axis represents the number of blocks in the networks. }}{21}{figure.4}}
\newlabel{fig:glm}{{4}{21}{Performance comparison under stochastic block models. The three panels plot the MSE when the response tensors are generated from (a) Gaussian (b) Poisson and (c) Bernoulli models. The $x$-axis represents the number of blocks in the networks}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Comparison with other tensor methods}{21}{subsection.5.3}}
\citation{HCP}
\citation{zhang2018mapping}
\citation{desikan2006automated}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison between different tensor methods. Panels (a) and (b) plot MSPE versus the number of modes with available features. Panels (c) and (d) plot MSPE versus the effective sample size $d^2$. We consider rank $\bm  {r}=(3,3,3)$ (low), $\bm  {r}=(4,5,6)$ (high), and signal $\alpha =3 $ (low), $\alpha =6$ (high).}}{23}{figure.5}}
\newlabel{fig:compare}{{5}{23}{Comparison between different tensor methods. Panels (a) and (b) plot MSPE versus the number of modes with available features. Panels (c) and (d) plot MSPE versus the effective sample size $d^2$. We consider rank $\mr =(3,3,3)$ (low), $\mr =(4,5,6)$ (high), and signal $\alpha =3 $ (low), $\alpha =6$ (high)}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Data analyses}{23}{section.6}}
\newlabel{sec:data}{{6}{23}{Data analyses}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Application to human brain connection data}{23}{subsection.6.1}}
\citation{desikan2006automated}
\citation{ingalhalikar2014sex}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Top edges with large effects. (a) Global effect; (b) Female effect; (c) Age 22-25; (d) Age 31+. Red edges represent positive effects and blue edges represent negative effects. The edge-width is proportional to the magnitude of the effect size. }}{24}{figure.6}}
\newlabel{fig:brain}{{6}{24}{Top edges with large effects. (a) Global effect; (b) Female effect; (c) Age 22-25; (d) Age 31+. Red edges represent positive effects and blue edges represent negative effects. The edge-width is proportional to the magnitude of the effect size}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Comparison of estimated feature effects the HCP data using (a) multi-response GLM and (b) supervised tensor decomposition (STD). }}{24}{figure.7}}
\newlabel{fig:s1}{{7}{24}{Comparison of estimated feature effects the HCP data using (a) multi-response GLM and (b) supervised tensor decomposition (STD)}{figure.7}{}}
\citation{nickel2011three}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Application to political relation data}{25}{subsection.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Estimated feature effects in the \emph  {Nations} data analysis. Panels (a)-(d) represent the estimated effects of country-level attributes towards the connection probability, for relations \emph  {warning}, \emph  {violentactions}, \emph  {treaties}, and \emph  {aidenemy}, respectively. }}{26}{figure.8}}
\newlabel{fig:est}{{8}{26}{Estimated feature effects in the \emph {Nations} data analysis. Panels (a)-(d) represent the estimated effects of country-level attributes towards the connection probability, for relations \emph {warning}, \emph {violentactions}, \emph {treaties}, and \emph {aidenemy}, respectively}{figure.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces $K$-means clustering of relations based on dimension reduction on the 3$^\text  {rd}$ mode. }}{26}{table.3}}
\newlabel{tab:s1}{{3}{26}{$K$-means clustering of relations based on dimension reduction on the 3$^\text {rd}$ mode}{table.3}{}}
\citation{tibshirani1993introduction}
\citation{chen2019inference,xia2019confidence}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion and future work}{27}{section.7}}
\newlabel{sec:con}{{7}{27}{Discussion and future work}{section.7}{}}
\citation{lock2013joint,wang2019three}
\newlabel{eq:extention}{{7}{28}{Discussion and future work}{section.7}{}}
\bibdata{tensor_wang}
\bibcite{abbe2017community}{{1}{2017}{{Abbe}}{{}}}
\bibcite{adragni2009sufficient}{{2}{2009}{{Adragni and Cook}}{{}}}
\bibcite{pmlr-v108-berthet20a}{{3}{2020}{{Berthet and Baldin}}{{}}}
\bibcite{bi2018recom}{{4}{2018}{{Bi et~al.}}{{}}}
\bibcite{cao2016semi}{{5}{2016}{{Cao et~al.}}{{}}}
\bibcite{chen2019inference}{{6}{2019}{{Chen et~al.}}{{}}}
\bibcite{chi2012tensors}{{7}{2012}{{Chi and Kolda}}{{}}}
\bibcite{de2000multilinear}{{8}{2000}{{De~Lathauwer et~al.}}{{}}}
\bibcite{desikan2006automated}{{9}{2006}{{Desikan et~al.}}{{}}}
\bibcite{farias2019learning}{{10}{2019}{{Farias and Li}}{{}}}
\bibcite{gabriel1998generalised}{{11}{1998}{{Gabriel}}{{}}}
\bibcite{gahrooei2020multiple}{{12}{2020}{{Gahrooei et~al.}}{{}}}
\bibcite{HCP}{{13}{2016}{{Geddes}}{{}}}
\bibcite{hao2020sparse}{{14}{2020}{{Hao et~al.}}{{}}}
\bibcite{hoff2005bilinear}{{15}{2005}{{Hoff}}{{}}}
\bibcite{hong2020generalized}{{16}{2020}{{Hong et~al.}}{{}}}
\bibcite{hu2015scalable}{{17}{2015}{{Hu et~al.}}{{}}}
\bibcite{ingalhalikar2014sex}{{18}{2014}{{Ingalhalikar et~al.}}{{}}}
\bibcite{ioannidis2019coupled}{{19}{2019}{{Ioannidis et~al.}}{{}}}
\bibcite{kolda2009tensor}{{20}{2009}{{Kolda and Bader}}{{}}}
\bibcite{lock2018tensor}{{21}{2018}{{Lock}}{{}}}
\bibcite{lock2013joint}{{22}{2013}{{Lock et~al.}}{{}}}
\bibcite{lock2018supervised}{{23}{2018}{{Lock and Li}}{{}}}
\bibcite{narita2012tensor}{{24}{2012}{{Narita et~al.}}{{}}}
\bibcite{nickel2011three}{{25}{2011}{{Nickel et~al.}}{{}}}
\bibcite{rabusseau2016low}{{26}{2016}{{Rabusseau and Kadri}}{{}}}
\bibcite{rai2014scalable}{{27}{2014}{{Rai et~al.}}{{}}}
\bibcite{raskutti2019convex}{{28}{2019}{{Raskutti et~al.}}{{}}}
\bibcite{song2019tensor}{{29}{2019}{{Song et~al.}}{{}}}
\bibcite{srivastava2008models}{{30}{2008}{{Srivastava et~al.}}{{}}}
\bibcite{tibshirani1993introduction}{{31}{1993}{{Tibshirani and Efron}}{{}}}
\bibcite{wang2019common}{{32}{2019a}{{Wang et~al.}}{{}}}
\bibcite{wang2019three}{{33}{2019b}{{Wang et~al.}}{{}}}
\bibcite{wang2018learning}{{34}{2020}{{Wang and Li}}{{}}}
\bibcite{wang2017tensor}{{35}{2017}{{Wang and Song}}{{}}}
\bibcite{zeng2019multiway}{{36}{2019}{{Wang and Zeng}}{{}}}
\bibcite{xia2019confidence}{{37}{2019}{{Xia}}{{}}}
\bibcite{yu2016learning}{{38}{2016}{{Yu and Liu}}{{}}}
\bibcite{zhang2018mapping}{{39}{2018}{{Zhang et~al.}}{{}}}
\bibcite{zhao2012higher}{{40}{2012}{{Zhao et~al.}}{{}}}
\bibcite{zhou2013tensor}{{41}{2013}{{Zhou et~al.}}{{}}}
\bibstyle{apalike}
