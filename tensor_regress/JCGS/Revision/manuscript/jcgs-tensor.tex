\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url}
\usepackage{enumitem}

\usepackage{amsmath,amssymb,amsthm,bm,mathtools}
\usepackage{algorithm}
\usepackage{dsfont,multirow,hyperref,setspace,enumerate}
\hypersetup{colorlinks,linkcolor={black},citecolor={black},urlcolor={black}}
\usepackage{lscape}
\usepackage{afterpage}
%\usepackage{pifont}% http://ctan.org/pkg/pifont
%\newcommand{\cmark}{\ding{51}}%
%\newcommand{\xmark}{\ding{55}}%

\allowdisplaybreaks
%\usepackage{enumerate}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}[section]
\newtheorem{pro}{Property}
\newtheorem{cor}{Corollary}[section]

\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}
\newtheorem{condition}{Condition}

\newtheorem{innercustomgeneric}{\customgenericname}
\providecommand{\customgenericname}{}
\newcommand{\newcustomtheorem}[2]{%
  \newenvironment{#1}[1]
  {%
   \renewcommand\customgenericname{#2}%
   \renewcommand\theinnercustomgeneric{##1}%
   \innercustomgeneric
  }
  {\endinnercustomgeneric}
}

\newcustomtheorem{customexample}{Example}

\usepackage{appendix}
\usepackage{wrapfig}
\mathtoolsset{showonlyrefs}


\input macros.tex


\usepackage[english]{babel}

\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}


\usepackage{algpseudocode,algorithm}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}

\def\ci{\perp\!\!\!\perp}

\def\fixme#1#2{\textbf{\color{red}[FIXME (#1): #2]}}
\usepackage{booktabs}
\newcommand\doubleRule{\toprule\toprule}
\allowdisplaybreaks

\usepackage{xr}
% this is for overleaf cross reference
\makeatletter
\newcommand*{\addFileDependency}[1]{% argument=file name and extension
  \typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother

\newcommand*{\myexternaldocument}[1]{%
    \externaldocument{#1}%
    \addFileDependency{#1.tex}%
    \addFileDependency{#1.aux}%
}
\myexternaldocument{supp}




\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\if0\blind
{
  \title{\bf Supervised tensor decomposition with features on multiple modes}
  \author{Jiaxin Hu, Chanwoo Lee, and Miaoyan Wang\thanks{The authors gratefully acknowledge NSF grant DMS-1915978 and funding from the Wisconsin Alumni Research Foundation. }\hspace{.2cm}\\
    Department of Statistics, University of Wisconsin-Madison}
  \maketitle
  
  
  
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Supervised tensor decomposition with features on multiple modes}
\end{center}
  \medskip
} \fi


\bigskip
\begin{abstract}
Higher-order tensors have received increased attention across science and engineering. While most tensor decomposition methods are developed for a single tensor observation, scientific studies often collect side information, in the form of node features and interactions thereof, together with the tensor data. Such data problems are common in neuroimaging, network analysis, and spatial-temporal modeling. Identifying the relationship between a high-dimensional tensor and side information is important yet challenging. Here, we develop a tensor decomposition method that incorporates multiple feature matrices as side information. Unlike unsupervised tensor decomposition, our supervised decomposition captures the effective dimension reduction of the data tensor confined to feature space of interest. An efficient alternating optimization algorithm with provable spectral initialization is further developed. Our proposal handles a broad range of data types, including continuous, count, and binary observations. We apply the method to diffusion tensor imaging data from human connectome project and multi-relational political network data. We identify the key global connectivity pattern and pinpoint the local regions that are associated with available features. The package and data used are available at~\url{https://CRAN.R-project.org/package=tensorregress}. Supplementary materials for this article are available online.

\end{abstract}

\noindent%
{\it Keywords:}  Tensor data analysis, Supervised dimension reduction, Exponential family distribution, Generalized multilinear model, Alternating optimization
\vfill

\newpage
\spacingset{1.5} % DON'T change the spacing!
\section{Introduction}
\label{sec:intro}
Multi-dimensional arrays, known as tensors, are often collected with side information on multiple modes in modern scientific and engineering studies. A popular example is in neuroimaging~\citep{zhou2013tensor}. The brain connectivity networks are collected from a sample of individuals, accompanied by individual characteristics such as age, gender, and diseases status (see Figure~\ref{fig:intro1}a). Another example is in network analysis~\citep{pmlr-v108-berthet20a,hoff2005bilinear}. A typical social network consists of nodes that represent people and edges that represent  friendships. Side information such as peopleâ€™s demographic information and friendship types are often available. In both examples, we are interested in identifying the variation in the data tensor (e.g., brain connectivities, social community patterns) that is affected by available features. These seemingly different scenarios pose a common yet challenging problem for tensor data modeling. 

In addition to the aforementioned challenges, many tensor datasets consist of non-Gaussian measurements. Examples include the political interaction dataset \citep{nickel2011three} which measures action counts between countries under various relations, and the brain connectivity network dataset \citep{zhou2013tensor,wang2020learning} which is a collection of binary adjacency matrices. Classical tensor decomposition methods are based on minimizing the Frobenius norm of deviation, leading to suboptimal predictions for binary- or count-valued response variables. A number of supervised tensor methods have been proposed \citep{lock2018supervised,li2017parsimonious,sun2017store,hao2019sparse,raskutti2019convex} to address the tensor regression problem in various forms, such as scalar-to-tensor regression and tensor-response regression. These methods often assume Gaussian distribution for the tensor entries, or impose random designs for the feature matrices, both of which are less suitable for applications of our interest. The gap between theory and practice means a great opportunity to model paradigms and better capture the complexity in tensor data. 

\begin{figure*}[t]
\begin{center}
\includegraphics[width=16cm]{demo.pdf}
\end{center}
\caption{Examples of supervised tensor decomposition with side information. (a) Network population model. (b) Spatio-temporal growth model.} \label{fig:intro1}
\vspace{-.2cm}
\end{figure*}

We present a general model and associated method for decomposing a data tensor whose entries are from exponential family with side information. We formulate the learning task as a structured regression problem, with tensor observation serving as the response, and the multiple side information as features. Figure~\ref{fig:intro1}b illustrates our model in the special case of order-3 tensors. A low-rank structure is imposed to the conditional mean of tensor observation, where unlike classical decomposition, the tensor factors $\mX_k\mM_k\in\mathbb{R}^{d_k\times r_k}$ belong to the space spanned by features $\mX_k\in\mathbb{R}^{d_k\times p_k}$ for $k=1,2,3$. The unknown matrices $\mM_k\in\mathbb{R}^{p_k\times r_k}$ (referred to as ``dimension reduction matrices'') link the conditional mean to the feature spaces, thereby allowing the identification of variations in the tensor data attributable to the side information.


Our proposal blends the modeling power of generalized linear model (GLM) and the exploratory capability of tensor dimension reduction in order to take the best out of both worlds. We leverage GLM to allow heteroscedacity due to the mean-variance relationship in the non-Gaussian data. This flexibility is important in practice. Furthermore, our low-rank model on the (transformed) conditional mean tensor effectively mitigates the curse of high dimensionality. In classical GLM, the sample size and feature dimension are well defined; however, in the tensor data analysis, we observe only one realization of an order-$K$ tensor and up to $K$ feature matrices. Both the number of tensor entries and feature dimension grow exponentially in $K$. Dimension reduction is therefore crucial for prediction and interpretability. We establish the statistical and algorithmic convergences of our estimator, and we quantify the gain in accuracy through simulations and case studies. 
 
 
Our work is closely related to but also clearly distinctive from several lines of previous work. {The first line is a class of}\label{change:firstline} \textit{unsupervised} tensor decomposition such as classical Tucker and CP decomposition~\citep{de2000multilinear, kolda2009tensor} and generalized decomposition for non-Gaussian data \citep{chi2012tensors, tarzanagh2019regularized,hong2020generalized,  li2020generalized}. {\color{blue}Regardless of the implementation, the unsupervised methods aim to find the best low-rank representation of a data tensor alone. In contrast, our model is a \textit{supervised} tensor learning, which aims to identify the association between a data tensor and multiple features. The low-rank factorization is determined jointly by the tensor data and feature matrices in our model.}

The second line of work studies the tensor-to-tensor regression. {\color{blue}This category is further divided into three scenarios, depending on whether tensor is treated as predictors~\citep{zhou2013tensor,raskutti2019convex,han2020optimal}, as responses~\citep{li2017parsimonious,sun2017store,zhang2018network,lock2018supervised,luo2018leveraging}, or both~\citep{lock2018tensor,gahrooei2020multiple}. As we show in Section~\ref{sec:connection}, our supervised tensor decomposition falls into this general category, and we provide a \emph{provable} solution in new settings that have broader practical significance. Earlier work in this vein~\citep{lock2018tensor,lock2018supervised,gahrooei2020multiple,li2020generalized} focuses on algorithm development, but not on the statistical accuracy. \cite{li2017parsimonious} introduces an envelope-based approach to identify sufficient dimension reduction~\citep{adragni2009sufficient}, but its theory is restricted to Gaussian data with one-sided feature matrix only. \cite{raskutti2019convex} establishes the statistical accuracy for convex relaxed maximum likelihood estimator (MLE) of tensor regression. However, convex relaxation for tensor optimizations suffers from computational intractability and statistical sub-optimality. Recent work has demonstrated the success of non-convex approaches in various tensor problems~\citep{sun2017store,zhang2018network,raskutti2019convex,han2020optimal}; we go step further by allowing multiple feature matrices with either fixed or random designs. In Sections~\ref{subsec:statprob}, we show that incorporating multiple feature matrices substantially improves the statistical accuracy. We provide a detailed comparison in Section~\ref{sec:connection}; see Table~\ref{table:comp_table}.}



The third line of work uses side information for various tensor learning tasks, such as for completion~\citep{song2019tensor} and for recommendation system~\citep{farias2019learning}. These methods also study tensors with side information, but they take data-mining approaches to penalize predictions that are distant from side information. One important difference is that their goal is prediction but not parameter estimation. The effects of features and their interactions are not estimated in these data-driven approaches. In contrast, our goal is interpretable prediction, and we estimate the low-rank decomposition using a model-based approach.
The model-based approaches benefits the interpretability in prediction. In this regards, our method opens up new opportunities for tensor data analysis in a wider range of applications.


The remainder of the paper is organized as follows. Section~\ref{sec:pre} introduces tensor preliminaries. Section~\ref{sec:model} presents the main model and three motivating examples for supervised tensor decomposition. We describe the likelihood estimation and alternating optimization algorithm with theoretical guarantees in Section~\ref{sec:est}. Connection with related work is provided in Section~\ref{sec:connection}. In Section~\ref{sec:simulation}, we present numerical experiments and assess the performance in comparison to alternative methods. In Section~\ref{sec:data}, we apply the method to diffusion tensor imaging data from human connectome project and multi-relational social network data. We conclude in Section~\ref{sec:con} with discussions about our findings and avenues of future work. All proofs are deferred to Supplementary Notes. 

\section{Preliminaries}\label{sec:pre}
We introduce the basic tensor properties used in the paper. We use lower-case letters (e.g.,\ $a,b,c$) for scalars and vectors, upper-case boldface letters (e.g.,\ $\mA, \mB, \mC$) for matrices, and calligraphy letters (e.g.,\ $\tA, \tB, \tC$) for tensors of order three or greater. We use $\mI$ to denote the identity matrix whose dimension may vary from line by line given the contexts. Let $\tY=\entry{y_{i_1,\ldots,i_K}}\in \mathbb{R}^{d_1\times \cdots\times d_K}$ denote an order-$K$ $(d_1,\ldots,d_K)$-dimensional tensor, where $K$ is the number of modes and also called the order. The multilinear multiplication of a tensor $\tY\in\mathbb{R}^{d_1\times \cdots\times d_K}$ by matrices $\mX_k=\entry{x_{i_k,j_k}^{(k)}}\in\mathbb{R}^{p_k\times d_k}$ is defined as 
\[
\tY \times_1\mX_1\times \cdots \times_K \mX_K=\entry{\sum_{i_1,\ldots,i_K}y_{i_1,\ldots,i_K}x_{j_1,i_1}^{(1)}\cdots x_{j_K,i_K}^{(K)}},
\]
which results in an order-$K$ $(p_1,\ldots,p_K)$-dimensional tensor. For ease of presentation, we use the shorthand $\tY\times\{\mX_1,\ldots,\mX_K\}$ to denote the tensor-by-matrix product. For any two tensors $\tY=\entry{y_{i_1,\ldots,i_K}}$, $\tY'=\entry{y'_{i_1,\ldots,i_K}}$ of identical order and dimensions, their inner product is defined as 
\[
\langle \tY,\ \tY'\rangle =\sum_{i_1,\ldots,i_K}y_{i_1,\ldots,i_K}y'_{i_1,\ldots,i_K}.
\]
The tensor Frobenius norm and maximum norm are defined as
\[
\FnormSize{}{\tY}=\langle \tY,\ \tY \rangle^{1/2},\quad \text{and}\quad \mnormSize{}{\tY}=\max_{i_1,\ldots,i_K}y_{i_1,\ldots,i_K}.
\]
When $a$ is a vector, we use $\vnormSize{}{a}=\langle a,a \rangle^{1/2}$ to denote the vector 2-norm. We use $[d]$ to denote the $d$-set $[d]=\{1,\ldots,d\}$, and use $\mathbb{O}(d,r)$ to denote the collection of all $d$-by-$r$ matrices with orthonormal columns; i.e., $\mathbb{O}(d,r)=\{\mP\in\mathbb{R}^{d\times r}\colon \mP^T\mP = \mI\}$. 

A higher-order tensor can be reshaped into a lower-order object. We use $\textup{vec}(\cdot)$ to denote the operation that reshapes the tensor into a vector, and $\text{Unfold}_k(\cdot)$ to denote the unfolding operation that reshapes the tensor along mode $k$ into a matrix of size $d_k$-by-$\prod_{i\neq k}d_i$. We use $\text{rank}(\tY)=\mr$ to denote the multilinear rank of an order-$K$ tensor $\tY$, where $\mr=(r_1,\ldots,r_K)$ is a length-$K$ vector and $r_k$ is the rank of matrix $\textup{Unfold}_k(\tY)$ for $k\in[K]$. For ease of notation, we allow the basic arithmetic operators (e.g.,\ $+, -, \geq $) and univariate functions $f\colon \mathbb{R}\to \mathbb{R}$ to be applied to tensors in an element-wise manner. For two positive sequences $\{a_n\}$ and $\{b_n\}$, we use $a_n\lesssim b_n$ or $a_n = \tO(b_n)$ to denote the fact that $a_n \leq Cb_n$ for some constant $C >0$.

\section{Motivation and model}\label{sec:model}
\subsection{General framework for tensor decomposition}\label{sec:mainmodel}
We begin with a general framework for supervised tensor decomposition and then discuss its implication in three concrete examples. Let $\tY=\entry{y_{i_1,\ldots,i_K}}\in\mathbb{R}^{d_1\times \cdots\times d_K}$ denote an order-$K$ data tensor. Suppose the side information is available on each of the $K$ modes. Let $\mX_k=\entry{x_{ij}}\in\mathbb{R}^{d_k\times p_k}$ denote the feature matrix on the mode $k\in[K]$, where $x_{ij}$ denotes the $j$-th feature value for the $i$-th tensor entity, for $(i,j)\in[d_k]\times[p_k]$. 

We propose a multilinear conditional mean model between the data tensor and feature matrices. Assume that, conditional on the features $\mX_k$, the entries of tensor $\tY$ are independent realizations from an exponential family distribution. Further, the conditional mean tensor admits the rank-$\mr$ model with $\mr=(r_1,\ldots,r_K)$,
\begin{align}\label{eq:decomp}
\mathbb{E}(\tY|\mX_1,\ldots,\mX_K) &= f\left(\tC\times\{\mX_1\mM_1, \ldots, \mX_K\mM_K\}\right),\notag\\
\text{with} \ &\ \mM_k^T\mM_k = \mI_{r_k},\ \mM_k\in\mathbb{R}^{p_k\times r_k}\quad \text{for all }k=1,\ldots,K,
\end{align}
where $\tC \in \mathbb{R}^{r_1\times \cdots \times r_K}$ is an unknown full-rank core tensor, $\mM_k \in \mathbb{R}^{p_k\times r_k}$ are unknown factor matrices for all $k\in [K]$, $f(\cdot)$ is a known link function whose form depending on the data type of $\tY$, and $\times$ denotes the tensor-by-matrix product. The choice of link function is based on the assumed distribution family of tensor entries. Common choices of link functions include identity link for Gaussian distribution, logistic link for Bernoulli distribution, and exponential link for Poisson distribution. In general, dispersion parameters can also be included in the model. Because our main focus is the tensor decomposition under the mean model, we suppress the dispersion parameter in this section for ease of presentation. 

Figure~\ref{fig:intro1}b provides a schematic illustration of our model. The features $\mX_k$ affect the distribution of tensor entries in $\tY$ through the reduced features $\mX_k\mM_k$, which are $r_k$ linear combinations of features on mode $k$. We call $\mM_k$ the ``dimension reduction matrix'' or ``tensor factors.'' The core tensor $\tC$ collects the interaction effects between reduced features across $K$ modes. We call $\tB=\tC\times\{\mM_1,\ldots,\mM_K\}$ the coefficient tensor, and $\Theta=\tB\times\{\mX_1,\cdots,\mX_K\}$ the linear predictor. By the definition of multilinear rank, the model~\eqref{eq:decomp} implies the linear predictor $\Theta$ and coefficient tensor $\tB$ are of rank-$\mr$. The conditional mean tensor $\mathbb{E}(\tY|\mX_1,\ldots,\mX_K)$ is however often high rank, due to the nonlinearity of the link function~\citep{lee2021beyond}. 

Our goal is to estimate the low-rank tensor $\tB$, or equivalently, the core tensor and factors $(\tC, \mM_1,\ldots,\mM_K)$, from our model~\eqref{eq:decomp}. {\color{blue}We make several remarks about model identifiability. First, the identifiability of $\tB$ requires the feature matrices $\mX_k$ are of full column rank with $p_k\leq d_k$. We impose this rank non-deficiency assumption to $\mX_k$; this is a mild condition common in literature~\citep{lock2018supervised, li2017parsimonious, li2020generalized}. In the presence of rank deficiency, we recommend to remove redundant features from $\mX_k$ before applying our method. Second, the decomposition $\tB = \tC\times\{\mM_1,\ldots,\mM_K\}$ are non-unique, as in standard tensor decomposition~\citep{kolda2009tensor}. For any invertible matrices $\mO_k\in\mathbb{R}^{r_k\times r_k}$, $\tB=\tC\times\{\mM_1,\ldots,\mM_K\}=\tC'\times\{\mM_1\mO_1,\ldots,\mM_K\mO_K\}$ are two equivalent parameterizations with $\tC'=\tC\times\{\mO^{-1}_1,\ldots,\mO^{-1}_K\}$ . To resolve this ambiguity, we impose orthonormality to $\mM_k\in\mathbb{O}(p_k,r_k)$ and assess the estimation error of $\mM_k$ using angle distance. The angle distance is invariant to orthogonal rotations due to its geometric definition. See Section~\ref{subsec:statprob} for more details. The orthonormality of $\mM_k$ is imposed purely for technical convenience. This normalization incurs no impacts in our statistical inference, but may help with numerical stability in empirical optimization~\citep{de2000multilinear, kolda2009tensor}.} Finally, the problem size is quantified by $p_k$ and $d_k$, where $p_k$ specifies the number of features and $d_k$ the number of samples at mode $k\in[K]$. Our theory treats the rank $r_k$ as known and fixed, whereas both $p_k$ and $d_k$ are allowed to increase. The adaptation to unknown rank in practice will be addressed in Section~\ref{sec:tuning}. 


\subsection{Three examples}\label{sec:motivation}
We give three seemingly different examples that can all be formulated as our supervised tensor decomposition model~\eqref{eq:decomp}.
\begin{example}[Spatio-temporal growth model]
The growth curve model~\citep{gabriel1998generalised,srivastava2008models} was originally proposed as an example of bilinear model for matrix data, and we adopt its higher-order extension here. Let $\tY=\entry{y_{ijk}}\in\mathbb{R}^{d \times m\times n}$ denote the pH measurements of $d$ lakes at $m$ levels of depth and for $n$ time points. Suppose the sampled lakes belong to $q$ types, with $p$ lakes in each type. Let $\{\ell_j\}_{j\in[m]}$ denote the sampled depth levels and $\{t_k\}_{k\in[n]}$ the time points. Assume that the expected pH trend in depth is a polynomial of order at most $r$ and that the expected trend in time is a polynomial of order $s$. Then, the conditional mean model for the spatio-temporal growth can be represented as
\begin{equation}\label{eq:time}
\mathbb{E}(\tY|\mX_1,\mX_2,\mX_3)=\tC\times\{\mX_1\mM_1,\ \mX_2\mM_2,\ \mX_3\mM_3\},
\end{equation}
where $\mX_1=\text{blockdiag}\{\mathbf{1}_p,\ldots,\mathbf{1}_p\}\in \{0,1\}^{d\times q}$ is the design matrix for lake types, and
\[
\mX_2=
\begin{pmatrix}
1 & \ell_1&\cdots &\ell^{r}_1\\
1 & \ell_2&\cdots &\ell^{r}_2\\
\vdots &\vdots&\ddots&\vdots\\
1&\ell_{m}&\cdots&\ell^{r}_{m}
\end{pmatrix},\quad
\mX_3=
\begin{pmatrix}
1 & t_1&\cdots &t^{s}_1\\
1 & t_2&\cdots &t^{s}_2\\
\vdots &\vdots&\ddots&\vdots\\
1&t_{n}&\cdots&t^{s}_{n}
\end{pmatrix}
\]
are the design matrices for spatial and temporal effects, respectively, $\tC\in\mathbb{R}^{r_1\times r_2\times r_3}$ is the unknown core tensor, and $\mM_k$ are unknown dimension reduction matrices on each mode. The factors $\mX_k\mM_k$ are reduced features in the mean model~\eqref{eq:time}. The spatial-temporal model is a special case of our supervised tensor decomposition model~\eqref{eq:decomp}, with features available on each of the three modes.
\end{example}


\begin{example}[Network population model]\label{example:brain}
Network response model~\citep{rabusseau2016low} is recently developed for neuroimaging analysis. The goal is to study the relationship between brain network connectivity pattern and features of individuals. Suppose we have a sample of $n$ observations, $\{(\mY_i, \mx_i)\colon i=1,\ldots,n\}$, where for each individual $i\in[n]$, $\mY_i\in\{0,1\}^{d\times d}$ is the undirected adjacency matrix whose entries indicate presences/absences of connectivities between $d$ brain nodes, and $\mx_i\in\mathbb{R}^p$ is the individual's feature such as age, gender, cognition score, etc. The network-response model  has the conditional mean
\begin{equation}\label{eq:network}
\textup{logit}(\mathbb{E}(\mY_i|\mx_i))=\tB\times_3\mx_i, \quad \text{for }i=1,\ldots,n,
\end{equation}
where $\tB\in \mathbb{R}^{d\times d\times p}$ is a rank-$(r_1,r_1,r_2)$ coefficient tensor, and $\tB$ is assumed to be symmetric in the first two modes.  

The model~\eqref{eq:network} is a special case of our supervised tensor decomposition, with feature matrix on the last mode of the tensor. Specifically, we stack the network observations $\{\mY_i\}$ together and obtain an order-3 response tensor $\tY\in\{0,1\}^{d\times d\times n}$. Define a feature matrix $\mX=[\mx_1,\ldots,\mx_n]^T\in\mathbb{R}^{n\times p}$. Then, the model~\eqref{eq:network} has the equivalent representation of supervised tensor decomposition,
\[
\textup{logit}(\mathbb{E}(\tY|\mX))=\tC\times\{\mM,\ \mM,\ \mX\mM'\},
\]
where $\tC\in\mathbb{R}^{r_1\times r_1\times r_2}$ is the core tensor, $\mM\in\mathbb{R}^{d\times r_1}$ is the dimension reduction matrix on the first two modes, and $\mM'\in\mathbb{R}^{p\times r_2}$ is for the last mode.  \end{example}
 
 \begin{example}[Dyadic data with node attributes] Dyadic dataset consists of measurements on pairs of objects. Common examples include graphs and networks. Let $\tG=(V,E)$ denote a graph, where $V=[d]$ is the node set of the graph, and $E\subset V\times V$ is the edge set. Suppose that we also observe feature vector $\mx_i\in\mathbb{R}^p$ associated to each node $i\in V$. A probabilistic model on the graph $\tG=(V,E)$ can be described by the following matrix regression. The edge connects the two vertices $i$ and $j$ independently of other pairs, and the probability of connection is modeled as
\begin{equation}\label{eq:edge}
 \textup{logit}\left(\mathbb{P}\left((i,j)\in E\right)\right)=\mx^T_i\mB\mx_j=\langle \mB,\ \mx^T_i\mx_j\rangle,
 \end{equation}
 where $\mB\in\mathbb{R}^{p\times p}$ is a symmetric rank-$r$ matrix. The low-rankness in $\mB$ has demonstrated its success in modeling transitivity, balance, and communities in  networks~\citep{hoff2005bilinear}. We show that our supervised tensor decompostion~\eqref{eq:decomp} also incorporates the graph model as a special case. Let $\tY=\entry{y_{ij}}$ be a binary matrix where $y_{ij}=\mathds{1}_{(i,j)\in E}$. Define $\mX=[\mx_1,\ldots,\mx_n]^T\in\mathbb{R}^{n\times p}$. Then, the graph model~\eqref{eq:edge} can be expressed as
 \[
 \textup{logit}(\mathbb{E}(\mY|\mX))=\mC\times\{\mX\mM,\ \mX\mM\}, 
  \]
  where $\mC\in\mathbb{R}^{r\times r}, \mM\in\mathbb{R}^{p\times r}$ are from the singular value decomposition of $\mB=\mM\mC\mM^T$. 
  \end{example}




In the above three examples and many other studies, researchers are interested in uncovering the variation in the data tensor that can be explained by features. Our supervised tensor decomposition~\eqref{eq:decomp} allows arbitrary numbers of feature matrices. When certain mode $k$ has no side information, we set $\mX_k=\mI$ in the model~\eqref{eq:decomp}. In particular, our model~\eqref{eq:decomp} reduces to classical unsupervised tensor decomposition~\citep{de2000multilinear,hong2020generalized} when no side information is available; i.e., $\mX_k=\mI$ for all $k\in[K]$.


\section{Estimation}\label{sec:est}

\subsection{Rank-constrained MLE}\label{sec:rankM}
We develop a likelihood-based procedure to estimate $\tC$ and $\mM_k$ in~\eqref{eq:decomp}. We adopt the exponential family as a flexible framework for different data types. In a classical generalized linear model with a scalar response $y$ and feature $\mx$, the density is expressed as
\[
p(y|\mx, \boldsymbol{\beta})=c(y,\phi)\exp\left(\frac{y\theta- b(\theta)}{\phi}\right)\ \text{with}\ \theta=\boldsymbol{\beta}^T\mx,
\]
where $b(\cdot)$ is a known function, $\theta$ is the linear predictor, $\phi>0$ is the dispersion parameter, and $c(\cdot)$ is a known normalizing function. The choice of link functions depends on the data types and on the observation domain of $y$, denoted $\mathbb{Y}$. For example, the observation domain is $\mathbb{Y}=\mathbb{R}$ for continuous data, $\mathbb{Y}=\mathbb{N}$ for count data, and  $\mathbb{Y}=\{0,1\}$ for binary data. The canonical link function $f$ is chosen to be $f(\cdot)=b'(\cdot)$, the first-order derivative of $b(\cdot)$. Table~\ref{table:link} summarizes the canonical link functions for common types of distributions. 

\begin{table}[htb]
\centering
\begin{tabular}{c|cccc}
Data type &Gaussian & Poisson& Bernoulli &  \\
\hline
Domain $\mathbb{Y}$& $\mathbb{R}$&$\mathbb{N}$&$\{0,1\}$  \\
 $b(\theta)$&$\theta^2/2$& $\exp(\theta)$&$\log (1+\exp(\theta))$  \\
 link $f(\theta)$&$\theta$&$\exp(\theta)$&$(1+\exp(-\theta))^{-1}$ \\
\end{tabular}
\caption{Canonical links for common distributions.}\label{table:link}
\end{table}


In our context, we model the entries in data tensor $\tY$, conditional on linear predictor $\Theta$, as independent draws from an exponential family. Ignoring constants that do not depend on $\Theta$, the quasi log-likelihood of~\eqref{eq:decomp} is equal to Bregman distance between $\tY$ and $b'(\Theta)$:
\begin{align}\label{eq:loglikelihood}
\tL_{\tY}(\tC,\mM_1,\ldots,\mM_K)&=\langle \tY, \Theta \rangle - \sum_{i_1,\ldots,i_K} b(\theta_{i_1,\ldots,i_K}),\notag \\
 \text{where}\quad \Theta&=\tC\times\{\mX_1\mM_1,\ldots,\mX_K\mM_K\}.
\end{align}
 We propose the constrained maximum quasi-likelihood estimate (MLE),
\begin{align} \label{eq:MLE} 
(\hat \tC_{\text{MLE}}, \hat \mM_{1,\text{MLE}},\ldots,\hat \mM_{K,\text{MLE}}) =\argmax_{(\tC,\mM_1,\ldots,\mM_K)\in \tP(\mr)} \ \tL_{\tY}(\tC,\mM_1,\ldots,\mM_K),
\end{align}
where the parameter space $\tP(\mr)$ is defined by
\begin{equation}\label{eq:p}
\tP(\mr)=\left\{(\tC, \mM_1,\ldots,\mM_K) \ \Big| \ \mM_k\in\mathbb{O}(p_k,r_k)\ \text{for all }k\in[K],\  \mnormSize{}{\Theta}\leq \alpha \right\},
\end{equation}
with a large constant $\alpha>0$. Recall that $\tB=\tC\times\{\mX_1,\ldots,\mM_K\}$ by definition. Correspondingly, we estimate the coefficient tensor $\tB$ by
\[
\hat \tB_{\text{MLE}}=\hat \tC_{\text{MLE}}\times\{\hat \mM_{1,\text{MLE}},\ldots,\hat \mM_{K,\text{MLE}}\}.
\] 

The maximum norm constraint on the linear predictor $\Theta$ is a technical condition to ensures the existence (boundedness) of MLE. 
{\color{blue}The condition precludes the ill-defined MLE when the optimizer of~\eqref{eq:MLE} diverges to $\pm \infty$; this phenomenon may happen in logistic regression when the Bernoulli responses $\{0,1\}$ are perfectly separable by covariates~\citep{wang2020learning}. For Gaussian models, no maximum norm constraint is needed. In Section~\ref{subsec:statprob}, we show that setting $\alpha$ to an extremely large constant does not compromise the statistical rate in quantities of interest. In practice, the unbounded search is often indistinguishable from the bounded search, since the boundary constraint $\onorm{\Theta}_{\infty} \leq \alpha$ would likely never be active. Similar techniques are commonly used in high-dimensional non-Gaussian problems~\citep{wang2020learning,han2020optimal}.}



The optimization~\eqref{eq:MLE} is a non-convex problem with possibly local optimizers. We propose an alternating optimization algorithm to {\it approximately} solve~\eqref{eq:MLE}. 
The decision variables in the objective function~\eqref{eq:MLE} consist of $K+1$ blocks of variables, one for the core tensor $\tC$ and $K$ for the factor matrices $\mM_k$. We notice that, if any $K$ out of the $K+1$ blocks of variables are known, then the optimization reduces to a simple GLM with respect to the last block of variables. This observation leads to an iterative updating scheme for one block at a time while keeping others fixed. Given an initialization $(\hat\tC^{(0)},\hat\mM_1^{(0)}, \ldots, \hat\mM_K^{(0)})$ to be described in the next paragraph, the $t$-th iterate from the algorithm is denoted $(\hat\tC^{(t)},  \hat\mM^{(t)}_1,\ldots,\hat\mM^{(t)}_K)$ for $t=1, 2,3, \ldots$ The iteration scheme is detailed in Algorithm~\ref{alg:B}. 
\begin{algorithm}[!h]
\caption{Supervised Tensor Decomposition with Side Information}\label{alg:B}
\begin{algorithmic}[1]
\INPUT Response tensor $\tY\in \mathbb{R}^{d_1\times \cdots \times d_K}$, feature matrices $\mX_k\in\mathbb{R}^{d_k\times p_k}$ for $k=1,\ldots,K$, target rank $\mr=(r_1,\ldots,r_K)$, link function $f$, initialization $(\hat\tC^{(0)},\hat \mM^{(0)}_1,\ldots,\hat\mM^{(0)}_K)$.
\For {$t=1,2,3,\ldots$}
\For { $k=1$ to $K$}
\State Obtain the factor matrix $\hat\mM^{(t)}_k\in\mathbb{R}^{p_k\times r_k}$ by a GLM with link function $f$.
\State Perform QR factorization $\hat\mM^{(t)}_k=\mQ_k\mR_k$, where $\mQ_k\in\mathbb{O}(p_k, r_k)$.
\State Update $\hat\mM^{(t)}_k\leftarrow \mQ_k$ and core tensor $\hat\tC^{(t)}\leftarrow \hat\tC^{(t)}\times_k \mR_k$.
\EndFor
\State Update the core tensor $\tC$ by solving a GLM with $\textup{vec}(\tY)$ as response, $\otimes_{k=1}^K[ \mX_k\mM_k]$ as features, and $f$ as link function. Here $\otimes$ denotes the Kronecker product of matrices. 
\EndFor
\OUTPUT factor estimate $(\hat\tC^{(t)}, \hat\mM^{(t)}_1,\ldots,\hat\mM^{(t)}_K)$ from the $t$-th iterate, and coefficient tensor estimate $\hat\tB^{(t)}=\hat\tC^{(t)}\times\{\hat\mM^{(t)}_1,\ldots, \hat\mM^{(t)}_K\}$.
\end{algorithmic}
\end{algorithm}

{\color{blue}We provide two initialization schemes, one with QR-adjusted spectral initialization (warm initialization), and the other with random initialization (cold initialization). The warm initialization is an extension of unsupervised spectral initialization~\citep{zhang2018tensor} to supervised setting with multiple feature matrices.} Specifically, we project normalized data tensor $\bar \tY$ to the normalized multilinear feature space and obtain an unconstrained coefficient tensor $\hat\tB^{(0)}$. We perform a rank-$\mr$ higher-order SVD (HOSVD) on $\bar \tB$, which yields the rank-constrained $\hat\tB^{(0)}$. The desired initialization is obtained by re-normalizing $\hat\tB^{(0)}$ back to the original scales of features. The initialization scheme is described in Algorithm~\ref{alg:A}. 


{\color{blue}The warm initialization enjoys provable accuracy guarantees at a cost of extra technical assumptions (see Section~\ref{subsec:statprob}). The cold initialization, on the other hand, shows robust in practice but its theoretical guarantee remains an open challenge~\citep{luo2021low}. We incorporate both options in our software package to provide flexibility to practitioners.}


\begin{algorithm}[!h]
\caption{QR-adjusted spectral initialization}\label{alg:A}
\begin{algorithmic}[1]
\INPUT Response tensor $\tY\in \mathbb{R}^{d_1\times \cdots \times d_K}$, feature matrices $\mX_k\in\mathbb{R}^{d_k\times p_k}$, Tucker rank $\mr$. 
\State Normalize date tensor $\bar \tY\leftarrow \tY$ for Gaussian model, $\bar \tY\leftarrow 2\tY-1$ for Bernoulli model, and $\bar \tY\leftarrow \log(\tY+0.5)$ for Poisson model. 
\State Normalize feature matrices via QR factorization $\mX_k=\mQ_k\mR_k$ for all $k\in[K]$.
\State Obtain $\bar \tB \leftarrow \bar \tY\times\{\mQ^T_1,\ldots,\mQ^T_K\}$ by projecting $\bar \tY$ to the multilinear feature space. 
\State Obtain $\hat\tB^{(0)}\leftarrow \textup{HOSVD}(\bar \tB,\mr)$.
\State Normalize representation $\{\hat\tC^{(0)}, \hat\mM^{(0)}_1,\ldots,\hat\mM^{(0)}_K\}$ such that $\hat\tC^{(0)}\times\{ \hat\mM^{(0)}_1,\ldots,\hat\mM^{(1)}_K\} = \hat\tB^{(0)}\times\{ \mR^{-1}_1,\ldots,\mR^{-1}_K\}$ and $\hat\mM^{(0)}_k\in\mathbb{O}(p,r)$ for all $k\in[K]$.
\OUTPUT Core tensor $\hat\tC^{(0)}$ and factors $\hat\mM^{(0)}_k$ for all $k\in[K]$.
\end{algorithmic}
\end{algorithm}

\subsection{Statistical accuracy}\label{subsec:statprob}
{\color{blue}This section presents the accuracy guarantees for both global and local optimizers of~\eqref{eq:MLE}. We first provide the statistical accuracy for the global MLE~\eqref{eq:MLE}. Then, we provide the convergence rate for the local optimizer from Algorithm~\ref{alg:B} with warm initialization. The rate reveals an interesting interplay between statistical and computational efficiency. We show that a polynomial number of iterations suffices to reach the desired accuracy under certain assumptions.} The empirical performance for cold initialization is also investigated. 

For cleaner exposition, we present the results for balanced setting in this section, i.e., $p_1=\cdots=p_K=p$, $r_1=\cdots=r_K=r$, and $d_1=\cdots=d_K=d$. The general setting follows exactly the same framework and incurs only notational complexity. We are particularly interested in the high-dimensional regime in which both $d$ and $p$ grows while $p\leq d$. {\color{blue}The requirement $p\leq d$ is necessary to ensure rank non-deficiency of  feature matrices $\mX_k$.} The classical MLE theory is not directly applicable, because the number of unknown parameters grows with the size of data tensor. We leverage the recent development in random tensor theory and high-dimensional statistics to establish the error bounds of the estimation. 


\begin{assumption}\label{ass}We make the following assumptions:
\begin{enumerate}[noitemsep,topsep=0pt]
\item [A1.] There exist two positive constants $c_1, c_2>0$ such that $c_1\leq \sigma_{\min}(\mX_k)\leq  \sigma_{\max}(\mX_k)\leq c_2$ for all $k\in[K]$. Here $\sigma_{\min}(\cdot)$ and $\sigma_{\max}(\cdot)$ denote the smallest and largest matrix singular values.
\item [A1'.] The feature matrices $\mX_k$ are Gaussian designs with i.i.d.\ $N(0,1)$ entries.
\item [A2.] There exist two positive constants $L, U>0$, such that $\min_{|\theta|\leq \alpha}b''(\theta)\geq \phi L$ and $\sup_{\theta\in \mathbb{R}}b''(\theta)\leq \phi U$. Here, $\alpha$ is the upper bound of the linear predictor in~\eqref{eq:MLE}, and $b''(\cdot)$ denotes the second-order derivative.
\end{enumerate}
\end{assumption}
The assumptions are fairly mild. Assumptions A1 and A1' consider two separate scenarios about feature matrices. Assumption A1 is applicable when feature matrix is asymptotically non-singular and has bounded spectral norm, whereas Assumption A1' imposes the commonly-used Gaussian design~\citep{raskutti2019convex}.  The Assumption 2 is essentially imposed to the response variance because of the identity $\text{Var}(y|\theta)=\phi b''(\theta)$~\citep{mccullagh1989generalized}. The lower bound  ensures the non-degeneracy of the variance in the feasible domain of $\theta$, whereas the upper bound ensures the finiteness of the variance in the entire family. In fact, except for Poisson responses, most members in the exponential family, e.g., Gaussian, Bernoulli, and binomial responses, satisfy this condition. 


\subsubsection{Statistical accuracy for global optimizers}\label{sec:global}
We need some extra notation to state the results in full generality. Recall that the factor matrices $\mM_k$ are identifiable only up to orthogonal rotations. Therefore, we choose to use angle distance to assess the estimation accuracy of $\mM_k$. For any two column-orthonormal matrices $\mA,\mB\in\mathbb{O}(d,r)$ of same dimension, the angle distance is defined as
\[
\sin \Theta(\mA,\mB)=\max\left\{ \frac{\langle \mx, \my\rangle}{\vnormSize{}{\mx}\vnormSize{}{\my}}\colon \ \mx\in \textup{Span}(\mA),\ \my\in \textup{Span}(\mB^{\perp})\right\},
\]
where $\text{Span}(\cdot)$ represents the column space of the matrix. We use the superscript ``true'' to denote the true parameters from generic decision variables in optimization. For instance, $\trueB$ denotes the true coefficient tensor, whereas $\tB$ denotes a decision variable in~\eqref{eq:loglikelihood}. 

Define the signal level $\lambda$ as the minimal singular value of the unfolded matrices obtained from $\trueB$,  
\[
\lambda=\min_{k\in[K]}\sigma_{\min}(\text{Unfold}_k(\tB_{\text{true}})).
\]
Intuitively, $\lambda$ quantifies the level of rank non-degeneracy for the true coefficient tensor $\trueB$. 

\begin{thm}[Statistical rate for global optimizers]\label{thm:MLE}Consider generalized tensor models with multiple feature matrices. Under Assumptions A1 and A2 with scaled feature matrices $\bar \mX_k= \sqrt{d}\mX_k$, or Assumptions A1' and A2 with original feature matrices, we have
\begin{equation}\label{eq:bound}
\max_{k\in[K]}\sin^2\Theta(\trueM, \hat \mM_{k,\text{MLE}})\lesssim {\phi (r^K+Kpr)\over \lambda^2 d^K}, \quad \FnormSize{}{\tB_{\text{true}}-\hat \tB_{\text{MLE}} }^2\lesssim {\phi (r^K+Kpr) \over d^K},
\end{equation}
with probability at least $1-\exp(-p)$.
\end{thm}
Theorem~\ref{thm:MLE} establishes the statistical convergence for the global MLE~\eqref{eq:MLE}. The result in~\eqref{eq:bound} implies that the estimation has a convergence rate $\tO(Kp/d^{K})$ as $(p,d)\to \infty$. {\color{blue}This rate agrees with intuition, since in our setting, the number of parameters with $K$ feature matrices is of order $\tO(Kp)$, whereas the number of tensor entries $\tO(d^K)$ corresponds to the total sample size. Because $p\leq d$, our rate is faster than $\tO(d^{-(K-1)})$ obtained by tensor decomposition without features~\citep{wang2020learning}.} 

Inspection of our proof (Supplementary Notes) shows that the desired convergence rate holds not only for the MLE, but also for all local optimizers satisfying $\tL_{\tY}(\tC, \mM_1,\ldots,\mM_K)\geq \tL_{\tY} (\tC_{\textup{true}},\mM_{1,\textup{true}},\ldots,\mM_{K,\textup{true}})$. The observation indicates the global optimality is not necessarily a serious concern in our context, as long as the convergent objective is large enough. In next section, we will provide the statistical accuracy for \emph{local} optimizer with provable convergence guarantee, at a cost of extra signal requirement. 



\subsubsection{Empirical accuracy for local optimizers}\label{sec:local}

The optimization~\eqref{eq:MLE} is a non-convex problem due to the low-rank constraint in the feasible set $\tP$. {\color{blue}Under mild conditions, our warm initialization enjoys stable performance, and the subsequent iterations further improve the accuracy via linear convergence; i.e.\ sequence of iterates generated by Algorithm~\ref{alg:B} converges to optimal solutions at a linear rate. 

\begin{prop}[Polynomial-time angle estimation]\label{lem:ini} Consider Gaussian tensor models with $b(\theta)=\theta^2/2$ in the objective function~\eqref{eq:loglikelihood}. Suppose the signal-to-noise ratio $\lambda^2/\phi \geq C p^{K/2}d^{-K}$ for some sufficiently large universal constant $C>0$. 
Under Assumption A1 with scaled feature matrices $\bar \mX_k=\sqrt{d}\mX_k$, or Assumption A1' with original feature matrices, the outputs from initialization Algorithm~\ref{alg:A} and iteration Algorithm~\ref{alg:B} satisfy the following two properties.
\begin{enumerate}[label=(\alph*)]
\item With probability at least $1-\exp(-p)$. 
\begin{equation}\label{eq:warm}
\max_{k\in[K]}\sin^2\Theta(\trueM,\hat \mM_k^{(0)})\leq {1\over 4}. 
\end{equation}
\item Let $t=1,2,3,\ldots,$ denote the iteration. There exists a contraction parameter $\rho\in(0,1)$, such that, with probability at least $1-\exp(-p)$, 
\begin{equation}\label{eq:local}
\max_{k\in[K]}\textup{sin}^2 \Theta(\trueM, \hat \mM^{(t)}_k) \lesssim \KeepStyleUnderBrace{{\phi p \over \lambda^2 d^K}}_{\text{statistical error}}+\KeepStyleUnderBrace{\rho^{t}\max_{k\in[K]}\textup{sin} \Theta^2(\trueM, \hat \mM_k^{(0)})}_{\text{algorithmic error}}.
\end{equation}
\end{enumerate}
\end{prop}

Proposition~\ref{lem:ini} provides the estimation errors for algorithm outputs at initialization and at each of the subsequent iterations. The initialization bound~\eqref{eq:warm} demonstrates the stability of warm initialization under a mild SNR requirement $\lambda^2/\phi \gtrsim p^{K/2}d^{-K}$. We can think of $d$ as the sample size while $p$ the number of parameters at mode $K$. This threshold is less stringent than $d^{K/2}$ required for unsupervised tensor decomposition features~\citep{han2020optimal,zhang2018tensor}. The condition confirms that a higher sample size mitigates the required signal level. The iteration bound~\eqref{eq:local} consists of two terms: the first term is the statistical error, and the second is the algorithmic error. The algorithmic error decays exponentially with the number of iterations, whereas the statistical error remains the same as $t$ grows. The statistical error is unavoidable and also appears in the global MLE; see Theorem~\ref{thm:MLE}.

As a direct consequence, we find the optimal iteration $t$ after which the algorithmic error is negligible compared to statistical error. 
\begin{thm}[Statistical rate for local optimizers]\label{thm:local}Consider the same condition as in Proposition~\ref{lem:ini} and the outputs by combining algorithms 1 and 2. There exists a constant $C>0$, such that, after $t\gtrsim K\log_{1/\rho}p$ iterations, our algorithm outputs satisfies
\[
\max_{k\in[K]}\textup{sin}^2 \Theta(\trueM, \hat \mM^{(t)}_k) \lesssim {\phi p\over \lambda^2d^K}, \quad \FnormSize{}{\trueB- \hat \tB^{(t)}}^2 \lesssim {\phi(r^K+Kpr)\over d^K}.
\]
\end{thm}
}


In practice, the signal level $\lambda$ is unknown, so the assumption in Theorem~\ref{thm:local} is challenging to verify in practice. We supply the theory by providing an alternative scheme -- random initialization -- and investigate its empirical performance. Figure~\ref{fig:loglike} shows the trajectories of objective function for order-3 tensors based on model~\eqref{eq:decomp}, where $ d \in\{25,30\}$, $p = 0.4d$, $ r\in\{3,6\}$ at all three modes. We consider data tensors with Gaussian, Bernoulli, and Poisson entries. Under all combinations of the dimension $d$, rank $r$, and type of the entries, Algorithm~\ref{alg:B} converges quickly in a few iterations upon random initialization, and the objective values at convergent points are close to or larger than the value at true parameters. In the experiment we conduct, we find little difference in the final estimation errors between the two initialization schemes. Random initialization appears good enough for Algorithm~\ref{alg:B} to find a convergent point with desired statistical guarantees. In practice, we recommend to run both warm and cold initializations, and choose the one with better convergent objective values.  


\begin{figure}[t]
\centering
\includegraphics[width=16cm]{loglike.pdf}
\caption{Trajectory of the objective function with various dimension $d$ and rank $r$ under (a) Gaussian (b) Bernoulli (c) Poisson models. The dashed line represents the objective value at true parameters. }\label{fig:loglike}
\vspace{-.2cm}
\end{figure}


We conclude this section by revisiting the three examples mentioned in Section~\ref{sec:model}.

\begin{customexample}{1}[Spatio-temporal growth model] The estimated type-by-time-by-space coefficient tensor converges at the rate $\tO\left((p+r+s)/(dmn)\right)$ with $(p,r,s)\leq (d,m,n)$. The estimation achieves consistency as the dimension grows along either of the three modes.
\end{customexample}

\begin{customexample}{2} [Network population model] The estimated node-by-node-by-feature tensor converges at the rate $\tO\left((2d+p)/(d^2n)\right)$ with $p\leq n$. The estimation achieves consistency as the number of individuals or the number of nodes grows. 
\end{customexample}

\begin{customexample}{3} [Dyadic data with node attributes] The estimated feature-by-feature matrix converges at the rate $\tO\left(p/d^2\right)$ with $p\leq d$. Again, our estimation achieves consistency as the number of nodes grows. 
\end{customexample}


\subsection{Rank selection and computational complexity}\label{sec:tuning}
Our algorithm assumes the rank $\mr$ is given. In practice, the rank is often unknown and must be determined from the data. We propose to use Bayesian information criterion (BIC) and choose the rank that minimizes BIC, where
\begin{equation}\label{eq:BIC}
\textup{BIC}(\mr)=-2\tL_{\tY}(\hat \tC, \hat \mM_1,\ldots,\hat \mM_K)+p_e(\mr)\log (\prod\nolimits_k d_k).
\end{equation}
Here, $p_e(\mr)\stackrel{\textup{def}}{=}\sum_k (p_k-r_k)r_k+\prod_k r_k$ is the effective number of parameters in the model. We choose $\hat \mr$ that minimizes $\textup{BIC}(\mr)$ via grid search. Our choice of BIC aims to balance between the goodness-of-fit for the data and the degree of freedom in the population model. We evaluate the empirical performance of BIC in Section~\ref{sec:simulation}.  

The computational complexity of our Algorithm is $\tO\left(d \sum_k p^3_k\right)$ for each iteration, where $d=\prod_k d_k$ is the total size of the data tensor. The update of $K$ factor matrices is $\tO(d\sum_k r^3_k p_k^3)$ via standard GLM routines. Furthermore, we demonstrate that, under certain SNR conditions, a polynomial number of iterations suffices to reach the desired statistical accuracy. Therefore, the total computational cost is polynomial in $p$ and $d$.


\section{Connection to other tensor regression methods}\label{sec:connection}
{\color{blue}We compare our supervised tensor decomposition (\textbf{STD}) with recent 12 tensor methods in the literature. Table~\ref{table:comp_table} summarizes these methods with their properties from four aspects: i) model specification, ii) number of feature matrices allowed, (iii) capability of addressing non-Gaussian response, and (iv) capability of addressing non-independent noise. The four closet methods to our are {\bf SupCP}~\citep{lock2018supervised}, {\bf Envelope}~\citep{li2017parsimonious}, {\bf mRRR}~\citep{luo2018leveraging} and {\bf GLSNet}~\citep{zhang2018network}; these methods all relate a data tensor to feature matrices with low-rank structure on the coefficients. As seen from the table, our method is the only one that allows multiple feature matrices among the five. {\bf Envelope} and {\bf SupCP} are developed for Gaussian data, and the Gaussianity facilities flexible extension to non-independent noise. In particular, {\bf Envelope} allows noise correlation in Kronecker structured form, whereas {\bf SupCP} allows noise correlation implicitly through decomposing the latent factors into fixed effects (related to features) and random effects (unrelated to features). On the other hand, the other three methods ({\bf mRRR}, {\bf GLSNet} and {\bf STD}) are developed for exponential family distribution with possibly non-additive noise. The generality makes the full modeling of noise correlation computationally challenging. We will compare the numerical performance of these methods in Section~\ref{sec:simulation}.

\afterpage{
\begin{landscape}
 \renewcommand{\arraystretch}{1.5} % Default value: 1
\begin{table}[h!]
\centering
\color{blue}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c||c|ccc}
\hline
Method & Model& No.\ of features & non-Gaussianity & Non-independence \\
\hline
	STD (Ours) & $\bbE\tY = f(\tB \times \{\mX_1,\mX_2,\mX_3\}),\ \tB = \tC \times \{\mM_1,\mM_2,\mM_3\}$ &$3$& $\surd$& $\times$\\
	GCP, CP-ARP, CORALS & $ \bbE\tY = f(\entry{ \mA_1,\mA_2,\mA_3})$ &0& $\surd$& $\times$\\
         DCOT & $\bbE\tY = f((\tC_1+\tC_2) \times \{\mM_1, \mM_2, \mM_3 \})$ &0& $\surd$ & $\times$\\
         LRT, CRT & $y_{n} = \langle \tB,\ \tX_{n} \rangle + \epsilon_{n}$, various structure on $\tB$&0& $\times$& $\times$\\
	STAR & $y_n = \sum_m \langle \tB_m, \tF_m(\tX_{ijk})\rangle + \epsilon_n$, sparse-CP $\tB_m$ &0& $\times$ & $\times$\\
	SupCP &   $\tY = \entry{ \mA_1,\mA_2,\mA_3} + \tE,\  \mA_1 = \mX \mB +\tE',\ \tE\perp \tE'$ &1& $\times$& $\surd$\\
	mRRR &  $\bbE \mY =  f(\mX \mB)$, low-rank $\mB$ &1& $\surd$& $\times$ \\
	Envelope & $\tY = \tB \times_3 \mX + \tE,\  \tB = \tC \times \{ \mM_1, \mM_2, \mI \},\  \tE \sim \mathcal{TN}(\mSigma_1, \mSigma_2,\mI)$ &1& $\times$ & $\surd$\\
	GLSNet& $\bbE\tY = f({\bf 1} \otimes {\bf \Theta} + \tB \times_3 \mX)$, low-rank ${\bf \Theta}$, sparse $\tB$&1& $\surd$& $\times$\\
	STORE & $\tY = \tB \times_3 \mX + \tE$,  sparse-CP $\tB$ &1& $\times$& $\times$ \\
	\hline
\end{tabular}
}
\caption{\footnotesize Comparison of tensor regression/factorization methods. We focus on order-3 tensors for illustration. Calligraphic letters denote tensors, bold capital letters denote matrices, and little letters denote scalars. The dimension of tensors and matrices can be identified from the contexts.\\
\vspace{-0.1cm}\\
- Data: tensor response $\tY$, feature matrices $\mX,\mX_k$, predictor tensor $\tX_n$, scalar response $y_n$, sample index $n$, tensor mode $k=1,2,3$.  \\
- Parameter: Tuckor factors $\mM_k$, CP factors $\mA_k$, CP decomposition $\entry{\mA_1, \mA_2, \mA_3}$, coefficient tensor and matrix $\tB, \tB_m, {\bf \Theta}, \mB$.\\
- Function: a known link function $f(\cdot)$, a known basis function $\tF_m(\cdot)$. \\
- Noise: Gaussian tensor with i.i.d.\ entries $\tE, \tE'$, Gaussian tensor with Kronecker covariance $\tE \sim \mathcal{TN}(\mSigma_1, \mSigma_2, \mI)$, meaning $\text{Cov}(\text{vec}(\tE))=\mSigma_1\otimes\mSigma_2\otimes \mI$.\\
\vspace{-0.1cm}\\
- GCP: Generalized canonical polyadic tensor decomposition~\citep{hong2020generalized};\\
- CP-APR: CP alternating Poisson regression~\citep{chi2012tensors};\\
- CORALS: Generalized co-clustering method~\citep{li2020generalized};\\
- DCOT: Double core tensor decomposition~\citep{tarzanagh2019regularized};\\
- SupCP: Supervised PARAFAC/CANDECOMP factorization~\citep{lock2018supervised};\\
- mRRR: Mixed-response reduced-rank regression~\citep{luo2018leveraging};\\
- Envelope: Parsimonious tensor response regression~\citep{li2017parsimonious};\\
- GLSNet: Generalized connectivity matrix response regression~\citep{zhang2018network};\\
- STORE: Sparse tensor response regression~\citep{sun2017store};\\
- LTR: Low-rank tensor regression~\citep{han2020optimal};\\
- CRT: Convex regularized multi-response tensor regression~\citep{raskutti2019convex};\\
- STAR: Sparse tensor additive regression~\citep{hao2019sparse}.
}\label{table:comp_table}
\end{table}
\end{landscape}
}
}

Our model also has a close connection to higher-order interaction model~\citep{hao2020sparse} and tensor-to-tensor regression~\citep{lock2018tensor}. Model~\eqref{eq:decomp} can be viewed as a regression model with across-mode interactions in the reduced feature space. We take an order-3 tensor under the Gaussian model for illustration. Let $\mX,\mZ,\mW$ denote the feature matrix on mode $k=1, 2, 3$, respectively. Suppose that each mode has two-dimensional reduced features, denoted $\mM_1\mX=[\mx_1,\mx_2]$, $\mM_2\mZ=[\mz_1,\mz_2]$, $\mM_3\mW=[\mw_1,\mw_2]$. Here $\mx_1,\mx_2,\ldots,\mw_1, \mw_2$ are column vectors. Then the model~\eqref{eq:decomp} is equivalent to a regression model with across-mode interactions
\begin{equation}
\mathbb{E}(y_{ijk}|\mX,\mZ,\mW)=c_{111}\mx_{1i}\mz_{1j}\mw_{1k}+c_{121}\mx_{i1}\mz_{j2}\mw_{k1}+\cdots+c_{221}\mx_{2i}\mz_{2j}\mw_{1k}+c_{222}\mx_{2i}\mz_{2j}\mw_{2k},
\end{equation}
where $\entry{c_{ijk}} \in \mathbb{R}^{2\times 2\times 3}$ are unknown interaction effects, $\mx_{1i}$ denotes the $i$-th entry in the feature vector $\mx_1$, and similar notations apply to other features. Note that lower-order interactions are naturally incorporated if we include an intercept column in the reduced feature matrices. The above example shows the connection of our supervised tensor decomposition to multivariate regressions. 


\section{Numerical experiments}\label{sec:simulation}
We evaluate the empirical performance of our supervised tensor decomposition (\textbf{STD}) through simulations. We consider order-3 tensors with a range of distribution types. Unless otherwise specified, the conditional mean tensor is generated form model~\eqref{eq:decomp}, where the core tensor entries are i.i.d.\ drawn from Uniform[-1,1], the factor matrix $\mM_k$ is uniformly sampled with respect to Haar measure from matrices with orthonormal columns. The feature matrix $\mX_k$ is either an identity matrix (i.e.,\ no feature  available) or Gaussian random matrix with i.i.d.\ entries from $N(0,1)$. The linear predictor $\Theta=\tC\times\{\mM_1\mX_1,\mM_2\mX_2,\mM_3\mX_3\}$ is scaled such that $\mnormSize{}{\Theta}=1$. Conditional on the linear predictor $\Theta=\entry{\theta_{ijk}}$, the entries in the tensor $\tY=\entry{y_{ijk}}$ are drawn independently according to three probabilistic models:

\begin{enumerate}[noitemsep,topsep=0pt]
\item[(a)] Gaussian model: continuous tensor entries $y_{ijk}\sim N\left(\alpha \theta_{ijk}, 1\right)$.
\item[(b)] Poisson model: count tensor entries $y_{ijk}\sim\textup{Poisson}\left(e^{\alpha \theta_{ijk}}\right)$.
\item[(c)] Bernoulli model: binary tensor entries $y_{ijk}\sim \textup{Bernoulli}\left( \frac{e^{\alpha \theta_{ijk}}}{1+e^{\alpha \theta_{ijk}}}\right)$.
\end{enumerate}
Here $\alpha>0$ controls the magnitude of the effect size, which is also the maximum norm of coefficient tensor as in~\eqref{eq:p}.
In each experiment, we report the summary statistics averaged across $30$ simulation replications. 

\subsection{Finite-sample performance}
The first experiment assesses the selection accuracy of our BIC criterion~\eqref{eq:BIC}. We consider the balanced situation where $d_k=d$, $p_k=0.4d_k$ for $k=1,2,3$. We set $\alpha=4$ and consider various combinations of dimension $d$ and rank $\mr=(r_1,r_2,r_3)$. For each combination, we minimize BIC using a grid search from $(r_1-3,r_2-3,r_3-3)$ to $(r_1+3,r_2+3,r_3+3)$. We remove invalid rank such as $r^2_{\max} \geq \prod_{k=1}^3 r_k$ and use parallel search to reduce the computational cost. Table~\ref{tab:rank} reports the selected rank averaged over $n_{\textup{sim}}=30$ replicates. We find that in the high-rank setting with $d=20$, the selected rank slightly underestimates the true rank, and the accuracy immediately improves when either the dimension increases to $d = 40$ or the rank reduces to $\mr = (3,3,3)$. This agrees with our expectation, because in the tensor decomposition, the sample size is related to the number of tensor entries. A larger $d$ implies a larger sample size, so the BIC selection becomes more accurate. 

\begin{table}[tb]
\resizebox{\textwidth}{!}{%
\centering
\begin{tabular}{c|cc|cc}
%True Rank $\mr$ &\multicolumn{2}{c|}{Dimension (Gaussian tensors)}&\multicolumn{2}{c}{Dimension (Poisson tensors)}\\
True rank $\mr$& $d=20$  (Gaussian) &$d=40$ (Gaussian) &$d=20$ (Poisson) &$d=40$ (Poisson)\\
\hline
$(3,\ 3,\ 3)$&$({\bf 3.0},\ {\bf 3.0},\ {\bf 3.0})$&$({\bf 3.0},\ {\bf 3.0},\ {\bf 3.0})$& $({\bf 3.0},\ {\bf 3.0},\ {\bf 3.0})$ & $({\bf 3.0},\ {\bf 3.0},\ {\bf 3.0})$\\
$(4,\ 4,\ 6)$&$(3.0,\ 3.0,\ {\bf 4.6})$&$({\bf 4.0},\ {\bf 4.0},\ {\bf 5.3})$&$(3.0,\ 3.0,\ {\bf 5.3})$&$({\bf 4.0},\ {\bf 4.0},\ {\bf 5.6})$\\
$(6,\ 8,\ 8)$&$(5.0,\ 5.0,\ 5.0)$&$({\bf 6.0},\ {\bf 8.0},\ {\bf 8.0})$&$(5.0,\ 5.0,\ 6.7)$&$({\bf 6.0},\ {\bf 8.0},\ {\bf 8.0})$\\
\end{tabular}
}
\caption{Rank selection via BIC. The estimated ranks are averaged across 30 simulation. Bold number indicates the ground truth is within two standard deviations of the estimate.}\label{tab:rank}
\end{table}

\begin{figure}[!h]
\centering
\includegraphics[width=14cm]{dimension.pdf}
\caption{Estimation error against effective sample size. The three panels plot the MSE when the response tensors are generated from (i) Gaussian (ii) Poisson and (iii) Bernoulli models. The dashed curves correspond to $\tO({1/d^2})$.}\label{fig:dim}
\vspace{-.1cm}
\end{figure}

The second experiment evaluates the accuracy when features are available on all modes. We set $\alpha=10, d_k=d, p_k=0.4d_k, r_k=r\in\{2,4,6\}$ and increase $d$ from 30 to 60. Our theoretical analysis suggests that $\hat \tB$ has a convergence rate $\tO(d^{-2})$ in this setting. Figure~\ref{fig:dim} plots the mean squared error (MSE) $\FnormSize{}{\hat \tB-\tB_{\text{true}}}^2$ versus the effective sample size $d^2$ under three different distribution models. We find that the empirical MSE decreases roughly at the rate of $1/d^2$, which is consistent with our theoretical results. We also observe that, tensors with higher rank tend to yield higher estimation errors, as reflected by the upward shift of the curves as $r$ increases. Indeed, a larger $r$ implies a higher model complexity and thus greater difficulty in the estimation. 





\subsection{Comparison with other tensor methods}
We compare our supervised tensor decomposition with three other tensor methods:

{\color{blue}\begin{itemize}[noitemsep,topsep=0pt]
\item Supervised PARAFAC/CANDECOMP factorization (\textbf{SupCP}, \citep{lock2018supervised}).
    \item Parsimonious tensor response regression (\textbf{Envelope}, \citep{li2017parsimonious});
    \item Mixed-response reduced-rank regression (\textbf{mRRR}, \citep{luo2018leveraging});
    \item Generalized connectivity matrix response regression (\textbf{GLSNet}, \citep{zhang2018network});
\end{itemize}
}

{\color{blue}These four methods are the closest methods to ours, in that they all relate a data tensor to feature matrices with low-rank structure on the coefficients.} We consider Gaussian and Bernoulli tensors in experiments. For methods not applicable for Bernoulli data ({\bf SupCP} and {\bf Envelope}), we provide the algorithm $\{-1,1\}$-valued tensors as inputs. Because {\bf mRRR} allows matrix response only, we provide the algorithm the unfolded matrix of response tensor as inputs. We measure the accuracy using the response error defined as $1-\text{Cor}(\hat \tY, f(\trueT))$, where $\hat \tY$ is the fitted tensor from each method, and $f(\trueT)$ is the true conditional mean of the tensor. Note that the response error is a scale-insensitive metric; a smaller error implies a better fit of the model.


{\color{blue}The comparison is assessed from three aspects: (i) benefit of incorporating features from multiple modes; (ii) prediction error with respect to sample size; (iii) robustness of model misspecification.} We use similar simulation setups as in our first experiment in last section. We consider rank $\mr=(3,3,3)$ (low) vs.\ $(4,5,6)$ (high), effect size $\alpha = 3$ (low) vs.\ $6$ (high), dimension $d$ ranging from 20 to 100 for modes with features, and $d = 20$ for modes without features. The method \textbf{Envelope} and \textbf{mRRR} require the tensor rank as inputs, respectively. For fairness, we provide both algorithms the true rank. The methods \textbf{SupCP} and \textbf{GLSNet} use different notions of model rank, and \textbf{GLSNet} takes sparsity as an input. We use a grid search to set the hyperparameters in \textbf{SupCP} and \textbf{GLSNet} that give the best performance.

\begin{figure}[t]
\centering
\includegraphics[width=12cm]{comp_final_normal.pdf} 
\caption{Comparison between tensor methods with Gaussian data. Panels (a) and (b) plot estimation error versus the number of modes with available features. Panels (c) and (d) plot ME versus the effective sample size $d^2$.
We consider rank $\mr=(3,3,3)$ (low), $\mr=(4,5,6)$ (high), and effect size $\alpha =3 $ (low), $\alpha=6$ (high).}~\label{fig:comp}
\vspace{-.5cm}
\end{figure}



{\color{blue}Figure~\ref{fig:comp}a-b shows the impact of features to estimation error. We see that our \textbf{STD} outperforms others, especially in the low-effect high-rank setting. As the number of informative modes increases, the \textbf{STD} exhibits a reduction in error whereas others remain unchanged. The accuracy gain in Figure~\ref{fig:comp} demonstrates the benefit of incorporating informative features from multiple modes. In addition, we find that the relative performance among the competing methods reveals the benefits of low-rankness. 
The second best method is \textbf{SupCP} which imposes low-rankness on three modes; the next one is \textbf{Envelope} which imposes low-rankness on two modes; the less favorable one is \textbf{mRRR} which imposes low-rank structure on one mode only; the worst one is \textbf{GLSNet} which imposes sparsity but no low-rankness on the feature effects. 

Figure~\ref{fig:comp}c-d compares the prediction error with respect to effective sample size $d^2$. For fair comparison, we consider the setting with feature matrix on one mode only. We find that our \textbf{STD} method has similar performance as \textbf{Envelope} and \textbf{SupCP} in the high-effect low-rank regime, whereas the improvement becomes more pronounced in the low-effect high-rank regime. The latter setting is notably harder, and our \textbf{STD} method shows advantage in addressing this challenge. Among other methods, \textbf{Envelope}, \textbf{SupCP}, and \textbf{mRRR} show decreasing errors as $d$ increases, implying the benefits of low-rankness methods. In contrast, \textbf{GLSNet} suffers from non-decreasing error and indicates the poor fit of sparsity methods in addressing low-rank data. 

\begin{figure}[t]
\centering
\includegraphics[width=12cm]{comp_final_binary.pdf} 
\caption{Comparison between tensor methods with Binary data. The panel legends are the same as in Figure~\ref{fig:comp}.}~\label{fig:comp_b}
\vspace{-.5cm}
\end{figure}

We also evaluate the performance comparison with Bernoulli tensors. Figure~\ref{fig:comp_b} indicates the necessarity of generalized model in addressing non-Gaussian data. Indeed, methods that assume Gaussiannity (\textbf{Envelope} and \textbf{SucCP}) perform less favorably in Bernoulli setting (Figure~\ref{fig:comp_b}c) compared to Gaussian setting (Figure~\ref{fig:comp}c). Our method shows improved accuracy as the number of informative features increases (Figure~\ref{fig:comp_b}a-b). In the absence of multiple features, our method still performs favorably compared to others (Figure~\ref{fig:comp_b}c-d), for the same reasons we have argued in Gaussian data. 

\begin{figure}[t]
\centering
\includegraphics[width=12cm]{noniid_final.pdf} 
\caption{Comparison between tensor methods under model misspecification. Panels (a)-(b) assess the noise correlation, and panels (d)-(d) assess the sparsity. }~\label{fig:noniid}
\vspace{-.5cm}
 \end{figure}


Lastly, we assess the performance of our method {\bf STD} under model misspecification. We consider two aspects: (i) non-independent noise, and (ii) sparse feature effects. Note that our method {\bf STD} imposes neither of these two assumptions, so the experiment allows us to assess the robustness. We select competing methods from Table~\ref{table:comp_table} that specifically addresses these two aspects. We use {\bf Envelope} and {\bf SupCP} as benchmark for noise correlation experiment, and {\bf GLSNet} for sparsity experiment.

Figure~\ref{fig:noniid}a-b assesses the impact of noise correlation to the estimation accuracy. The data is simulated from {\bf Envelope} model with envelope dimensions $r=(3,3)$ (low) and $(4,5)$ (high). The noise is generated from a zero-mean Gaussian tensor with Kronecker structured covariance; see Supplementary Notes for details. As expected, {\bf Envelope} shows the best performance in the high correlation setting. Remarkably, we find that our method {\bf STD} has comparable and sometimes better performance when noise correlation is moderate-to-low. In contrast, {\bf SupCP} appears less suitable in this setting. Although {\bf SupCP} allows noise correlation implicitly through latent random factors, the induced correlation may not belong to the Kronecker covariance structure in the simulation. 



Figure~\ref{fig:noniid}c-d assesses the impact of sparsity to estimation performance. We generate data from {\bf GLSNet} model, except that we modify the coefficient tensor to be joint sparse and low-rank (the original {\bf GLSNet} model assumes full-rankness on the coefficient tensor). 
The sparsity level ($x$-axis in Figure~\ref{fig:noniid}c-d) quantifies the proportion of zero entries in the coefficient tensor. Since neither our method {\bf STD} nor {\bf GLSNet} follow the simulated model, this setting allows a fair comparison. We find that our method outperforms {\bf GLSNet} in the low-rank setting, whereas {\bf GLSNet} shows a better performance in the high-rank setting. This observation suggests the robustness of our method to sparsity when the tensor of interest is simultaneously low-rank and sparse. When sparsity is the only salient structure, then methods specifically addressing sparsity would provide a better fit. }




\section{Data applications}\label{sec:data}
We apply our supervised tensor decomposition to two datasets. The first application studies the brain networks in response to individual attributes (i.e.,\ feature on one mode), and the second application focuses on multi-relational network analysis with dyadic attributes (i.e.,\ features on two modes). 

\subsection{Application to human brain connection data} 

The Human Connectome Project (HCP) aims to build a network map that characterizes the anatomical and functional connectivity within healthy human brains~\citep{van2013wu}. We follow the preprocessing procedure as in~\citep{desikan2006automated} and parcellate the brain into 68 regions of interest. The dataset consists of 136 brain structural networks, one for each individual. Each brain network is represented as a 68-by-68 binary matrix, where the entries encode the presence or absence of fiber connections between the 68 brain regions. We consider four individual features: gender (65 females vs.\ 71 males), age 22-25 ($n=35$), age 26-30 ($n=58$), and age 31+ ($n=43$). The preprocessed dataset is released in our R package \texttt{tensorregress}. The goal is to identify the connection edges that are affected by individual features. A key challenge in brain network is that the edges are correlated; for example, the nodes in edges may be from a same brain region, and it is of importance to take into account the within-dyad dependence. 

\begin{figure}[!h]
\centering
\includegraphics[width=16cm]{HCP.pdf}
\caption{Top edges with large effects. (a) Global effect; (b) Female effect; (c) Age 22-25; (d) Age 31+. Red edges represent positive effects and blue edges represent negative effects. The edge-width is proportional to the magnitude of the effect size.
}\label{fig:brain}\label{fig:s1}
\vspace{-.5cm}
\end{figure}


We perform the supervised tensor decomposition to the HCP data. %The data tensor is binary, $\tY\in\{0,1\}^{68\times 68\times 136}$, and the features are of dimension 4 on the 3$^{\text{rd}}$ mode. 
The BIC selection suggests a rank $\mr=(10,10,4)$ with quasi log-likelihood $\tL_{\tY}=-174654.7$. We utilize the sum-to-zero contrasts in coding the feature effects, and depict only the top 3\% edges whose connections are non-constant across the sample. Figure~\ref{fig:brain} shows the top edges with high effect size, overlaid on the Desikan atlas brain template~\citep{desikan2006automated}. We find that the global connection exhibits clear spatial separation, and that the nodes within each hemisphere are more densely connected with each other (Figure~\ref{fig:brain}a). In particular, the superior-temproal (\emph{SupT}), middle-temporal (\emph{MT}) and Insula are the top three popular nodes in the network. Interestingly, female brains display higher inter-hemispheric connectivity, especially in the frontal, parental and temporal lobes (Figure~\ref{fig:brain}b). This is in agreement with a recent study showing that female brains are optimized for inter-hemispheric communication~\citep{ingalhalikar2014sex}. We find several edges with declined connection in the group Age 31+. Those edges involve Frontal-pole (\emph{Fploe}), superior-frontal (\emph{SupF}) and Cuneus nodes. The Frontal-pole region is known for its importance in memory and cognition, and the detected decline with age further highlights its biological importance. 

\subsection{Application to political relation data}

The second application studies the multi-relational networks with node-level attributes. We consider \emph{Nations} dataset~\citep{nickel2011three} which records 56 relations among 14 countries between 1950 and 1965. The multi-relational networks can be organized into a $14 \times 14 \times 56$ binary tensor, with each entry indicating the presence or absence of an action, such as ``sending tourist to'', ``export'', ``import'', between countries. The 56 relations span the fields of politics, economics, military, religion, etc. In addition, country-level attributes are also available, and we focus on the following six features: \emph{constitutional, catholics, law ngo, political leadership, geography}, and \emph{medicine ngo}. The goal is to identify the variation in connections due to country-level attributes and their interactions. 

We apply our tensor model to the \emph{Nations} data. The multi-relational network $\tY$ is a binary data tensor, and the country attributes $\mX\in\mathbb{R}^{14\times 6}$ are features on both the 1$^\text{st}$ and 2$^\text{nd}$ modes. {\color{blue} We use BIC as guidance to select the rank of coefficient tensor $\tB$. Since several rank configurations give similar BIC values, we present here the most interpretable results with $\mr=(4,4,4)$. Detailed rank selection procedure is in Supplementary Notes.}
We perform the supervised tensor decomposition and obtain the dimension reduction matrices $\hat \mM_k$ from the model. Then we apply $K$-mean clustering to dimension reduction matrix on each of the modes. Supplementary Table~\ref{fig:s1} shows the K-means clustering of the 56 relations based on the dimension reduction matrix on the 3$^\text{rd}$ mode. We find that the relations reflecting the similar aspects of actions are grouped together. In particular, Cluster I consists of military relations such as \emph{violentactions}, \emph{warnings} and \emph{militaryactions}; Clusters II and III capture the economic relations such as \emph{economicaid, booktranslations, tourism}; and Cluster IV represents the political alliance and territory relations. 

\begin{figure}[!h]
\centering
\includegraphics[width=16cm]{coef.pdf}
\caption{Estimated feature effects in the \emph{Nations} data analysis. Panels (a)-(d) represent the estimated effects of country-level attributes towards the connection probability, for relations \emph{warning}, \emph{violentactions}, \emph{treaties}, and \emph{aidenemy}, respectively. }\label{fig:est}
\end{figure}

To investigate the effects of dyadic attributes towards connections, we depict the estimated coefficients $\hat \tB=\entry{\hat b_{ijk}}$ for several relation types (Figure~\ref{fig:est}). {\color{blue}The entry $\hat b_{ijk}$ estimates the contribution, at the logit scale, of feature pair $(i,j)$ ($i$th feature for the ``sender'' country and $j$th feature for the ``receiver'' country) towards the connection of relation $k$.} Several interesting findings emerge from the observation. We find that relations belonging to a same cluster tend to have similar feature effects. For example, the relations ``warning'' and ''violentactions'' are classified into Cluster I, and both exhibit similar effect patterns (Figure~\ref{fig:est}a-b). Moreover, the feature \emph{constitutional} has a strong effect in the relation ``violentactions'' and ``warning'', whereas the effect is weaker in the relation ``treaties''. The result is plausible because the constitutional attributes affect political actions more often than economical actions. The entries in $\tB$ are useful for revealing interaction effects in a context-specific way. 
%For a given relation $k$, the off-diagonal entries in the matrix $\hat \tB(\colon,\colon,k)$ represent the pairwise interactions between features. 
From Figure~\ref{fig:est}, we find a strong interaction between \emph{geography} and \emph{political leadership} in the relation ``warning'', and a strong interaction between \emph{geogrphy} and \emph{medicine ngo} in the relation ``aidenemy''. The relation-specific effect pattern showcases the applicability of our method in revealing complex interactions. 

\vspace{-0.5cm}

\section{Discussion and future work}\label{sec:con}
We have developed a supervised tensor decomposition method with side information on multiple modes. One important challenge of tensor data analysis is the complex interdependence among tensor entries and between multiple features. Our approach incorporates side information as feature matrices in the conditional mean tensor. The empirical results demonstrate the improved interpretability and accuracy over previous approaches. Applications to the brain connection and political relationship datasets yield conclusions with sensible interpretations, suggesting the practical utility of the proposed approach.
 
There are several possible extensions from the work. We have provided accuracy guarantees for parameter estimation in the supervised tensor model. Statistical inference based on tensor decomposition is an important future direction. Measures of uncertainty, such as confidence envelope for space estimation, would be useful. One possible approach would be performing parametric bootstrap~\citep{efron1994introduction} to assess the uncertainty in the estimation. 
For example, one can simulate tensors from the fitted low-rank model based on the estimates, and then assess the empirical distribution of the estimates. 
While being simple, bootstrap approach is often computationally expensive for large-scale data. Another possibility is to leverage recent development in debiased inference with distributional characterization~\citep{chen2019inference}. This approach has led to fruitful results for matrix data analysis. Uncertainly quantification involving tensors are generally harder, and establishing distribution theory for tensor estimation remains an open problem.

One assumption made by our method is that tensor entries are conditionally independent given the linear predictor $\Theta$. This assumption can be extended by introducing a more general mixed-effect tensor model. For example, in the special case of Gaussian model, we can model the first two moments of data tensor using
\begin{align}\label{eq:extention}
\mathbb{E}(\tY|\mX_1,\ldots,\mX_K)&=\tC\times\mM_1\times \cdots \times \mM_K,\\
 \text{Var}(\tY|\mX_1,\ldots,\mX_K)&=\mPhi_1\otimes \cdots \otimes \mPhi_K,
\end{align}
where $\mPhi_k\in\mathbb{R}^{d_k\times d_k}$ is the unknown covariance matrix on the mode $k\in[K]$. For general exponential family, an additional mean-variance relationship should also be considered. The joint estimation of mean model $\Theta$ and variance model $\mPhi_k$ will lead to more efficient estimation in the presence of unmeasured confounding effects. However, the introduction of unknown covariance matrices $\mPhi_k$ dramatically increases the number of parameters in the problem. {\color{blue}Suitable regularization such as graphical lasso or specially-structured covariance~\citep{li2017parsimonious,lock2018supervised} should be considered.} The extension of tensor modeling with heterogeneous mixed-effects will be an important future direction. 

Although we have presented the data applications in the context of order-3 data tensors, the framework of the supervised tensor decomposition applies to a variety of multi-way datasets. One possible application is the integrative analysis of omics data, in which multiple types of omics measurements (gene expression, DNA methylation, microRNA) are collected in the same set of individuals~\citep{lock2013joint,wang2019three}. Other applications include time-series tensor data with multiple side information. Exploiting the benefits and properties of  supervised tensor decomposition in specialized task will boost scientific discoveries.


\bigskip
\begin{center}
{\large \bf SUPPLEMENTARY MATERIALS}
\end{center}

\begin{description}
\item[Supplementary notes:]  technical proofs, additional simulation, and data analysis results.
\item[Data and software:] 
Our simulation code, R-package \texttt{tensorregress}, and datasets used in the paper are available at \url{https://CRAN.R-project.org/package=tensorregress}
\end{description}

\setstretch{1.25}
\bibliographystyle{apalike}

\bibliography{tensor_wang}



\end{document}
