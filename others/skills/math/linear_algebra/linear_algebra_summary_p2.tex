\documentclass[11pt]{article}

\usepackage{fancybox}


\usepackage{color}
\usepackage{url}
\usepackage[margin=1in]{geometry}

\setlength\parindent{0pt}
\renewcommand{\textfraction}{0.0}
\renewcommand{\topfraction}{1.0}
%\renewcommand{\textfloatsep}{5mm}

\usepackage{comment}
% Definitions of handy macros can go here
\usepackage{amsmath,amssymb,amsthm,bm,mathtools}
\usepackage{multirow}
\usepackage{dsfont,multirow,hyperref,setspace,natbib,enumerate}
%\usepackage{dsfont,multirow,hyperref,setspace,enumerate}
\hypersetup{colorlinks,linkcolor={blue},citecolor={blue},urlcolor={red}} 
\usepackage{algpseudocode,algorithm}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\tr}{tr}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{assumption}{Assumption}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}


\setcounter{figure}{0}   
\setcounter{table}{0}  


\newcommand{\cmt}[1]{{\leavevmode\color{red}{#1}}}

\usepackage{dsfont}
\usepackage{multirow}

\DeclareMathOperator*{\minimize}{minimize}


\mathtoolsset{showonlyrefs}
\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}
\usepackage{xr}

\input macros.tex



\title{\textbf{Linear Algebra -- Part II}\\A summary for MIT 18.06SC}

\date{\today}
\author{%
Jiaxin Hu
}



\begin{document}

% Makes the title and author information appear.

\maketitle


% Abstracts are required.
\section{Orthogonality}

\subsection{Orthogonal vectors and subspaces}
\begin{defn}[\textit{Orthogonal vectors}]\label{def:vortho}
	Let  $x,y \in \bbR^{n}$ be two vectors. The vectors $x$ and $y$ are orthogonal,  denoted $x \perp y$, if and only if $x^T y = y^T x = 0$.
\end{defn}

\begin{defn}[\textit{Orthogonal subspaces}]\label{def:sortho}
	Let  $S,T$ be two subspaces. The subspaces $S$ and $T$ are orthogonal,  denoted $S \perp T$, if and only if for any $s \in S$ and any $ t \in T$, $s^T t = t^T s = 0$.
\end{defn}

Let $\mA \in \bbR^{m \times n}$ be a matrix. There are four subspaces related to $\mA\colon$ column space $C(\mA)$, row space $C(\mA^T)$, nullspace $N(\mA)$, and left nullspace $N(\mA^T)$. Suppose the rank of $\mA$ is $\rank(\mA) = r$. The dimensions of these subspaces are: 
\[ \dim(C(\mA)) = \dim(C(\mA^T)) = r,\quad  \dim(N(A)) = n-r,\quad \dim(N(\mA^T)) = m-r. \] 

\begin{thm}[Orthogonality of matrix subspaces]\label{thm:ortho}
	Let  $\mA \in \bbR^{m \times n}$ be a matrix. The row space $C(\mA^T)$ and the nullspace $N(\mA)$ are orthogonal. The column space $C(\mA)$ and the left nullspace $N(\mA^T)$ are orthogonal. That is
	\[ C(\mA^T) \perp N(\mA)\quad  \text{and} \quad C(\mA) \perp N(\mA^T).  \]
\end{thm}

\begin{proof}
	For any vector $x \in N(\mA)$, we have $\mA x = 0$. Specifically,
	\begin{align}
		\mA x  = \begin{bmatrix}
			a_1^T x \\ \vdots \\ a_m^T x 
		\end{bmatrix} = \begin{bmatrix}
			0\\ \vdots \\0
		\end{bmatrix},
	\end{align}
	where $a_i \in \bbR^n$ is the $i$-th  row of $\mA$, for all $ i \in [m]$. By Definition~\ref{def:vortho}, $x$ is orthogonal to the rows in matrix $\mA$. For any vector $v \in C(\mA^T)$, $v$ is a linear combination of the rows $v = c_1 a_1 + ...+c_m a_m$, where $c_i \in \bbR$, for all $i\in [m]$. Taking inner product between vectors $v$ and $x$, we have
	\[  v^T x =  c_1 a_1^T x + \cdots + c_m a_m^T x = 0.  \]
	Therefore, $v \perp x$, and $N(\mA) \perp C(\mA^T)$.
	
	Similarly, for any $x \in N(\mA^T)$, we have $\mA^T x = 0$, which implies  that $N(\mA^T) \perp C(\mA)$. 
\end{proof}

For notational convenience, we use the shorthands $\mA$ for the column space $C(\mA)$ and $\mA^{\perp}$ for the nullspace $N(\mA)$ in the rest of this section. Though $\mA$ usually denotes a matrix, the meaning of $\mA$ should be clear given the context. For example, $\rank(\mA)$ refers to the rank of matrix $\mA$, while $\dim(\mA)$ refers to the dimension of the column space $C(\mA)$. Specifically, for a vector $v$, $v \in \mA$ means that the vector $v$ belongs to the column space $C(\mA)$.
 
\vspace{0.2cm}
Given a matrix $\mA \in \bbR^{m \times n}$, we also rewrite the Theorem~\ref{thm:ortho} as
\[ \mA^T \perp \mA^{\perp} \quad \text{and} \quad \mA \perp (\mA^{T})^{\perp}. \]

\begin{thm}[Relationship between $\mA^T \mA$ and $\mA$]\label{thm:aa}Let $\mA \in \bbR^{m \times n}$ be a matrix. We have
	\begin{equation}
		(\mA^T \mA)^{\perp} = \mA^{\perp} \quad \text{and} \quad \rank(\mA^T \mA) = \rank(\mA).
	\end{equation}
\end{thm}
\begin{proof}
	First, we prove that $(\mA^T \mA)^{\perp} = \mA^{\perp}$.
	
	On one hand, if the vector $x \in \mA^{\perp}$, then $\mA x = 0$. Multiplying the matrix $\mA^T$ on both sides of the equation, we have $ \mA^T  \mA x = 0$. Therefore, for any $x \in \mA^{\perp}$, the vector $x \in (\mA^T \mA)^{\perp}$. 
	\iffalse
	
	On the other hand, we  prove  by contradiction that for any $x \in (\mA^T \mA)^{\perp}$ the vector $x \in \mA^{\perp}$.
	
	Suppose there is a vector $x \in (\mA^T \mA^T)^{\perp}$ but $x \notin \mA^{\perp}$. We have
	\begin{equation}\label{eq:aaa}
		\mA x = b \neq 0, \quad \mA^T \mA x = 0 \quad \Rightarrow \quad \mA^T b = 0.
	\end{equation} 
	By the first equality in \eqref{eq:aaa}, $b \in \mA$, and by the third equation in \eqref{eq:aaa}, $ b \in \mA^{T,\perp}$. This contradicts the Theorem~\ref{thm:ortho} that $\mA \perp \mA^{T,\perp}$. Therefore, for any $x \in (\mA^T \mA)^{\perp}$, the vector $x \in \mA^{\perp}$.
	
	\fi
	
	On the other hand, if the vector $x \in (\mA^T \mA)^{\perp}$, we have
	\[ \mA^T \mA x = 0.\]
	Then, the vector $\mA x \in  (\mA^{T})^{\perp}$. By the definition of matrix-to-vector multiplication, we also have $\mA x \in \mA$.  By Theorem~\ref{thm:ortho}, the subspaces $(\mA^{T})^{\perp} \perp \mA$. Therefore, $\mA x = 0$, and we obtain that $x \in \mA^{\perp}$.
	\vspace{0.2cm}
	
	Next, we prove that $\rank(\mA^T \mA) = \rank(\mA)$. 
	
	Given $(\mA^T \mA)^{\perp} = \mA^{\perp}$, we have $\rank(\mA^T \mA) = n - \dim((\mA^T \mA)^{\perp}) = n - \dim(\mA^{\perp}) = \rank(\mA)$.
\end{proof}

\begin{cor}[Invertibility of $\mA^T \mA$]\label{cor:invert}
	If $\mA$ has independent columns, then $\mA^T \mA$ is invertible.
\end{cor}
\begin{proof}
Suppose $\mA \in \bbR^{m \times n}$ is a matrix with independent columns; i.e., $\rank(\mA) = n$. By Theorem~\ref{thm:aa}, $\rank(\mA^T \mA) = \rank(\mA) = n$. Since $\mA^T \mA \in \bbR^{n \times n}$ is a square matrix, $\mA^T \mA$ is invertible.
\end{proof}


\subsection{Projections onto subspaces}
\begin{defn}[\textit{Projection and projection matrix}]\label{def:project} Let  $x \in \bbR^{x}$ be a vector and $\mA^{n \times m}$ be a matrix. The vector $p_{\mA}(x) \in \mA$ that satisfies
	\begin{equation}\label{eq:project}
		(x-p_{\mA}(x)) \perp \mA,
	\end{equation} 
	is called the projection of vector $x$ onto the column space $\mA$. For all $x \in \bbR^n$, the matrix  $\mP_{\mA} \in \bbR^{n \times n}$  that satisfies	\begin{equation}
		p_{\mA}(x) =\mP_{\mA} x,
	\end{equation}
	is called the projection matrix of $\mA$ from $\bbR^n$ onto the column space $\mA$. Note that the projection $p_{\mA}(x)$ depends on the vector $x$ and the space $\mA$, while the projection matrix $\mP_{\mA}$ depends on the space $\mA$ only. 
\end{defn}

\begin{prop}[Projection matrix of $\mA$]\label{prop:project}
	Let  $x \in \bbR^{n}$ be a vector and $\mA^{n \times m}$ be a matrix with independent columns. The projection matrix of $\mA$ from $\bbR^n$ onto the column space  $\mA$ is 
	\begin{equation}
		\mP_{\mA} =  \mA (\mA^T \mA)^{-1} \mA^T.
	\end{equation}
\end{prop}

\begin{proof}
Let $x_0$ be an arbitrary vector of $\bbR^n$. By Definition~\ref{def:project}, the projection $p_{\mA}(x_0) \in \mA$. Then, there exists a vector $v(x_0) \in \bbR^m$ such that $p_{\mA}(x_0) = \mA  v(x_0)$.  By equation~\eqref{eq:project}, we have 
\begin{equation}\label{eq:projectca}
	A^T (x_0 - \mA v(x_0)) = 0 \quad \Rightarrow  \quad v(x_0) = (\mA^T \mA)^{-1} \mA^T x_0 \quad \Rightarrow \quad p_{\mA}(x_0)  = \mA (\mA^T \mA)^{-1} \mA^T  x_0. 
\end{equation}
Since $x_0$ is arbitrary, the equation~\eqref{eq:projectca} still holds after replacing all the $x_0$ by $x$, for all $x \in \bbR^n$. Therefore, the matrix $\mP_{\mA} =  \mA (\mA^T \mA)^{-1} \mA^T$ is the projection matrix of $\mA$ from $\bbR^n$ onto the column space $\mA$.
\end{proof}
 
\begin{thm}[Properties of projection matrix]\label{thm:projectm}
Let $\mA \in \bbR^{m \times n}$ be a matrix, and $\mP_{\mA}$ be the projection matrix of $\mA$ onto the column space $\mA$. Then,
	\begin{equation}\label{eq:projectm}
		\mP_{\mA}^T = \mP_{\mA}\quad \text{and}\quad  \mP_{\mA}^2 = \mP_{\mA}.
	\end{equation}
\end{thm}
\begin{proof}
	By Proposition~\ref{prop:project}, the projection matrix is $\mP_{\mA} =   \mA(\mA^T \mA)^{-1} \mA^T$. Then,
	\[  \mP_{\mA}^T = (\mA(\mA^T \mA)^{-1} \mA^T )^T = \mA (\mA^T \mA)^{-1} \mA^T = \mP_{\mA}. \]
Similarly, we have
	\[  \mP_{\mA}^2 = \mP_{\mA}^T \mP_{\mA} =  \mA (\mA^T \mA)^{-1} \mA^T  \mA (\mA^T \mA)^{-1} \mA^T =  \mA (\mA^T \mA)^{-1} \mA^T = \mP_{\mA}. \]
\end{proof}

\begin{cor}[Projection onto $(\mA^{T})^{\perp}$]\label{cor:iprojectm}
	Suppose $\mP_{\mA}$ is the projection matrix of $\mA$ from $\bbR^n$ onto the column space $\mA$. Then $\mI-\mP_{\mA}$ is the projection matrix of $\mA$ from $\bbR^n$ onto the left nullspace $(\mA^{T})^{\perp}$. 
\end{cor}

\begin{proof}
	Let $x \in \bbR^n$ be a vector. By Definition~\ref{def:project}, $\mP_{\mA} x$ is the projection of $x$ from $\bbR^n$ onto $\mA$. Then,
	\[ x - \mP_{\mA} x \perp \mA \quad \Rightarrow \quad (\mI-\mP_{\mA})x \perp \mA. \]
	By Theorem~\ref{thm:ortho}, we have $\mA \perp (\mA^{T})^{\perp}$. Since $\dim(\mA) + \dim((\mA^{T})^{\perp}) = n$, we have $\mA  \cup (\mA^{T})^{\perp} = \bbR^n$. Then, we have
	\[ (\mI-\mP_{\mA})x \in  (\mA^{T})^{\perp}.\]
	Since $\mP_{\mA} x$ is the projection of $x$ from $\bbR^n$ to $\mA$,  the vector $\mP_{\mA} x \in \mA$. We have,
	\[ x -  (\mI-\mP_{\mA})x = \mP_{\mA} x \in \mA \quad \Rightarrow \quad  x -  (\mI-\mP_{\mA})x \perp (\mA^{T})^{\perp}.\]
	Therefore, $(\mI-\mP_{\mA})x$ is the projection of $x$ from $\bbR^n$ onto the left nullspace $\mA^{T,\perp}$. Since the above conclusion holds for all $x \in \bbR^n$, $\mI-\mP_{\mA}$ is the projection matrix of $\mA$ from $\bbR^n$ onto $\mA^{T,\perp}$.
\end{proof}

\subsection{Projection matrices and least squares}
Let $y \in \bbR^n$ be a vector and $\mX \in \bbR^{n \times (k+1)}$ be a design matrix. We propose the linear regression model as
\[ y = \mX \beta +\epsilon,\]
where $\beta = (\beta_0,\beta_1,...,\beta_k)^T$ are regression coefficients and $\epsilon$ is the noise. The least-squares estimate of $\beta$ is the minimizer of the squared loss; i.e.,
\[  \hat \beta_{LS} = \argmin_{\beta \in \bbR^{k+1}} \onorm{y - \mX \beta}^2, \]
where $\onorm{\cdot}$ is the Euclidean norm.  The projection of $y$ onto $\mX$ minimizes the Euclidean distance from $y$ to the column space $\mX$. Hence, the vector $\mX \hat \beta_{LS}$ is the projection of $y$ onto the column space $\mX$. Therefore, we may use projection to solve the least-squares problem. 

By Definition~\ref{def:project}, the projection $\mX \hat \beta_{LS}$ satisfies
\[ \mX^T( y - \mX \hat \beta_{LS} ) = 0 \quad \Rightarrow \quad \hat \beta_{LS} = (\mX^T \mX)^{-1} \mX^T y. \]

The estimate $\hat \beta_{LS}$ is the same as the estimates solved using the first-order condition for the least-squares optimization. 

\subsection{Orthogonal matrices and Gram-Schimidt}
\begin{defn}[\textit{Orthonormal vectors}]\label{def:othronov}
	The vectors $q_1,...,q_n$ are orthonormal if
	\begin{align}
		q_i^T q_j = \begin{cases}
			0& \text{if } i\neq j\\
			1 & \text{if } i =  j
		\end{cases}.
	\end{align}
\end{defn}

Orthonormal vectors are independent.

\begin{defn}[\textit{Orthonormal matrix and orthogonal matrix}]\label{def:orthonom}
	Let  $\mQ \in \bbR^{m \times n}$ be a matrix. The matrix $\mQ$ is an orthonormal matrix, if the columns of $\mQ$ are orthonormal. When $ m= n$, the square matrix $\mQ$ is an orthogonal matrix.
\end{defn}

Suppose $\mQ \in \bbR^{m \times n}$ is an orthonormal matrix. Then, we have $\mQ^T \mQ = \mI_n$.  When $m = n$, $\mQ$ is an orthogonal matrix and $\mQ^T = \mQ^{-1}$. The projection matrix of $\mQ$ onto the column space $C(\mQ)$, denoted $\mP_{\mQ}$,  becomes $\mP_{\mQ} = \mI_m$.

\begin{defn}[\textit{Gram-Schimidt Process and QR decomposition}]\label{gram-schimidt}
Let $\mA \in \bbR^{m \times n}$ be a matrix with $\rank(\mA) = n$, and  $a_i \in \bbR^{m}$ be the $i$-th column of matrix $\mA$, for all $i \in [n]$. Gram-Schimidt process finds the orthonormal basis for $\mA$ as follows,
\begin{align}
	&u_1 = a_1, &&\quad e_1 = \frac{u_1}{\onorm{u_1}};\\
	&u_2 = a_2 - \frac{u_1^T a_2}{u_1^T u_1} u_1 ,&&\quad  e_2 = \frac{u_2}{\onorm{u_2}};\\
	&u_3 = a_3 - \frac{u_1^T a_3}{u_1^T u_1} u_1 - \frac{u_2^T a_3}{u_2^T u_2} u_2,&&\quad  e_2 = \frac{u_3}{\onorm{u_3}};\\
	\vdots\\
	&u_n = a_n - \sum_{k = 1}^{n-1} \frac{u_k^T a_k}{u_k^T u_k} u_k, && \quad e_n = \frac{u_n}{\onorm{u_n}}.
\end{align}
 The vectors $e_1,...,e_n$ are orthonormal basis of $\mA$. Based on the basis obtained by Gram-Schimidit, we decompose the matrix $\mA$ as follows,
 \begin{equation}\label{eq:qr}
 	\mA = [a_1,...,a_n] = [e_1,....,e_2] \begin{bmatrix}
 		e_1^T a_1 & e_1^T a_2 & \cdots & e_1^T a_n\\
 		0&e_2^T a_2 & \cdots & e_2^T a_n\\
 		\vdots &\vdots & & \vdots \\
 		0 &0&\cdots & e_n^T a_n
 	\end{bmatrix} = \mQ \mR,
 \end{equation}
	where $\mQ \in \bbR^{m \times n}$ is an orthonormal matrix and $\mR \in \bbR^{n \times n}$ is an upper triangular matrix. The matrix decomposition in equation~\eqref{eq:qr} is called QR decomposition.
\end{defn}

\section{Determinants}
The \textit{determinant} is a number associated with a square matrix. For a square matrix $\mA$, let $\det(\mA)$ or $|\mA|$ denote the determinant of matrix $\mA$.

\subsection{Properties of determinants}
We give ten properties of determinants. The last seven properties are deduced from the first three basic properties.
\begin{enumerate}
	\item\label{prop1} The determinant of an identity matrix is equal to 1; i.e., $\det(\mI) = 1$.
	\item\label{prop2} Exchanging two rows of a matrix reverses the sign of the determinant. Hence, an odd number of row exchanges reverse the sign of the determinant, while an even number of row exchanges maintain the sign of the determinant.
	\item\label{prop3} \begin{enumerate} 
	\item[(a)] If one row of the matrix is multiplied by a constant $t$, the determinant of the matrix is multiplied by $t$.
	\begin{align}
		\begin{vmatrix}
			ta&tb\\c &d 
		\end{vmatrix} = t \begin{vmatrix}
			a&b\\c &d
			\end{vmatrix}.
	\end{align} 
	\item[(b)] The determinant is linearly additive by row.
	\begin{align}
		\begin{vmatrix}
			a+a'&b+b'\\c &d 
		\end{vmatrix} =  \begin{vmatrix}
			a&b\\c &d
			\end{vmatrix} +  \begin{vmatrix}
			a'&b'\\c &d
			\end{vmatrix}. 
	\end{align}
	\end{enumerate}

	\item \label{propidenticalrow} A matrix with two identical rows has the determinant 0.  
	\begin{proof}
		Suppose $\mA$ has two identical rows $a_i, a_j$. The matrix after exchanging $a_i, a_j$, denoted $\mA'$, is the same as the original matrix $\mA$. By property~\ref{prop2}, we have $\det(\mA) = -\det(\mA') = -\det(\mA)$. Therefore, $\det(\mA) = 0$.
	\end{proof}

	\item \label{propelim} Subtracting a multiplier of the $i$-th row from the $j$-th row preserves the determinant of the matrix, where $i \neq j$.
	\begin{proof}
		Take a 2-by-2 matrix as an example. By properties~\ref{prop3} and~\ref{propidenticalrow}, for any constant $t \in \bbR$, we have 
	\begin{align}
		 \begin{vmatrix}
			a & b \\ c - ta&d -tb
		\end{vmatrix} = \begin{vmatrix}
			a & b \\ c &d
		\end{vmatrix} -t \begin{vmatrix}
			a & b \\ a& b
		\end{vmatrix}  = \begin{vmatrix}
			a & b \\ c &d
		\end{vmatrix}.
	\end{align}
	The proof for higher-dimension matrices is similar.
	\end{proof}
	
	\item\label{prop0row} A matrix with a row that is all zeros has the determinant 0.
	\begin{proof}
		By property~\ref{prop3}, letting $t = 0$ leads to property~\ref{prop0row}.
	\end{proof}

	
	\item\label{propdiag} Let $\mA \in \bbR^{n \times n}$ be a triangular matrix with diagonal elements $d_1,...,d_n$. The determinant $\det(\mA) = \prod_{i = 1}^n d_i$. 
	\begin{proof}
	We eliminate the matrix $\mA$ to a diagonal matrix, denoted $\mA'$. By property~\ref{propelim}, $\det(\mA) = \det(\mA')$. Since $\mA$ is triangular, the diagonal elements of $\mA'$ are still $d_1,...,d_n$.
 		 By properties~\ref{prop3} and \ref{prop1}, $\det(\mA') = \prod_{i = 1}^n d_i\det(\mI_n) =\prod_{i = 1}^n d_i$. Therefore, $\det(\mA) = \prod_{i = 1}^n d_i$.
	\end{proof}

	
	\item\label{propsingular} A matrix is singular if and only if its determinant is  0.
	
	\begin{proof}
		($\Rightarrow$)If $\mA$ is a singular matrix, the reduced row echelon form of $\mA$, denoted $RREF(\mA)$, has a row with all 0 entries. By property~\ref{prop0row}, the determinant $\det(RREF(\mA)) = 0$. Therefore, by property~\ref{propelim}, we have $\det(\mA) =\det(RREF(\mA)) =  0$. 
		
		($\Leftarrow$) If the determinant $\det(\mA) = 0$, the determinant of $RREF(\mA)$ is 0. Since the reduced row echelon form  of a matrix contains only distinct nonzero rows, the zero determinant implies that there exists at least one zero row in $RREF(\mA)$. Therefore, the matrix $\mA$ is singular.
	\end{proof}

	
	\item\label{propproduct} Let  $\mA, \mB \in \bbR^{n \times n}$ be two matrices. The determinant of the matrix product is $\det(\mA \mB) = \det(\mA) \det(\mB).$
	
	\begin{proof}
		First, suppose that at least one of $\mA, \mB$ is singular. The product $\mA \mB$ is also singular. Then, $\det(\mA \mB) = \det(\mA) \det(\mB) = 0$. 
		
		Second, let $\mE$ be an elimination matrix. The product $\mE \mB$ means implementing a linear operation on the rows of $\mB$. By property~\ref{propelim}, linear operation preserves the determinant of $\mB$. Note that the determinant of an elimination matrix is 1. Therefore, we have
		\begin{equation}\label{eq:producteli}
			|\mE \mB| = |\mE| |\mB|.
		\end{equation}
		
		Last, suppose that both $\mA, \mB$ are invertible. By elimination, there exist a sequence of elimination matrices $\{ \mE_i \}_{i = 1}^n$ such that 
		\[ \mE_n \cdots \mE_1 \mA = \mI.  \]
		By Lemma 2 in Linear Algebra-Part I, the inverse of an elimination matrix is still an elimination matrix. Let $\mE'_k$ denote the inverse $\mE^{-1}_k$, for all $k \in [n]$. We rewrite $\mA$ as
		\[ \mA = \mE'_n \cdots \mE'_1. \]
		By equation~\eqref{eq:producteli}, we have
		\begin{equation}\label{eq:prod1}
			|\mA \mB| = | \mE'_n \cdots \mE'_1 \mB | = |\mE'_n| |\mE'_{n-1} \cdots \mE'_1 \mB| = \cdots = |\mE'_n|\cdots |\mE'_1||\mB|. 
		\end{equation}
		Applying the equation~\eqref{eq:producteli} again to the term $|\mE'_n|\cdots |\mE'_1| $, we have
		\begin{equation}\label{eq:prod2}
			|\mE'_n|\cdots |\mE'_1|  =|\mE'_n|\cdots |\mE'_3| |\mE'_2 \mE'_1| = \cdots = | \mE'_n \cdots \mE'_1|.
		\end{equation}
		Hence, combing equation~\eqref{eq:prod1} with equation~\eqref{eq:prod2}, we obtain the result 
		\[ |\mA \mB|  = | \mE'_n \cdots \mE'_1| |\mB| = |\mA| |\mB|.\]
	\end{proof}
	
	\item\label{proptrans} Let $\mA$ be a square matrix. The determinant $\det(\mA^T) = \det(\mA)$. 
	
	\begin{proof}
		By LU decomposition, we rewrite the matrix as $\mA = \mL \mU$, where $\mL$ is a lower-triangular matrix and $\mU$ is an upper-triangular matrix. Note that the transports $\mL^T,\mU^T$ have the same diagonal elements with $\mL, \mU$, respectively. By property~\ref{propdiag}, we have $\det(\mL^T)= \det(\mL)$ and $  \det(\mU^T) = \det(\mU)$. Therefore, by property~\ref{propproduct}, we obtain  $\det(\mA) = \det(\mL) \det(\mU) = \det(\mU^T) \det(\mL^T) = \det(\mU^T \mL^T) = \det(\mA^T)$. 
			\end{proof}
\end{enumerate}

\subsection{Determinant computation}
\begin{prop}[Big formula for computing determinant]
	Let  $\mA = \entry{a_{ij}} \in \bbR^{n \times n}$ be a square matrix. The big formula for computing the determinant of $\mA$ is
	\[ \det(\mA) =  \sum_{(\alpha_1,...,\alpha_n) \in \tP} (-1)^{N(\alpha_1,...,\alpha_n)} a_{1\alpha_1} ... a_{n \alpha_n},  \]
	where $(\alpha_1, \alpha_2, ...,\alpha_n)$ is the permutation of $(1,2,...,n)$, $\tP$ is the collection of all possible  $(\alpha_1,...,\alpha_n)$, and $N(\alpha_1,...,\alpha_n)$ is the number of swaps from $(\alpha_1,\alpha_2,...,\alpha_n)$  to $(1,2,...,n)$. 
\end{prop}

\begin{proof}
	Let $\mA = \entry{a_{ij}} \in \bbR^{n \times n}$ be a square matrix. We decompose the  matrix $\mA$ into the sum of $n^n$ matrices, denoted $\mA'_i$, where $i \in [n^n]$. For each $\mA'_i$, only one entry in each row comes from $\mA$, and the other entries of $\mA'_i $ are 0. By property~\ref{prop3}, the determinant $\det(\mA) = \sum_{i=1}^{n^n} \det(\mA'_i)$. Take a 3-by-3 square matrix as an example.
	\begin{align}
		\begin{vmatrix}
			a_{11} & a_{12} & a_{13}\\
			a_{21} & a_{22} & a_{23}\\
			a_{31} & a_{32} & a_{33}
		\end{vmatrix} = \begin{vmatrix}
			a_{11} & 0& 0\\
			a_{21} & 0 & 0\\
			a_{31} &0 & 0
		\end{vmatrix} + \begin{vmatrix}
			a_{11} & 0& 0\\
			0 & a_{22} & 0\\
			a_{31} &0 & 0
		\end{vmatrix} + \cdots +\begin{vmatrix}
			0 & 0& a_{13}\\
			0& 0 & a_{23}\\
			 0&0 & a_{33}
		\end{vmatrix} = \det(\mA'_1)  +\cdots + \det(\mA'_{27}).
	\end{align}
	There are $n!$ nonsingular matrices among matrices $\mA'_i$. The number of nonsingular matrices coincides with the size of $\tP$. Specifically, there are $n$ ways to choose an element from the first row, after which there are only $n-1$ ways to choose an element from the second row to avoid the zero determinant. Therefore, we have $n \times (n-1) \times (n-2) \times \cdots \times 2 = n!$ nonsingular matrices.
	
	By properties~\ref{propdiag} and \ref{prop2}, the determinant of a nonsingular $\mA'_i$ follows the formula \[(-1)^{N(\alpha_1,...,\alpha_n)}  a_{1\alpha_1} ... a_{n \alpha_n},\]
	where $(\alpha_1, \alpha_2, ...,\alpha_n)$ is the permutation of $(1,2,...,n)$, $\tP$ is the collection of all possible  $(\alpha_1,...,\alpha_n)$, and $N(\alpha_1,...,\alpha_n)$ is the number of swaps from $(\alpha_1,\alpha_2,...,\alpha_n)$  to $(1,2,...,n)$. The determinant of $\mA$ is the sum of the nonsingular matrices' determinants.
\end{proof}


\begin{defn}[\textit{Cofactors, cofactor matrix, and cofactor formula}]
	Let $\mA=  \entry{a_{ij}} \in \bbR^{n \times n}$ be a square matrix, and $\mA_{-i,-j}$ be the submatrix of $\mA$ after removing the $i$-th row and $j$-th column. The cofactor associated with $a_{ij}$ is defined as \[C_{ij} = (-1)^{i+j}\det(\mA_{-i,-j}).\]
	The matrix $\mC = \entry{C_{ij}} \in \bbR^{n \times n}$ is called the cofactor matrix of $\mA$.  The cofactor formula of $\det(\mA)$ is
	\[ \det(\mA) = \sum_{j=1}^n a_{ij} C_{ij} = \sum_{j=1}^n a_{ji} C_{ji} ,\quad \text{ for all } i \in [n].  \]
\end{defn}

\begin{example}[Tridiagonal matrix]
	One example of using the cofactor formula is computing the determinant of a \textit{tridiagonal matrix}. The tridiagonal matrix is a matrix in which nonzero elements locate only on or adjacent to the diagonal. Let $\mT_n \in \bbR^{n \times n}$ denote the tridiagonal matrix of 1's; i.e.,
\begin{align}
	\mT_n = \begin{bmatrix}
		1 &1&0 &\cdots & 0 &0\\
		1&1&1&\cdots &0&0\\
		0&1&1&\cdots &0&0\\
		\vdots &\vdots &\vdots &&\vdots &\vdots \\
		0&0&0&\cdots&1&1
	\end{bmatrix}_{n \times n}.
\end{align} 

Let $\mT_{n, -i,-j}$ be the submatrix of  $\mT_n$ after removing the $i$-th row and the $j$-th column.   By the cofactor formula, we have
\[  \det(\mT_n) = 1 \times \det(\mT_{n, -1,-1}) - 1 \times \det(\mT_{n,-1,-2}). \]
By the definition of tridiagonal matrix, we have $\mT_{n,-1,-1} = \mT_{n-1}$, and $\det( \mA_{n,-1,-2}) = 1 \times \det(\mT_{n-2}) $. Therefore, 
\[ \det(\mT_n) = \det(\mT_{n-1}) - \det(\mT_{n-2}).  \] 
\end{example}

\subsection{Inverse matrices}
Previously, we use Gauss-Jordan elimination to obtain the inverse matrix of an invertible matrix. Here, we apply the cofactor formula to compute the inverse matrix.

\begin{thm}[Inverse matrix by cofactors]\label{thm:inver}
Let $\mA \in \bbR^{n \times n}$ be an invertible matrix, and $\mC$ be the cofactor matrix of $\mA$. The inverse matrix of $\mA$ satisfies
\[  \mA^{-1} = \frac{1}{\det(\mA)} \mC^T. \]	
\end{thm}

\begin{proof}
	It is equivalent to  show that $\mA \mC^T = \det(\mA) \mI_n$. 
	
	Let $\mA = \entry{a_{ij}} \in \bbR^{n \times n}$ be an invertible matrix, and $C_{ij}$ be the cofactor of $a_{ij}$. By the definition of cofactor matrix, we have
	\begin{align}
		\mA  \mC^T = \begin{bmatrix}
			a_{11} & a_{12} &\cdots &a_{1n}\\
			a_{21} & a_{22} &\cdots &a_{2n}\\
			\vdots & \vdots & & \vdots\\
			a_{n1} & a_{n2} &\cdots &a_{nn}\\
		\end{bmatrix} \begin{bmatrix}
			C_{11} & C_{21} &\cdots &C_{n1}\\
			C_{12} & C_{22} &\cdots &C_{n2}\\
			\vdots & \vdots & & \vdots\\
			C_{1n} & C_{2n} &\cdots &C_{nn}\\
		\end{bmatrix},
	\end{align}
	
	Therefore, the diagonal elements of $\mA \mC^T$ are $(\mA \mC^T)_{ii} = \sum_{k=1}^n a_{ik} C_{ik} = \det(\mA)$, for all $ i \in [n]$. Consider the off-diagonal entries $(\mA \mC^T)_{ij} = \sum_{k =1}^n a_{ik} C_{jk}$, for all  $i \neq j$ and $i, j \in [n]$. By the cofactor formula, the sum   $ \sum_{k =1}^n a_{ik} C_{jk}$ is equal to the determinant of a matrix whose $i$-th row and $j$-th row are identical. By property~\ref{prop0row}, the off-diagonals of $\mA \mC^T$ are 0. Then, we obtain $\mA \mC^T = \det(\mA) \mI_n$.
\end{proof}

\begin{defn}[\textit{Cramer's rule}]
Let  $\mA\in\bbR^{n \times n}$  be  an invertible matrix, and $ x, b \in \bbR^n$ be two vectors. Applying Theorem~\ref{thm:inver} to the linear system $\mA x=b$, we obtain the Cramer's rule of $x = \mA^{-1} b$, 
\[  x = \frac{1}{\det(\mA)} \mC^T b\quad \text{ and } \quad x_j = \frac{\det(\mB_j)}{\det(\mA)}, \quad \text{for all }j \in [n], \] 
	where $x_j$ is $j$-th entry of $x$, and $\mB_j$ is the matrix $\mA$ after replacing the $j$-th column by $b$.
\end{defn}

\begin{defn}[\textit{Parallelepiped}]
	The parallelepiped determined by $n$ vectors $v_1,...,v_n \in \bbR^n$ is defined as the following subset,
	\[ P = \{ a_1 v_1 + \cdots + a_n v_n \colon 0 \leq a_1,...,a_n \leq 1 \}. \]
	 We use $\vol(P)$ to denote the volume of the parallelepiped  $P$. 
\end{defn}

\begin{prop}[Determinants and volumes]
The absolute determinant of a square matrix $\mA$ is the volume of parallelepiped determined by the rows of $\mA$; i.e.,
	\[ |\det(\mA)| = \vol(P(\mA)), \]
	where $P(\mA)$ is the parallelepiped determined by the rows of $\mA$.
\end{prop}

\begin{example}
	The area of a triangle with vertices at $(x_1,y_1), (x_2,y_2), (x_3,y_2)$ is
	\begin{align}
		\frac{1}{2} \begin{vmatrix}
			x_1 & y_1 & 1\\
			x_2 & y_2 & 1\\
			x_3 & y_3 & 1
		\end{vmatrix}.
	\end{align}
	
	\begin{proof}
		The two edges of the triangle are $v_1 = (x_2 - x_1, y_2 -y_1)$ and $v_2 = (x_3 - x_1, y_3 -y_1)$. The area of the triangle is half of the area of the parallelepiped determined by $v_1, v_2$. Therefore, the area of the triangle is 
		\begin{align}
			\frac{1}{2} \begin{vmatrix}
				x_2 - x_1 &  y_2 - y_1\\
				x_3 - x_1& y_3 -y_1
			\end{vmatrix} =  \frac{1}{2} \left(  \begin{vmatrix}
				x_2  &  y_2 \\
				x_3 & y_3
			\end{vmatrix} + \begin{vmatrix}
				x_1 &  y_1\\
				x_2& y_2
			\end{vmatrix}  - \begin{vmatrix}
				x_1 &  y_1\\
				x_3 & y_3 
			\end{vmatrix}    \right) = \frac{1}{2} \begin{vmatrix}
			x_1 & y_1 & 1\\
			x_2 & y_2 & 1\\
			x_3 & y_3 & 1
		\end{vmatrix}.
		\end{align}
	\end{proof}
\end{example}

\section{Eigenvalues and eigenvectors}
\begin{defn}[\textit{Eigenvalues and eigenvectors}]\label{def:eigen}
	Let $\mA \in \bbR^{n \times m}$ be a square matrix. Suppose there exists a nonzero vector $x \in \bbR^{m}$ such that
	\[ \mA x = \lambda x, \quad \text{for some } \lambda \in \mathbb{C}. \]
	The value $\lambda$ is called the eigenvalue of $\mA$, and the vector $x$ is called the eigenvector of $\mA$ associated with eigenvalue $\lambda$.
\end{defn}

 Usually, eigenvectors are normalized; i.e., $\onorm{x} = 1$, where $\onorm{\cdot}$ is the Euclidean norm. For simplicity, all the eigenvectors mentioned below are normalized. Besides, the eigenvectors associated with eigenvalue 0 span the nullspace of $\mA$.

\begin{defn}[\textit{Trace of square matrix}]\label{def:trace}
Let $\mA = \entry{a_{ij}} \in \bbR^{n \times n}$ be a square matrix. The trace of $\mA$ is defined as
\[ \tr(\mA) = \sum_{i = 1}^n a_{ii}.\] 
\end{defn}

\begin{defn}[\textit{Characteristic polynomial}]\label{def:charac}
	Let $\lambda$ denote an eigenvalue of the matrix $\mA\in \bbR^{n \times n}$.
	The determinant of $\mA - \lambda \mI_n$ is a polynomial of $\lambda$, denoted $P(\lambda)$. We call the polynomial $P(\lambda)$ as the characteristic polynomial of $\mA$. Specifically,
	\begin{equation}\label{def:charapoly}
		 P(\lambda)  = (-1)^n \lambda^n +  (-1)^n  \tr(\mA)\lambda^{n-1} + \cdots + (-1)^n \det(\mA). 
	\end{equation}
\end{defn}

Since the characteristic polynomial $P(\lambda)$ is of degree $n$, there exist $n$ solutions to the equation $P(\lambda) = 0$. The solutions to $P(\lambda) = 0$, denoted $\lambda_1,...,\lambda_n$, are eigenvalues of $\mA$.

\vspace{0.2cm}
The eigenvalues may be complex, and the complex eigenvalues come in conjugate pairs. Let $\lambda$ be a complex eigenvalue, $\overline \lambda$ be the conjugate of $\lambda$, and $\overline{P(\cdot)}$ be the conjugate of the characteristic polynomial. Because the characteristic polynomial satisfies $P(\overline{ \lambda}) = \overline{ P(\lambda)}$, the conjugate eigenvalue $\overline \lambda$ is also a solution to $P(\lambda) = 0$. 

\vspace{0.2cm}
The eigenvalues of a matrix are not necessarily distinct with each other.


\subsection{Properties for eigenvectors and eigenvalues}

\begin{thm}[Summation and production of eigenvalues]\label{thm:speigen}
	Let  $\mA \in \bbR^{n \times n}$ be a square matrix, and $\lambda_1,...\lambda_n$ be the eigenvalues of $\mA$. Then,
	\begin{equation}
		\sum_{i=1}^n \lambda_i = \tr(\mA)\quad \text{and} \quad \prod_{i=1}^n \lambda_i = \det(\mA).
	\end{equation} 
\end{thm}

\begin{proof}
	First, we re-write the characteristic polynomial $P(\lambda)$ of $\mA$ using the factorization form,
	\begin{equation}\label{eq:poly}
		P(\lambda) = (-1)^n (\lambda - \lambda_1)\cdots (\lambda - \lambda_n), 
	\end{equation}   
	where $\lambda_i$ are the eigenvalues of $\mA$, for all $ i \in [n]$. In equation~\eqref{eq:poly}, the coefficient for $\lambda^{n-1}$ is equal to $(-1)^n\sum_{i=1}^n \lambda_i$. Comparing equation~\eqref{eq:poly} with Definition~\ref{def:charac}, we obtain $\tr(\mA) = \sum_{i=1}^n \lambda_i$. Similarly, the constant term in equation~\eqref{eq:poly} is $(-1)^n \prod_{i=1}^n \lambda_i $, while the constant term in Defintion~\ref{def:charac} is $(-1)^n \det(\mA)$.  Therefore, we obtain $\prod_{i=1}^n \lambda_i = \det(\mA)$.
\end{proof}

\begin{defn}[\textit{Similar matrices}]\label{def:similar}
	Let $\mA \in \bbR^{n \times n}$ be a square matrix, and $\mB \in \bbR^{n \times n}$ be an invertible matrix. The matrix product $\mB \mA \mB^{-1}$ is called the similar matrix of $\mA$.
\end{defn}

\begin{thm}[Eigenvalues for similar matrices]\label{thm:similar}
	Let $\mA$ be a square matrix. The similar matrices of $\mA$ share the same eigenvalues of $\mA$.
\end{thm}
\begin{proof}
	Let $\mB \mA \mB^{-1}$ be a similar matrix of $\mA$, and $(\lambda_i, x_i)$ be the $i$-th eigenvalue-eigenvector pair of $\mA$, for all $i \in [n]$. Then, we have
	\[ \lambda_i x_i = \mA x_i \quad \Rightarrow \quad \lambda_i \mB x_i = \mB \mA \mB^{-1} \mB x_i, \quad \text{for all }i \in [n]. \]
	Since that $x_i$ is a nonzero vector, and $\mB$ is invertible, the vector $\mB x_i$ is also a nonzero vector. Therefore, $(\lambda_i, \mB x_i)$ is the $i$-th eigenvalue-eigenvector pair of $\mB \mA \mB^{-1}$, for all $i \in [n]$.
\end{proof}

\begin{thm}[Eigenvalues for powers of the matrix]
	Let $\mA$ be a matrix, and $(\lambda,x)$  be an eigenvalue-eigenvector pair of $\mA$. For any polynomial $P$, we have
	\[ P(\mA) x = P(\lambda) x. \]
\end{thm}
\begin{proof}
	For all integer $k \geq 0$ and any constant $c \in \bbR$, we have
	\[ c\mA^k x = c\mA^{k-1} \mA x= c\lambda \mA^{k-1} x = \cdots = c\lambda^k x.\]
	Therefore, $P(\mA)x = P(\lambda)x$ holds for any polynomial $P$.
\end{proof}

\begin{thm}[Eigenvalues for the inverse]
Let $\mA \in \bbR^{n \times n}$ be an invertible matrix, and $(\lambda_i, x_i)$ be the $i$-th eigenvalue-eigenvector pair of $\mA$, for all $i \in [n]$. Then, $(\lambda^{-1}_i, x_i)$ is the $i$-th eigenvalue-eigenvector pair of the inverse $\mA^{-1}$, for all $i \in [n]$.
\end{thm}

\begin{proof}
	Since $\mA$ is invertible, the eigenvalue $\lambda_i \neq 0$, for all $  i \in [n]$. Multiplying $x_i$ on both sides of the equation $\mA^{-1} \mA = \mI_n$ yields
	\[ \mA^{-1} \mA x_i = x_i \quad \Rightarrow\quad  \mA^{-1} x_i = {\lambda_i}^{-1} x_i, \quad \text{for all } i \in [n]. \]
Therefore, $(\lambda_i^{-1},x_i)$ is the $i$-th eigenvalue-eigenvector pair of $\mA^{-1}$, for all $i \in [n]$.
\end{proof}

\begin{thm}[Independence of eigenvectors]\label{thm:indepeigen} 
 The eigenvectors of a matrix associated with distinct eigenvalues are independent.
\end{thm}

\begin{proof}
	Suppose the square matrix $\mA \in \bbR^{n \times n}$ has $m$ distinct eigenvalues $\lambda_1,...,\lambda_m$ with associated eigenvectors  $v_1,...,v_m$. Consider a linear combination $c_1 v_1 + ... + c_m v_m = 0$, where $c_i \in \bbR$, for all $i \in [m]$.	
	Then, we have
	\begin{equation}\label{eq:indep1}
		\mA(c_1 v_1 + ... + c_m v_m ) = c_1 \lambda_1 v_1 + ... + c_m \lambda_m v_m = 0,
		\end{equation}
		and
		\begin{equation}\label{eq:indep2}
		\lambda_1 (c_1 v_1 + ... + c_m v_m ) = c_1 \lambda_1 v_1 + ... + c_m \lambda_1 v_m = 0. 
	\end{equation}
	Subtracting equations~\eqref{eq:indep1} from \eqref{eq:indep2} yields
	\begin{equation}\label{eq:indep3}
		 c_2 (\lambda_1 - \lambda_2) v_2 + \cdots + c_m (\lambda_1 - \lambda_m) v_m  = 0.
	\end{equation}
	Consider the new linear combination $ c_2 (\lambda_1 - \lambda_2) v_2 + \cdots + c_m (\lambda_1 - \lambda_m) v_m = 0$, and repeat the steps from equation~\eqref{eq:indep1} to equation~\eqref{eq:indep3}. After $m$ iterations, we have 
	\[ c_m(\lambda_1 - \lambda_m)(\lambda_2 - \lambda_m)\cdots (\lambda_{m-1} - \lambda_m) v_m = 0.    \]
	 Since $\lambda_1 ,..., \lambda_m$ are distinct, and $v_m$ is a nonzero vector, the coefficient $c_m = 0$.  Plugging $c_m = 0$ back to previous steps, we have $c_1 = \cdots = c_m = 0$. Therefore, $v_1,...,v_m$ are independent.
\end{proof}

\begin{thm}[Eigenvalues for triangular matrices]\label{thm:trianeigen}
	The eigenvalues of a triangular matrix are the diagonal elements.
\end{thm}

\begin{proof}
Let $\mA = \entry{a_{ij}} \in \bbR^{n \times n}$ be a triangular matrix. By the property of determinant~\ref{propdiag}, we have
\[ \det(\mA) = \prod_{i=1}^n a_{ii}  \quad \text{and} \quad \det(\mA - \lambda \mI_n) = \prod_{i=1}^n (a_{ii}-\lambda). \]
To let $\det(\mA - \lambda \mI_n) = 0$, we obtain $\lambda = a_{ii}$, for all $ i\in[n]$. Therefore, the diagonal elements are the eigenvalues of the triangular matrix.
\end{proof}


\subsection{Eigenvalues and eigenvectors for symmetric matrices}

\begin{defn}[Antisymmetric matrices]
	A matrix $\mA$ is antisymmetric if 
	\[\mA^T = - \mA. \] 
\end{defn}

\begin{thm}[Eigenvalues for symmetric and antisymmetric matrices]\label{thm:symmeign}
All the eigenvalues of a symmetric matrix with real entries only are real. All the eigenvalues of an antisymmetric matrix are imaginary.
\end{thm}
\begin{proof}
	First, we prove by contradiction that a symmetric matrix with real entries only has real eigenvalues. 
	
	Let  $\mA$  be a symmetric  matrix with real entries only. Suppose $\lambda$  is a complex eigenvalue of $\mA$ with  a nonzero imaginary part, and $x$ is the eigenvector associated with $\lambda$. Since all the entries of $\mA$ are real, the eigenvector $x$ is also a complex vector. Let $\overline \lambda$ and $\overline x$ denote the conjugate eigenvalue and eigenvector, respectively. Then,  we have
	\begin{equation}\label{eq:symm}
		\overline x^T \mA x = \overline x^T \lambda x,
	\end{equation} 
	and
	\begin{equation}\label{eq:symm2}
		x^T \mA \overline x = x^T \overline \lambda \overline x.
	\end{equation}
	 By the symmetry of $\mA$, we have
	\[ \overline x^T \mA x  = x^T \mA^T \overline x = x^T \mA \overline x .\] 
	 Subtracting equation~\eqref{eq:symm2} from equation~\eqref{eq:symm} yields	\[  0 = ( \overline x^T \mA x -  x^T \mA \overline x ) = (\lambda - \overline \lambda) x^T \overline x.  \]
	Since that $x^T \overline x$ is a nonzero real number, the imaginary part of $\lambda$ is equal to 0, which contradicts the assumption that $\lambda$ has a nonzero imaginary part. Therefore, the eigenvalues of $\mA$ are real.
	
	\vspace{0.2cm}
	Next, we prove that the eigenvalues for antisymmetric matrices are imaginary. 
	Let $\mA$ be an antisymmetric matrix,  $\lambda$ be a complex eigenvalue of $\mA$, and $x$ be the complex eigenvector associated with $\lambda$. By the antisymmetry of $\mA$, we have
	\[ \overline x^T \mA x  = x^T \mA^T \overline x  =  -x^T \mA \overline x .  \] 
	Then, 
	\[  0 = ( \overline x^T \mA x +  x^T \mA \overline x ) = (\lambda + \overline \lambda) x^T \overline x.   \]
	Since that $x^T \overline x$ is a nonzero real number, the real part of $\lambda$ is equal to 0. Therefore, the eigenvalues for an antisymmetric matrix are imaginary.
\end{proof}

\begin{thm}[Orthogonality of eigenvectors for symmetric matrices]\label{thm:symmortho}
The eigenvectors of a symmetric matrix associated with distinct eigenvalues are orthogonal.
\end{thm}


\begin{proof}
	Let $\lambda_1$ and $\lambda_2$ be two distinct eigenvalues of $\mA$, and  $x_1, x_2$ are two eigenvectors associated with $\lambda_1$ and $\lambda_2$, respectively. By the symmetry of $\mA$, we have
	\[ x_2^T \mA x_1 = x_1^T \mA^T x_2  = x_1^T \mA x_2.\]
	Then, we have
	\[  0 = ( x_2^T \mA x_1 -  x_1^T \mA x_2 ) = (\lambda_1 - \lambda_2) x_2^T x_1. \]
	Since $\lambda_1, \lambda_2$ are distinct, we have $\lambda_1 - \lambda_2 \neq 0$ and $x_2^T x_1 = 0$. Therefore, $x_1$ and $x_2$ are orthogonal. 
\end{proof}

\begin{thm}[Eigenvectors for repeated eigenvalues of symmetric matrices]\label{thm:symmrepeat} Let  $\mA \in \bbR^{n \times n}$ be a symmetric matrix. Suppose $\lambda_0$ is a repeated eigenvalue of $\mA$ with multiplicity $m$, where $2 \leq m \leq n$. There exist $m$ orthonormal eigenvectors associated with $\lambda_0$.
\end{thm}

\begin{proof}
	We prove the existence by construction. Let $x_0$ be an eigenvector associated with $\lambda_0$. Then, there exist $n-1$ additional orthogonal vectors $y_1,...,y_{n-1} \in \bbR^n$ such that $(x_0, y_1,...,y_{n-1})$ forms an orthonormal basis of $\bbR^n$. Let $\mY = [y_1,...,y_{n-1}]$ and $\mX = [x_0,Y]$. Since $\mA$ is a symmetric matrix, we have $x_0^T \mA = \lambda_0 x_0^T $. Consider the following matrix product,
	\begin{align}\label{eq:symmrepeat}
		\mX^T \mA \mX =  \begin{bmatrix}
			x_0^T \mA x_0 & x_0^T \mA \mY\\
			\mY^T \mA x_0 & \mY^T \mA \mY 
		\end{bmatrix} =  \begin{bmatrix}
			\lambda_0 & 0\\
			0 & \mY^T \mA \mY 
		\end{bmatrix} .
	\end{align}
	Note that $\mX$ is an orthogonal matrix satisfying $\mX^T = \mX^{-1}$. The product $\mX^T \mA \mX$ is a similar matrix of $\mA$. By Theorem~\ref{thm:similar},  $\lambda_0$ is also a repeated eigenvalue of $\mX^T \mA \mX$ with multiplicity $m$. The characteristic  polynomial of $\mX^T \mA \mX$ is 
	\begin{equation}\label{eq:symmploy}
		P(\lambda) = \det(\mX^T \mA \mX - \lambda \mI_n) = (\lambda_0 - \lambda) \det(\mY^T \mA \mY - \lambda \mI_{n-1}). 
	\end{equation}
	Since $\lambda_0$ has multiplicity $m \geq 2$, the term $\det(\mY^T \mA \mY - \lambda_0 \mI_{n-1}) = 0$. Therefore, the dimension of the nullspace of $\mX^T \mA \mX - \lambda_0 \mI_n$ is larger than 2. Let $v_1,v_2$ be the two orthogonal vectors in the nullspace of $\mX^T \mA \mX - \lambda_0 \mI_n$. Then, $v_1, v_2$ are two eigenvectors of $\mX^T \mA \mX$, and $\mX v_1, \mX v_2$ are two orthogonal eigenvectors of $\mA$ associated with $\lambda_0$. Therefore, we have obtained two orthonormal  eigenvectors of $\mA$ associated with $\lambda_0$.
	
	\vspace{0.2cm}
	Now, replace $x_0$ by two orthogonal vectors $x_1 = \mX v_1, x_2 = \mX v_2$ and repeat the above steps.
	
	\vspace{0.2cm}
	After $(m-1)$ repetitions, we have obtained $m$ orthogonal eigenvectors of $\mA$ associated with $\lambda_0$.  Let $\mX^{(m)}, \mY^{(m)}$ denote the matrix $\mX, \mY$ after the $(m-1)$-th repetition, respectively. The polynomial~\eqref{eq:symmploy} becomes
	\[ P(\lambda) = \det(\mX^{(m),T} \mA \mX^{(m)} - \lambda \mI_n) = (\lambda_0 - \lambda)^m \det(\mY^{(m),T} \mA \mY^{(m)} - \lambda \mI_{n-m}). \]
	Since $\lambda_0$ has multiplicity $m$, the determinant $\det(\mY^{(m),T} \mA \mY^{(Mm)} - \lambda \mI_{n-m}) \neq 0$. The dimension of the nullspace of $\mX^{(m),T} \mA \mX^{(m)} - \lambda_0 \mI_n$ is $m$. Therefore, we are unable to find the $(m+1)$-th orthonormal eigenvector associated with $\lambda_0$, and the repetition stops. 
\end{proof}

\subsection{Diagonalization}

\begin{defn}[Diagonalization]\label{def:diagonalization}
	Let  $\mA \in \bbR^{n \times n}$ be a square matrix with $n$ independent eigenvectors. We diagonalize the matrix $\mA$ as
	\begin{equation}\label{eq:defdiag}
		\mA = \mS^{-1} \Lambda \mS,
	\end{equation}
	where $\Lambda \in \bbR^{n \times n}$ is a diagonal matrix consisting of $n$ eigenvalues of $\mA$, and  $\mS \in \bbR^{n \times n}$ is a matrix whose columns are independent eigenvectors of $\mA$ ordered in the same as the eigenvalues in $\Lambda$.
\end{defn}

\begin{thm}[Diagonalization of symmetric matrices]
Symmetric matrices are diagonalizable.
\end{thm}

\begin{proof}
	By Theorems~\ref{thm:symmortho} and \ref{thm:symmrepeat}, for any symmetric matrix $\mA \in \bbR^{n \times n}$, $\mA$ has $n$ orthogonal eigenvectors with possibly repeated eigenvalues. Therefore, the matrix $\mA$   is able to be diagonalized as equation~\eqref{eq:defdiag}.
\end{proof}

\begin{thm}[Descent of matrix powers]
	Let $\mA \in \bbR^{n \times n}$ be  a diagonalizable matrix with eigenvalues $\lambda_1,...,\lambda_n$. The power of the matrix $\mA^k \rightarrow 0$ as $k \rightarrow +\infty$ if and only if $|\lambda_i| < 1$, for all $i \in [n]$.
\end{thm}

\begin{proof}
Since $\mA$ is diagonalizable, let $\mA =\mS^{-1} \Lambda \mS$, where $\Lambda, \mS$ are defined the same as Definition~\ref{def:diagonalization}. For all integer $k\geq 0$, we have $\mA^k = \mS^{-1} \Lambda^{k} \mS$. Then,
	\[ \lim_{k \rightarrow +\infty} \mA^k = \lim_{k \rightarrow +\infty} \mS^{-1} \Lambda^{k} \mS  =  \lim_{k \rightarrow +\infty} \mS^{-1} \begin{bmatrix}
		|\lambda_i|^k & \cdots &0\\
		\vdots & &\vdots\\
		0 & \cdots & |\lambda_n|^k
	\end{bmatrix} \mS  = 0 \quad \Leftrightarrow \quad  |\lambda_i| < 1, \text{ for all } i \in [n]. \]
\end{proof}



\end{document}